{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![citations.jpg](data:image/jpeg;base64,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)\n",
        "# Justin Johnson"
      ],
      "metadata": {
        "id": "-NbMDQ0PmWQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# “Derivatives, Backpropagation, and Vectorization” — Justin Johnson (2017)\n",
        "\n",
        "# https://cs231n.stanford.edu/handouts/derivatives.pdf\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This paper provides a clear and rigorous mathematical foundation for understanding **derivatives, backpropagation, and vectorization** in deep learning. It systematically builds from scalar derivatives to gradients, Jacobians, and generalized Jacobians for tensor-valued functions, explaining how backpropagation follows directly from the **chain rule** without explicitly forming large derivative matrices. The work emphasizes computational efficiency and conceptual clarity, bridging multivariable calculus with practical neural network implementation.\n",
        "\n",
        "---\n",
        "\n",
        "## Problems\n",
        "\n",
        "### Conceptual confusion around derivatives in deep learning  \n",
        "Many learners struggle to generalize scalar differentiation to vector- and tensor-valued functions used in neural networks.\n",
        "\n",
        "### Misunderstanding of backpropagation mechanics  \n",
        "Backpropagation is often treated as a black-box algorithm rather than a direct consequence of the chain rule.\n",
        "\n",
        "### Infeasibility of explicit Jacobian computation  \n",
        "Explicitly forming Jacobian matrices for modern networks is computationally and memory prohibitive.\n",
        "\n",
        "### Lack of clarity on tensor-level differentiation  \n",
        "Standard matrix calculus does not adequately explain how derivatives operate on high-dimensional tensors.\n",
        "\n",
        "---\n",
        "\n",
        "## Proposed Solutions\n",
        "\n",
        "### Progressive derivative framework  \n",
        "Introduces derivatives in increasing generality:\n",
        "- scalar derivatives  \n",
        "- gradients  \n",
        "- Jacobians  \n",
        "- generalized (tensor) Jacobians  \n",
        "\n",
        "### Jacobian-based chain rule formulation  \n",
        "Expresses backpropagation as structured multiplications of Jacobians and generalized Jacobians.\n",
        "\n",
        "### Vector–Jacobian product perspective  \n",
        "Replaces explicit Jacobian construction with efficient computation of Jacobian–vector products.\n",
        "\n",
        "### Concrete worked examples  \n",
        "Uses low-dimensional linear layer examples that generalize naturally to large-scale neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "The purpose of the paper is to provide a **mathematically precise yet intuitive** explanation of gradient computation in neural networks, demonstrating that backpropagation is a direct application of multivariable calculus and can be implemented efficiently without explicitly constructing large derivative objects.\n",
        "\n",
        "---\n",
        "\n",
        "## Methodology\n",
        "\n",
        "### Mathematical exposition  \n",
        "Provides formal definitions of derivatives for scalar, vector, matrix, and tensor-valued functions.\n",
        "\n",
        "### Chain rule generalization  \n",
        "Extends the chain rule from scalar functions to vector and tensor functions using Jacobian and generalized Jacobian notation.\n",
        "\n",
        "### Tensor-aware differentiation  \n",
        "Introduces structured indexing for generalized Jacobians to handle tensor inputs and outputs rigorously.\n",
        "\n",
        "### Computational analysis  \n",
        "Analyzes the memory and computational cost of explicit Jacobians and motivates vectorized alternatives.\n",
        "\n",
        "---\n",
        "\n",
        "## Results\n",
        "\n",
        "### Unified derivative interpretation  \n",
        "Shows that gradients, Jacobians, and tensor derivatives obey the same algebraic principles.\n",
        "\n",
        "### Efficient backpropagation formulation  \n",
        "Demonstrates that gradients with respect to inputs and parameters can be computed using vector–Jacobian products:\n",
        "$$\n",
        "\\nabla_x \\mathcal{L} = v^\\top J_f(x)\n",
        "$$\n",
        "without explicitly forming $$J_f(x)$$.\n",
        "\n",
        "### Scalability insight  \n",
        "Explains why modern deep learning frameworks scale to large models despite implicitly enormous Jacobians.\n",
        "\n",
        "### Practical formulas  \n",
        "Derives closed-form gradient expressions for linear layers that avoid explicit Jacobian construction.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "The paper clarifies that **backpropagation is fundamentally an application of the chain rule for vector- and tensor-valued functions**, with Jacobians as the underlying mathematical structure. By emphasizing generalized Jacobians and vectorized computation, it explains how deep learning systems efficiently compute gradients at scale. This work serves as a foundational reference for understanding both the **theory** and **implementation** of gradient-based optimization in modern neural networks.\n"
      ],
      "metadata": {
        "id": "EAFN7ZLSlxvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derivatives, Backpropagation, and Vectorization  \n",
        "*Mathematical Structure with Full $$-Based Notation*\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Scalar Derivatives\n",
        "\n",
        "### Definition\n",
        "\n",
        "For a scalar function\n",
        "$$\n",
        "f:\\mathbb{R} \\rightarrow \\mathbb{R},\n",
        "$$\n",
        "the derivative at $$x$$ is\n",
        "$$\n",
        "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}.\n",
        "$$\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "The derivative measures how much the output changes in response to a small change in the input.\n",
        "\n",
        "### Role\n",
        "\n",
        "Establishes the intuition of derivatives as **local linear approximations**, later generalized to vectors and tensors in neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. First-Order Approximation\n",
        "\n",
        "### Equation\n",
        "\n",
        "$$\n",
        "f(x + \\varepsilon) \\approx f(x) + \\varepsilon f'(x).\n",
        "$$\n",
        "\n",
        "### Meaning\n",
        "\n",
        "Small input perturbations produce approximately linear changes in the output.\n",
        "\n",
        "### Role\n",
        "\n",
        "Introduces the principle that derivatives link **input perturbations to output perturbations**, a core idea in backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Chain Rule (Scalar Case)\n",
        "\n",
        "### Equation\n",
        "\n",
        "If\n",
        "$$\n",
        "y = f(x), \\quad z = g(y),\n",
        "$$\n",
        "then\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x}\n",
        "=\n",
        "\\frac{\\partial z}{\\partial y}\n",
        "\\frac{\\partial y}{\\partial x}.\n",
        "$$\n",
        "\n",
        "### Meaning\n",
        "\n",
        "The influence of $$x$$ on $$z$$ propagates through intermediate variables.\n",
        "\n",
        "### Role\n",
        "\n",
        "Forms the conceptual backbone of backpropagation, later extended to vector- and tensor-valued functions.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Gradient: Vector Input, Scalar Output\n",
        "\n",
        "### Definition\n",
        "\n",
        "For\n",
        "$$\n",
        "f:\\mathbb{R}^N \\rightarrow \\mathbb{R},\n",
        "$$\n",
        "the gradient is\n",
        "$$\n",
        "\\nabla_x f(x)\n",
        "=\n",
        "\\left(\n",
        "\\frac{\\partial f}{\\partial x_1},\n",
        "\\ldots,\n",
        "\\frac{\\partial f}{\\partial x_N}\n",
        "\\right).\n",
        "$$\n",
        "\n",
        "### Linear Approximation\n",
        "\n",
        "$$\n",
        "f(x + \\Delta x)\n",
        "\\approx\n",
        "f(x) + \\nabla_x f(x) \\cdot \\Delta x.\n",
        "$$\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "Each component of the gradient measures sensitivity along one coordinate direction.\n",
        "\n",
        "### Role\n",
        "\n",
        "Explains how loss functions depend on vector inputs, enabling gradient-based learning.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Jacobian Matrix: Vector Input, Vector Output\n",
        "\n",
        "### Definition\n",
        "\n",
        "For\n",
        "$$\n",
        "f:\\mathbb{R}^N \\rightarrow \\mathbb{R}^M,\n",
        "$$\n",
        "with\n",
        "$$\n",
        "y = f(x),\n",
        "$$\n",
        "the Jacobian is\n",
        "$$\n",
        "J_f(x)\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_N} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial y_M}{\\partial x_1} & \\cdots & \\frac{\\partial y_M}{\\partial x_N}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "### Linear Approximation\n",
        "\n",
        "$$\n",
        "y(x + \\Delta x)\n",
        "\\approx\n",
        "y(x) + J_f(x)\\,\\Delta x.\n",
        "$$\n",
        "\n",
        "### Meaning\n",
        "\n",
        "The Jacobian encodes how **every output component depends on every input component**.\n",
        "\n",
        "### Role\n",
        "\n",
        "Provides the correct derivative object for neural network layers, which map vectors to vectors.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Chain Rule with Jacobians\n",
        "\n",
        "### Equation\n",
        "\n",
        "If\n",
        "$$\n",
        "y = f(x), \\quad z = g(y),\n",
        "$$\n",
        "then\n",
        "$$\n",
        "\\frac{\\partial z}{\\partial x}\n",
        "=\n",
        "\\frac{\\partial z}{\\partial y}\n",
        "\\frac{\\partial y}{\\partial x}.\n",
        "$$\n",
        "\n",
        "In matrix form:\n",
        "$$\n",
        "J_{g \\circ f}(x)\n",
        "=\n",
        "J_g(y)\\,J_f(x).\n",
        "$$\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "Backpropagation is mathematically a **sequence of Jacobian multiplications**.\n",
        "\n",
        "### Role\n",
        "\n",
        "Shows that gradient propagation in deep networks is a direct application of multivariable calculus.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Generalized Jacobian: Tensor Input, Tensor Output\n",
        "\n",
        "### Definition\n",
        "\n",
        "For tensor-valued functions\n",
        "$$\n",
        "f:\n",
        "\\mathbb{R}^{N_1 \\times \\cdots \\times N_{D_x}}\n",
        "\\rightarrow\n",
        "\\mathbb{R}^{M_1 \\times \\cdots \\times M_{D_y}},\n",
        "$$\n",
        "the derivative\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x}\n",
        "$$\n",
        "is a generalized Jacobian with shape\n",
        "$$\n",
        "(M_1 \\times \\cdots \\times M_{D_y})\n",
        "\\times\n",
        "(N_1 \\times \\cdots \\times N_{D_x}).\n",
        "$$\n",
        "\n",
        "### Elementwise Form\n",
        "\n",
        "$$\n",
        "\\left(\\frac{\\partial y}{\\partial x}\\right)_{i,j}\n",
        "=\n",
        "\\frac{\\partial y_i}{\\partial x_j}.\n",
        "$$\n",
        "\n",
        "### Role\n",
        "\n",
        "Extends differentiation to images, feature maps, and batches represented as tensors.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Generalized Matrix–Vector and Matrix–Matrix Products\n",
        "\n",
        "### Equation\n",
        "\n",
        "$$\n",
        "\\left(\n",
        "\\frac{\\partial y}{\\partial x}\\,\\Delta x\n",
        "\\right)_j\n",
        "=\n",
        "\\sum_i\n",
        "\\left(\\frac{\\partial y}{\\partial x}\\right)_{i,j}\n",
        "(\\Delta x)_i.\n",
        "$$\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "Tensor differentiation follows the same algebraic rules as matrix calculus, with higher-dimensional indexing.\n",
        "\n",
        "### Role\n",
        "\n",
        "Justifies using familiar linear algebra operations for tensor backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Backpropagation with Tensors\n",
        "\n",
        "### Core Equations\n",
        "\n",
        "For a layer\n",
        "$$\n",
        "y = f(x, w)\n",
        "$$\n",
        "and scalar loss $$L$$:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x}\n",
        "=\n",
        "\\frac{\\partial L}{\\partial y}\n",
        "\\frac{\\partial y}{\\partial x},\n",
        "\\qquad\n",
        "\\frac{\\partial L}{\\partial w}\n",
        "=\n",
        "\\frac{\\partial L}{\\partial y}\n",
        "\\frac{\\partial y}{\\partial w}.\n",
        "$$\n",
        "\n",
        "### Role\n",
        "\n",
        "Formally defines backpropagation as **Jacobian–vector products**, not explicit Jacobian construction.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Computational Infeasibility of Explicit Jacobians\n",
        "\n",
        "### Example\n",
        "\n",
        "For a linear layer with\n",
        "$$\n",
        "N = 64, \\quad D = M = 4096,\n",
        "$$\n",
        "the Jacobian size is approximately\n",
        "$$\n",
        "64 \\cdot 4096 \\cdot 64 \\cdot 4096\n",
        ">\n",
        "6.8 \\times 10^{10}\n",
        "$$\n",
        "entries.\n",
        "\n",
        "### Role\n",
        "\n",
        "Demonstrates why explicit Jacobians are impractical in real neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Vector–Jacobian Products\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "Instead of forming\n",
        "$$\n",
        "\\frac{\\partial y}{\\partial x},\n",
        "$$\n",
        "compute\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y}\n",
        "\\frac{\\partial y}{\\partial x}.\n",
        "$$\n",
        "\n",
        "### Role\n",
        "\n",
        "Explains how automatic differentiation frameworks efficiently implement backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Worked Example: Linear Layer\n",
        "\n",
        "### Model\n",
        "\n",
        "$$\n",
        "y = xw.\n",
        "$$\n",
        "\n",
        "### Resulting Gradient\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x}\n",
        "=\n",
        "\\frac{\\partial L}{\\partial y}\\,w^\\top.\n",
        "$$\n",
        "\n",
        "### Meaning\n",
        "\n",
        "Gradients are computed via simple matrix multiplications.\n",
        "\n",
        "### Role\n",
        "\n",
        "Shows how vectorization replaces explicit differentiation, enabling scalable learning.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Statistical Content\n",
        "\n",
        "This paper contains **no statistical inference**:\n",
        "- No hypothesis testing  \n",
        "- No estimators or confidence intervals  \n",
        "- No probabilistic modeling  \n",
        "\n",
        "The analysis is entirely deterministic, based on calculus and linear algebra.\n",
        "\n",
        "---\n",
        "\n",
        "## Final Mathematical Insight\n",
        "\n",
        "- All derivatives in deep learning reduce to **linear approximations**\n",
        "- Backpropagation equals **Jacobians + chain rule**\n",
        "- Scalability comes from **vector–Jacobian products**, not storing Jacobians\n",
        "- Tensor differentiation obeys the same rules as matrix calculus\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Summary\n",
        "\n",
        "Backpropagation is simply efficient multivariable calculus applied to vector- and tensor-valued functions, with Jacobians providing the mathematical structure and vectorization making large-scale learning computationally feasible.\n"
      ],
      "metadata": {
        "id": "Fe2zEDIRm69g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How the Jacobian Matrix, Its Determinant, and Norm-Based Constants Explain  \n",
        "## Derivatives, Backpropagation, and Vectorization\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Jacobian Matrix: The Core Object Linking All Three Perspectives\n",
        "\n",
        "### 1.1 Role in Derivatives\n",
        "\n",
        "In deep learning, most mappings are vector-valued:\n",
        "$$\n",
        "f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m.\n",
        "$$\n",
        "\n",
        "For such functions, the Jacobian matrix\n",
        "$$\n",
        "J_f(x) = \\frac{\\partial y}{\\partial x}\n",
        "$$\n",
        "is the correct generalization of the derivative.\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "The Jacobian provides a **first-order linear approximation**:\n",
        "$$\n",
        "f(x + \\Delta x)\n",
        "\\approx\n",
        "f(x) + J_f(x)\\,\\Delta x.\n",
        "$$\n",
        "\n",
        "This reframes derivatives as **local linear maps** acting on perturbation vectors, rather than symbolic expressions. Every neural network layer locally behaves like a linear transformation defined by its Jacobian.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 Role in Backpropagation\n",
        "\n",
        "A neural network is a composition of functions:\n",
        "$$\n",
        "f = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1.\n",
        "$$\n",
        "\n",
        "Applying the chain rule yields:\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial x}\n",
        "=\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial y_L}\n",
        "J_{f_L}\n",
        "J_{f_{L-1}}\n",
        "\\cdots\n",
        "J_{f_1}.\n",
        "$$\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "- Backpropagation is mathematically a **product of Jacobian matrices**\n",
        "- Gradient flow is fully explained by matrix multiplication\n",
        "- Vanishing and exploding gradients arise from repeated Jacobian products\n",
        "\n",
        "This provides a rigorous explanation for gradient behavior in deep networks.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 Role in Vectorization\n",
        "\n",
        "Explicit Jacobian matrices are prohibitively large. The key idea is:\n",
        "\n",
        "Instead of forming $$J$$, compute\n",
        "$$\n",
        "v^\\top J\n",
        "$$\n",
        "which is a **vector–Jacobian product**.\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "- Enables efficient gradient computation\n",
        "- Justifies vectorized implementations in deep learning frameworks\n",
        "- Explains how backpropagation scales to very large models\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Determinant of the Jacobian: Measuring Local Volume Change\n",
        "\n",
        "### 2.1 Geometric Meaning\n",
        "\n",
        "For square Jacobians ($$n = m$$), the determinant\n",
        "$$\n",
        "\\det(J_f(x))\n",
        "$$\n",
        "measures how the function locally transforms volume.\n",
        "\n",
        "- $$|\\det(J)| > 1$$ : local expansion  \n",
        "- $$|\\det(J)| < 1$$ : local contraction  \n",
        "- $$\\det(J) = 0$$ : collapse of dimensions  \n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Role in Backpropagation Intuition\n",
        "\n",
        "Although determinants are not computed explicitly during backpropagation, they provide crucial **geometric intuition**.\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "- Repeated contractions explain vanishing gradients\n",
        "- Strong expansions explain instability\n",
        "- Links optimization difficulty to geometric distortion of space\n",
        "\n",
        "This perspective is especially useful for understanding:\n",
        "- Deep linear networks  \n",
        "- Normalization effects  \n",
        "- Invertibility and information loss  \n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Role in Vectorization\n",
        "\n",
        "The determinant also explains why explicit Jacobians are unnecessary:\n",
        "\n",
        "- Full volume-change information is rarely required\n",
        "- Optimization depends on **directional sensitivity**, not total distortion\n",
        "- Vector–Jacobian products capture exactly what learning needs\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Constants (Norms, Lipschitz-Type Bounds): Controlling Scale and Stability\n",
        "\n",
        "While not always stated explicitly, norm-based constants play a central role.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 Jacobian Norm as a Stability Constant\n",
        "\n",
        "The operator norm\n",
        "$$\n",
        "\\|J_f(x)\\|\n",
        "$$\n",
        "bounds output sensitivity:\n",
        "$$\n",
        "\\|f(x_1) - f(x_2)\\|\n",
        "\\le\n",
        "\\|J_f(x)\\| \\, \\|x_1 - x_2\\|.\n",
        "$$\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "- Quantifies sensitivity to perturbations\n",
        "- Explains exploding gradients when norms are large\n",
        "- Explains vanishing gradients when norms are small\n",
        "- Connects calculus directly to optimization stability\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Role in Backpropagation\n",
        "\n",
        "For a deep network:\n",
        "$$\n",
        "\\|J_{\\text{network}}\\|\n",
        "\\le\n",
        "\\prod_{l=1}^{L} \\|J_l\\|.\n",
        "$$\n",
        "\n",
        "#### How It Helps\n",
        "\n",
        "- Explains why depth amplifies instability\n",
        "- Motivates:\n",
        "  - residual connections  \n",
        "  - normalization layers  \n",
        "  - careful initialization  \n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Role in Vectorization\n",
        "\n",
        "Norm-based bounds justify vectorized computation:\n",
        "\n",
        "- Gradients depend on **scaled directional derivatives**\n",
        "- Vectorization exploits linearity and boundedness\n",
        "- No need to track full higher-order geometric structure\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Unified View Across the Three Perspectives\n",
        "\n",
        "| Concept | What It Explains | Why It Matters |\n",
        "|---|---|---|\n",
        "| Jacobian matrix | Local linear behavior | Foundation of backpropagation |\n",
        "| Jacobian determinant | Local volume distortion | Geometric intuition for stability |\n",
        "| Jacobian norm / constants | Sensitivity bounds | Explains gradient explosion and vanishing |\n",
        "| Vector–Jacobian products | Efficient computation | Enables scalable training |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Big Picture Insight\n",
        "\n",
        "- Derivatives explain how outputs change  \n",
        "- Jacobians explain how **vectors** change  \n",
        "- Determinants explain how **space** deforms  \n",
        "- Norm-based constants explain **stability**  \n",
        "- Vectorization explains **efficient computation**  \n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Summary\n",
        "\n",
        "In derivatives, backpropagation, and vectorization, the Jacobian matrix provides the mathematical structure, the determinant provides geometric intuition, and norm-based constants provide stability control—together explaining how deep learning gradients are computed, propagated, and made computationally feasible.\n"
      ],
      "metadata": {
        "id": "ZFHGmwVqnuZV"
      }
    }
  ]
}