{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# https://distill.pub/2020/circuits/zoom-in/\n",
        "\n",
        "# Core Motivation and Conceptual Framework of Mechanistic Interpretability\n",
        "\n",
        "## 1. Core Motivation: “Zooming In” as a Scientific Shift\n",
        "\n",
        "Scientific progress has historically been driven by the development of tools that enable finer inspection of complex systems. Microscopes enabled the discovery of cells, and crystallography revealed the structure of DNA. The authors argue that neural network interpretability is at a comparable inflection point.\n",
        "\n",
        "Rather than attempting to explain entire models at a high level, they propose *zooming in* to internal components such as neurons, directions in activation space, weights, and their interconnections. At this scale, neural networks cease to be opaque artifacts and instead become systems with tractable, interpretable, and algorithmic internal structure.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Central Thesis\n",
        "\n",
        "The central thesis is that neural networks contain meaningful internal structure that can be studied with scientific rigor.\n",
        "\n",
        "Algorithms are not only implicit in the training procedure but are explicitly encoded within the learned weights. These algorithms are implemented through features that are connected into circuits, forming structured computational mechanisms inside the network.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Three Speculative Claims (Foundational Framework)\n",
        "\n",
        "### Claim 1 — Features\n",
        "\n",
        "Features are the fundamental units of neural networks.  \n",
        "A feature corresponds to a direction in activation space and may, but does not necessarily, align with a single neuron. These features are often meaningful, even when their function is initially unintuitive.\n",
        "\n",
        "Crucially, features can be empirically characterized, tested, and falsified, analogous to the study of neurons in neuroscience.\n",
        "\n",
        "---\n",
        "\n",
        "### Claim 2 — Circuits\n",
        "\n",
        "Features do not operate independently; they are connected by weights into circuits.  \n",
        "A circuit is a small subgraph of the network that implements a specific computation.\n",
        "\n",
        "Circuits frequently exhibit symmetry, modularity, and algorithmic clarity. By studying circuits, one can directly read computational procedures from the weights themselves.\n",
        "\n",
        "---\n",
        "\n",
        "### Claim 3 — Universality\n",
        "\n",
        "Similar features and circuits tend to reappear across different models and tasks.  \n",
        "This suggests convergent learning rather than arbitrary internal representations.\n",
        "\n",
        "If this claim holds broadly, interpretability could develop a shared vocabulary and taxonomy of features and circuits, enabling cumulative scientific progress.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Evidence for Claim 1: Features Exist and Are Understandable\n",
        "\n",
        "### Example A — Curve Detectors\n",
        "\n",
        "Curve detectors appear consistently in early layers of vision models. They detect curved boundaries with specific orientations and form families spanning all orientations.\n",
        "\n",
        "These features are supported by multiple independent lines of evidence:\n",
        "- Feature visualization\n",
        "- Dataset activation analysis\n",
        "- Synthetic stimuli\n",
        "- Joint tuning curves\n",
        "- Circuit construction\n",
        "- Downstream usage\n",
        "- Hand-designed reimplementations\n",
        "\n",
        "Together, these satisfy evidentiary standards comparable to those used in visual neuroscience.\n",
        "\n",
        "---\n",
        "\n",
        "### Example B — High–Low Frequency Detectors\n",
        "\n",
        "These features detect low-frequency patterns on one side of a receptive field and high-frequency patterns on the other. Although less intuitive initially, they become simple once understood.\n",
        "\n",
        "They likely function as boundary detection heuristics, particularly under blur, demonstrating that interpretability extends beyond obvious features.\n",
        "\n",
        "---\n",
        "\n",
        "### Example C — Pose-Invariant Dog Head Detector\n",
        "\n",
        "This is a high-level semantic feature responding to dog heads across multiple poses and orientations. Feature visualization, dataset evidence, and synthetic rendering all confirm the interpretation.\n",
        "\n",
        "This example demonstrates that even abstract semantic concepts are mechanistically accessible within neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Polysemantic Neurons and Superposition\n",
        "\n",
        "Not all neurons are pure; many are polysemantic and respond to multiple unrelated concepts, such as cat faces, car fronts, and cat legs.\n",
        "\n",
        "This phenomenon arises from *superposition*, where multiple features share the same neuron to save representational capacity. Superposition enables efficiency but complicates interpretability, revealing a tension between capacity optimization and feature disentanglement.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Evidence for Claim 2: Circuits Are Real and Structured\n",
        "\n",
        "### Circuit 1 — Curve Detection Circuit\n",
        "\n",
        "Curve detectors are constructed from aligned lower-level line and curve detectors. Weight matrices form explicit geometric patterns corresponding to curves and exhibit equivariance, rotating with feature orientation.\n",
        "\n",
        "This demonstrates that weights encode spatial algorithms rather than arbitrary numerical values.\n",
        "\n",
        "---\n",
        "\n",
        "### Circuit 2 — Oriented Dog Head Circuit\n",
        "\n",
        "The network maintains separate pathways for left-facing and right-facing dog heads, which inhibit each other to sharpen selectivity. A later layer unions these pathways to produce pose-invariant detection.\n",
        "\n",
        "This implements a form of case-splitting followed by conditional recombination, showing that gradient descent can learn structured conditional computation.\n",
        "\n",
        "---\n",
        "\n",
        "### Circuit 3 — Cars in Superposition\n",
        "\n",
        "A pure car detector exists at one stage, but later layers mix it into dog-related neurons. This illustrates intentional feature packing rather than accidental entanglement and explains how polysemantic neurons can still support reliable downstream computation.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Circuit Motifs (Recurring Patterns)\n",
        "\n",
        "Across different circuits, repeated abstract patterns appear, including:\n",
        "- Equivariance\n",
        "- Unioning over cases\n",
        "- Superposition\n",
        "\n",
        "These motifs are analogous to recurring circuit motifs in systems biology and may be more fundamental than individual circuits. Understanding motifs provides leverage across many architectures.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Evidence for Claim 3: Universality\n",
        "\n",
        "Certain low-level features, such as curve detectors and frequency detectors, appear consistently across:\n",
        "- Different architectures\n",
        "- Different datasets\n",
        "- Different training runs\n",
        "\n",
        "Prior work on representational similarity supports this observation. While current evidence is suggestive rather than conclusive, broad universality would enable the construction of a “periodic table” of features.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Interpretability as a Natural Science\n",
        "\n",
        "Interpretability is currently pre-paradigmatic, in the sense described by Kuhn. There is no consensus on objects of study, evaluation standards, or success criteria.\n",
        "\n",
        "Circuits offer a path forward by enabling:\n",
        "- Small, falsifiable claims\n",
        "- Empirical testing\n",
        "- Predictive power, such as targeted weight edits\n",
        "\n",
        "Circuits may therefore serve as the epistemic foundation for understanding entire models.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Closing Perspective\n",
        "\n",
        "Early microscopy was qualitative, slow, and controversial, yet ultimately transformative. Mechanistic interpretability may follow a similar trajectory.\n",
        "\n",
        "Understanding neural networks from the inside out is feasible, scientific, and necessary. Circuits research reframes deep learning models as objects of empirical investigation rather than inscrutable artifacts.\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Synthesis\n",
        "\n",
        "Neural networks are structured computational systems whose features and circuits implement discoverable algorithms, and studying them at the appropriate scale can ground interpretability as a true natural science.\n"
      ],
      "metadata": {
        "id": "wqiGMHds6gGr"
      }
    }
  ]
}