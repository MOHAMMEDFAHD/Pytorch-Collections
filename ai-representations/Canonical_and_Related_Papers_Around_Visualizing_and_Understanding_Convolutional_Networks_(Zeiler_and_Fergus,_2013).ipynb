{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Canonical and Related Papers Around *Visualizing and Understanding Convolutional Networks* (Zeiler and Fergus, 2013)\n",
        "\n",
        "| Year | Paper Title | Author(s) | Primary Contribution / Role |\n",
        "|---:|---|---|---|\n",
        "| 2009 | *Learning Multiple Layers of Features from Tiny Images* | Alex Krizhevsky | Introduced the CIFAR datasets and early CNN training methodology for small natural images. |\n",
        "| 2009 | *Visualizing Higher-Layer Features of a Deep Network* | Dumitru Erhan; Yoshua Bengio; Aaron Courville; Pascal Vincent | Early systematic attempt to visualize and interpret internal representations of deep networks. |\n",
        "| 2009 | *ImageNet: A Large-Scale Hierarchical Image Database* | Jia Deng et al. | Created ImageNet, enabling large-scale supervised training and standardized evaluation for CNNs. |\n",
        "| 2012 | *ImageNet Classification with Deep Convolutional Neural Networks* | Alex Krizhevsky; Ilya Sutskever; Geoffrey Hinton | Introduced AlexNet; established large-scale CNN training as a dominant paradigm in vision. |\n",
        "| 2013 | *Visualizing and Understanding Convolutional Networks* | Matthew D. Zeiler; Rob Fergus | Introduced deconvolutional networks and systematic filter/activation visualization to diagnose and improve CNNs. |\n",
        "| 2013 | *Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps* | Karen Simonyan; Andrea Vedaldi; Andrew Zisserman | Proposed gradient-based saliency maps for interpreting CNN predictions and visualizing discriminative evidence. |\n",
        "| 2013 | *Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation* | Ross Girshick et al. | Introduced R-CNN; demonstrated strong transferability of CNN features to detection and segmentation pipelines. |\n",
        "| 2013 | *OverFeat: Integrated Recognition, Localization and Detection using CNNs* | Pierre Sermanet et al. | Unified recognition, localization, and detection in a single CNN-style framework. |\n",
        "| 2013 | *DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition* | Jeff Donahue et al. | Demonstrated CNN activations as general-purpose transferable features for downstream recognition tasks. |\n",
        "| 2013 | *Hierarchical Convolutional Deep Learning in Computer Vision* | Matthew D. Zeiler | Expanded empirical and conceptual analysis of hierarchical feature representations in CNNs. |\n",
        "| 2014 | *Very Deep Convolutional Networks for Large-Scale Image Recognition* | Karen Simonyan; Andrew Zisserman | Introduced VGG; established depth as a key architectural principle for improving CNN performance. |\n",
        "| 2014 | *Understanding Deep Image Representations by Inverting Them* | Aravindh Mahendran; Andrea Vedaldi | Proposed feature inversion to reveal what information is retained in intermediate CNN representations. |\n",
        "| 2014 | *How Transferable Are Features in Deep Neural Networks?* | Jason Yosinski et al. | Quantified feature transferability across layers, supporting modular reuse and fine-tuning practices. |\n",
        "| 2014 | *Supplementary Material for: How Transferable Are Features in Deep Neural Networks?* | Jason Yosinski et al. | Extended experimental analysis and additional results supporting transferability claims. |\n",
        "| 2014 | *Caffe: Convolutional Architecture for Fast Feature Embedding* | Yangqing Jia et al. | Introduced a widely adopted deep learning framework that accelerated CNN experimentation and deployment. |\n",
        "| 2014 | *Fully Convolutional Networks for Semantic Segmentation* | Evan Shelhamer; Jonathan Long; Trevor Darrell | Introduced FCNs, enabling end-to-end dense prediction for semantic segmentation. |\n",
        "| 2014 | *Deeply-Supervised Nets* | Chen-Yu Lee et al. | Proposed layer-wise supervision to improve optimization and gradient flow in deep CNNs. |\n",
        "| 2014 | *Dropout: A Simple Way to Prevent Neural Networks from Overfitting* | Nitish Srivastava et al. | Introduced dropout regularization, improving generalization in deep networks including CNNs. |\n",
        "| 2014 | *Learning and Transferring Mid-level Image Representations Using CNNs* | Maxime Oquab et al. | Demonstrated transfer learning for detection/classification with limited labeled data. |\n",
        "| 2015 | *Deep Residual Learning for Image Recognition* | Kaiming He et al. | Introduced ResNet, addressing optimization degradation and enabling very deep CNNs. |\n",
        "| 2015 | *Identity Mappings in Deep Residual Networks* | Kaiming He et al. | Refined residual learning with improved skip-connection formulations for easier optimization. |\n",
        "| 2015 | *Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks* | Shaoqing Ren et al. | Introduced end-to-end detection with region proposal networks, improving speed and accuracy. |\n",
        "| 2015 | *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification* | Kaiming He et al. | Analyzed rectifiers and introduced PReLU, improving optimization and performance in deep CNNs. |\n",
        "| 2015 | *Evaluating the Visualization of What a Deep Neural Network Has Learned* | Wojciech Samek et al. | Proposed quantitative perspectives for evaluating interpretability and visualization methods. |\n",
        "| 2015 | *On Pixel-Wise Explanations by Layer-Wise Relevance Propagation* | Sebastian Bach et al. | Introduced LRP for pixel-wise explanation of model decisions. |\n",
        "| 2015 | *Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition* | Grégoire Montavon et al. | Provided a theoretical foundation for LRP-style explanations via Taylor-based relevance decomposition. |\n",
        "| 2016 | *Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning* | Christian Szegedy et al. | Combined inception modules with residual connections, improving accuracy and training stability. |\n",
        "| 2016 | *Feature Pyramid Networks for Object Detection* | Tsung-Yi Lin et al. | Introduced multi-scale feature pyramids for robust detection across object sizes. |\n",
        "| 2016 | *R-FCN: Object Detection via Region-based Fully Convolutional Networks* | Jifeng Dai et al. | Proposed efficient detection using shared convolutional features with region-based position-sensitive scoring. |\n",
        "| 2016 | *DeepLab: Semantic Image Segmentation with Atrous Convolution* | Liang-Chieh Chen et al. | Introduced atrous (dilated) convolutions to expand receptive fields for segmentation without losing resolution. |\n",
        "| 2017 | *SmoothGrad: Removing Noise by Adding Noise* | Daniel Smilkov et al. | Improved gradient-based explanations by averaging gradients under noise perturbations. |\n",
        "| 2017 | *Axiomatic Attribution for Deep Networks* | Mukund Sundararajan; Ankur Taly; Qiqi Yan | Introduced Integrated Gradients, providing axiomatic attribution with improved reliability. |\n",
        "| 2017 | *Focal Loss for Dense Object Detection* | Tsung-Yi Lin et al. | Addressed class imbalance in dense detection by down-weighting easy negatives. |\n",
        "| 2017 | *Squeeze-and-Excitation Networks* | Jie Hu et al. | Introduced channel-wise attention (SE blocks) to recalibrate feature responses and improve representational power. |\n",
        "| 2017 | *Interpretation of Neural Networks is Fragile* | Amirata Ghorbani et al. | Demonstrated instability and brittleness of saliency-based interpretations under small perturbations. |\n",
        "| 2017 | *Research on Deep Learning of Small Sample Data Based on Transfer Learning* | Wei Zhao | Applied transfer learning techniques under small-sample regimes, emphasizing data efficiency. |\n",
        "| 2018 | *A Benchmark for Interpretability Methods in Deep Neural Networks* | Sara Hooker et al. | Provided systematic benchmarking and comparative evaluation of interpretability methods. |\n",
        "| 2018 | *VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving* | Mariusz Bojarski et al. | Proposed fast, architecture-aware visualization for CNN decision support in driving contexts. |\n",
        "| 2020 | *An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale* | Alexey Dosovitskiy et al. | Introduced Vision Transformer (ViT), shifting representation learning for vision beyond CNN-centric designs. |\n"
      ],
      "metadata": {
        "id": "LY7ywg2prPGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How This Table Should Be Read\n",
        "\n",
        "**2009–2012: Foundations**  \n",
        "This period establishes the practical and conceptual feasibility of deep convolutional models. Large-scale datasets, early visualization attempts, and the first successful deep CNNs demonstrate that learned representations can outperform hand-crafted features and scale to real-world vision problems.\n",
        "\n",
        "**2013–2014: CNN Interpretability and Transfer Learning Era**  \n",
        "Research shifts toward understanding *what* CNNs learn and *why* they work. Visualization methods, saliency maps, feature inversion, and transfer learning studies reveal that CNN representations are hierarchical, reusable, and increasingly interpretable across tasks.\n",
        "\n",
        "**2015–2016: Depth, Residual Learning, and Task Maturity**  \n",
        "This phase resolves optimization barriers through residual connections and identity mappings, enabling very deep networks. Detection and segmentation frameworks mature, and multi-scale representations become standard, signaling architectural consolidation.\n",
        "\n",
        "**2017–2018: Explainable AI Rigor and Robustness Analysis**  \n",
        "Interpretability becomes more systematic and critical. Attribution methods gain theoretical grounding, benchmarks emerge, and robustness studies expose fragility and limitations of existing explanation techniques, raising standards for scientific validity.\n",
        "\n",
        "**2020: Paradigm Transition Beyond CNNs**  \n",
        "The introduction of vision transformers marks a conceptual shift away from convolution-centric inductive biases, reframing representation learning around attention and global context, and opening a new phase in visual representation research.\n"
      ],
      "metadata": {
        "id": "VMV9Wc8OrUn4"
      }
    }
  ]
}