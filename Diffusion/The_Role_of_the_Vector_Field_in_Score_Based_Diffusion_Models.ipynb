{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Role of the Vector Field in Score-Based / Diffusion Models\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What “Vector Field” Means?\n",
        "\n",
        "The learned object is\n",
        "\n",
        "$$\n",
        "s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "This is a **vector-valued function**:\n",
        "\n",
        "- **Input:** a point $$x \\in \\mathbb{R}^d$$ (and optionally time or noise level $t$ )\n",
        "\n",
        "- **Output:** a vector in $$\\mathbb{R}^d$$\n",
        "\n",
        "So at every point in space (and time), the model outputs:\n",
        "\n",
        "a direction and a magnitude telling you where probability mass increases fastest.\n",
        "\n",
        "That is the definition of a **vector field**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why the Model Must Be a Vector Field (Not a Scalar)\n",
        "\n",
        "### Classical density models\n",
        "\n",
        "Try to learn a scalar function\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "Must integrate to 1, which leads to a normalization nightmare.\n",
        "\n",
        "### Score-based models\n",
        "\n",
        "Learn **directional information only**.\n",
        "\n",
        "No normalization is required.\n",
        "\n",
        "The only requirement is:\n",
        "\n",
        "input dimension = output dimension.\n",
        "\n",
        "This is why Yang Song explicitly states:\n",
        "\n",
        "“The only requirement on the score-based model is that it should be a vector-valued function with the same input and output dimensionality.”\n",
        "\n",
        "Without the vector field:\n",
        "\n",
        "- No gradients  \n",
        "- No sampling  \n",
        "- No reverse diffusion  \n",
        "- No generative process at all  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. The Vector Field Defines Probability Geometry\n",
        "\n",
        "Think geometrically.\n",
        "\n",
        "- High-density regions are basins  \n",
        "- Low-density regions are hills  \n",
        "\n",
        "The score vector at \\( x \\):\n",
        "\n",
        "- points uphill in log-probability  \n",
        "- is always orthogonal to level sets of \\( p(x) \\)\n",
        "\n",
        "So the vector field:\n",
        "\n",
        "- encodes how probability mass flows  \n",
        "- defines the shape of the distribution without ever writing it down  \n",
        "\n",
        "You never see the density.  \n",
        "You only feel its force field.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Sampling Equals Following the Vector Field\n",
        "\n",
        "### Langevin dynamics\n",
        "\n",
        "$$\n",
        "x_{k+1} = x_k + \\epsilon \\, s_\\theta(x_k) + \\sqrt{2\\epsilon} \\, \\xi_k\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Deterministic part follows the vector field  \n",
        "- Noise enables exploration  \n",
        "\n",
        "Without the vector field:\n",
        "\n",
        "- Noise drifts randomly  \n",
        "- There is no attraction to the data manifold  \n",
        "\n",
        "The vector field is what pulls samples from noise into structure.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Why Low-Density Regions Were Fatal (And Why Vector Fields Fix This)\n",
        "\n",
        "Key failure discovered by the author:\n",
        "\n",
        "Score matching is weighted by\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "The vector field is poorly learned where \\( p(x) \\) is small.\n",
        "\n",
        "But sampling **starts** in low-density regions.\n",
        "\n",
        "This is why naive score fields fail.\n",
        "\n",
        "Noise perturbations combined with time-dependent vector fields ensure:\n",
        "\n",
        "- The vector field is well-defined everywhere  \n",
        "- Sampling trajectories do not collapse  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Vector Field as the Drift of the Reverse-Time SDE\n",
        "\n",
        "In the SDE framework:\n",
        "\n",
        "### Forward process (destruction)\n",
        "\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\,dW_t\n",
        "$$\n",
        "\n",
        "### Reverse process (generation)\n",
        "\n",
        "$$\n",
        "dx =\n",
        "\\left[\n",
        "f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\n",
        "\\right] dt\n",
        "+ g(t)\\, d\\bar{W}_t\n",
        "$$\n",
        "\n",
        "The score vector field is **literally the drift correction term**.\n",
        "\n",
        "Meaning:\n",
        "\n",
        "- The vector field defines the generative dynamics  \n",
        "- Learning the vector field is equivalent to learning how to reverse entropy  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Probability Flow ODE: Vector Field Without Randomness\n",
        "\n",
        "The deterministic ODE formulation is\n",
        "\n",
        "$$\n",
        "dx =\n",
        "\\left[\n",
        "f(x,t) - \\frac{1}{2} g(t)^2 s_\\theta(x,t)\n",
        "\\right] dt\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- The vector field becomes a deterministic velocity field  \n",
        "- The entire generative model is a continuous flow  \n",
        "\n",
        "This is why:\n",
        "\n",
        "- Exact likelihoods become possible  \n",
        "- The model becomes a continuous normalizing flow  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Inverse Problems: Vector Field as a Bayesian Update Engine\n",
        "\n",
        "Bayes’ rule written in score form:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x \\mid y)\n",
        "=\n",
        "\\nabla_x \\log p(x)\n",
        "+\n",
        "\\nabla_x \\log p(y \\mid x)\n",
        "$$\n",
        "\n",
        "So:\n",
        "\n",
        "- Prior vector field plus measurement vector field  \n",
        "- Add them to obtain the posterior vector field  \n",
        "- Sample using the same machinery  \n",
        "\n",
        "This is impossible with scalar density models without retraining.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. The Deepest Insight (Why This Works at All)\n",
        "\n",
        "A vector field:\n",
        "\n",
        "- Does not need to be integrable  \n",
        "- Does not need a closed-form density  \n",
        "- Only needs to be locally correct  \n",
        "\n",
        "Score matching exploits this fact.\n",
        "\n",
        "You never reconstruct\n",
        "\n",
        "$$\n",
        "p(x)\n",
        "$$\n",
        "\n",
        "You only learn how probability wants to move.\n",
        "\n",
        "This is why diffusion models scale to:\n",
        "\n",
        "- \\(1024 \\times 1024\\) images  \n",
        "- Audio waveforms  \n",
        "- Medical reconstruction  \n",
        "- Physics-inspired generation  \n",
        "\n",
        "---\n",
        "\n",
        "## Final One-Sentence Truth\n",
        "\n",
        "In score-based diffusion models, the vector field **is** the probability distribution, expressed not as “what is likely,” but as “where to move.”\n"
      ],
      "metadata": {
        "id": "19arSl2mQrit"
      }
    }
  ]
}