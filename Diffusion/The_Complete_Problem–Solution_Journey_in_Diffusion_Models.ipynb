{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Complete Problem–Solution Journey  \n",
        "## Score-Based and Diffusion Generative Modeling\n",
        "\n",
        "This section presents a complete, logically ordered account of the **problems that historically blocked likelihood-based generative modeling**, and the **precise theoretical solutions** that culminated in modern score-based diffusion models. Each step resolves a concrete failure mode and motivates the next conceptual advance.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Problem: Likelihood-Based Models Require Intractable Normalization\n",
        "\n",
        "### The problem\n",
        "\n",
        "Classical likelihood-based generative models define densities as\n",
        "\n",
        "$$\n",
        "p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}\n",
        "$$\n",
        "\n",
        "where the normalizing constant\n",
        "\n",
        "$$\n",
        "Z_\\theta = \\int e^{-E_\\theta(x)} dx\n",
        "$$\n",
        "\n",
        "is typically **intractable** in high dimensions.\n",
        "\n",
        "This forces practitioners to choose between:\n",
        "\n",
        "- **Restrictive architectures** (normalizing flows, autoregressive models), or  \n",
        "- **Approximate objectives** (VAEs, contrastive divergence),\n",
        "\n",
        "both of which significantly limit modeling flexibility.\n",
        "\n",
        "### The solution\n",
        "\n",
        "Model the **score function** instead of the density:\n",
        "\n",
        "$$\n",
        "s(x) = \\nabla_x \\log p(x)\n",
        "$$\n",
        "\n",
        "Key insight:\n",
        "\n",
        "- The score **does not depend on the normalization constant**\n",
        "- Any free-form neural network can parameterize it\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Tractable learning without computing likelihoods or partition functions.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Problem: Ground-Truth Score Is Unknown\n",
        "\n",
        "### The problem\n",
        "\n",
        "The natural training objective is the Fisher divergence:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_{p(x)} \\left\\| s_\\theta(x) - \\nabla_x \\log p(x) \\right\\|^2\n",
        "$$\n",
        "\n",
        "However, the true score  \n",
        "$$\n",
        "\\nabla_x \\log p(x)\n",
        "$$  \n",
        "is unknown, making direct optimization impossible.\n",
        "\n",
        "### The solution\n",
        "\n",
        "**Score Matching** (Hyvärinen, 2005):\n",
        "\n",
        "- Reformulates the Fisher divergence\n",
        "- Eliminates dependence on the unknown true score\n",
        "- Produces an objective computable from data alone\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Score models can be trained with standard SGD, without adversarial objectives.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Problem: Naive Score Matching Fails in High Dimensions\n",
        "\n",
        "### The problem\n",
        "\n",
        "Score matching weights errors by the data density \\( p(x) \\):\n",
        "\n",
        "- Low-density regions contribute almost nothing to the loss\n",
        "- But sampling *starts* in low-density regions\n",
        "\n",
        "### Consequence\n",
        "\n",
        "Langevin dynamics diverges immediately.  \n",
        "This was the first major empirical failure encountered in practice.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Problem: Inaccurate Scores in Low-Density Regions Break Sampling\n",
        "\n",
        "### The solution\n",
        "\n",
        "Perturb the data with noise:\n",
        "\n",
        "$$\n",
        "x \\sim p(x) \\quad \\Rightarrow \\quad x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
        "$$\n",
        "\n",
        "Effects:\n",
        "\n",
        "- Fills low-density regions\n",
        "- Makes score estimation well-posed everywhere\n",
        "- Stabilizes both training and sampling\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Score estimation becomes globally accurate.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Problem: No Single Noise Level Is Optimal\n",
        "\n",
        "### The problem\n",
        "\n",
        "- Large noise: good coverage, poor fidelity  \n",
        "- Small noise: accurate detail, poor coverage  \n",
        "\n",
        "A single noise scale is fundamentally insufficient.\n",
        "\n",
        "### The solution\n",
        "\n",
        "**Noise Conditional Score Networks (NCSN)**\n",
        "\n",
        "Train models of the form:\n",
        "\n",
        "$$\n",
        "s_\\theta(x, \\sigma_i) \\approx \\nabla_x \\log p_{\\sigma_i}(x)\n",
        "$$\n",
        "\n",
        "Using:\n",
        "\n",
        "- Multiple noise scales\n",
        "- Geometric noise schedules\n",
        "- Joint training over all \\( \\sigma \\)\n",
        "\n",
        "Sampling via **Annealed Langevin Dynamics** gradually reduces noise.\n",
        "\n",
        "### Outcome\n",
        "\n",
        "High-quality samples competitive with GANs.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Problem: Discrete Noise Schedules Are Heuristic\n",
        "\n",
        "### The problem\n",
        "\n",
        "- Finite noise levels lack theoretical elegance\n",
        "- No exact likelihood computation\n",
        "- Difficult to analyze or generalize\n",
        "\n",
        "### The solution\n",
        "\n",
        "Move to **continuous-time noise** using stochastic differential equations (SDEs).\n",
        "\n",
        "Forward diffusion:\n",
        "\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\,dW_t\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Infinitely many noise scales\n",
        "- Smooth diffusion from data to noise\n",
        "\n",
        "Common choices:\n",
        "\n",
        "- Variance Exploding (VE)\n",
        "- Variance Preserving (VP)\n",
        "- sub-VP SDEs\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Continuous theory with improved sample quality.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Problem: How Can an SDE Be Reversed?\n",
        "\n",
        "### The problem\n",
        "\n",
        "The forward SDE destroys information.  \n",
        "A principled reverse process is required.\n",
        "\n",
        "### The solution\n",
        "\n",
        "**Reverse-time SDE** (Anderson, 1982):\n",
        "\n",
        "$$\n",
        "dx = \\left[f(x,t) - g(t)^2 \\nabla_x \\log p_t(x)\\right]dt + g(t)\\,d\\bar{W}_t\n",
        "$$\n",
        "\n",
        "Crucial observation:\n",
        "\n",
        "- The reverse drift depends on the **score**\n",
        "- Exactly what the model learns\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Sampling reduces to solving a reverse-time SDE.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Problem: Reverse SDE Requires Time-Dependent Scores\n",
        "\n",
        "### The solution\n",
        "\n",
        "Train a **time-dependent score model**:\n",
        "\n",
        "$$\n",
        "s_\\theta(x,t) \\approx \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "Training via:\n",
        "\n",
        "- Continuous-time score matching\n",
        "- Weighted Fisher divergence\n",
        "\n",
        "Special case:\n",
        "\n",
        "- Likelihood weighting corresponds to KL minimization\n",
        "\n",
        "### Outcome\n",
        "\n",
        "State-of-the-art likelihoods without maximum likelihood estimation.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Problem: Numerical SDE Solvers Are Slow and Noisy\n",
        "\n",
        "### The solution\n",
        "\n",
        "**Predictor–Corrector Sampling**\n",
        "\n",
        "- Predictor: Euler–Maruyama or higher-order solvers\n",
        "- Corrector: Langevin MCMC using the learned score\n",
        "\n",
        "Key principle:\n",
        "\n",
        "- Only the marginal distributions must be correct\n",
        "- Exact trajectories are unnecessary\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Faster, higher-quality sampling than GANs.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Problem: No Exact Likelihood with Stochastic Sampling\n",
        "\n",
        "### The solution\n",
        "\n",
        "**Probability Flow ODE**\n",
        "\n",
        "Every diffusion SDE admits a deterministic ODE with identical marginals:\n",
        "\n",
        "$$\n",
        "dx = \\left[f(x,t) - \\frac{1}{2} g(t)^2 s_\\theta(x,t)\\right]dt\n",
        "$$\n",
        "\n",
        "Benefits:\n",
        "\n",
        "- Exact likelihood computation\n",
        "- Invertible generative model\n",
        "- Neural ODE framework\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Exact density estimation and continuous normalizing flows.\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Problem: Inverse Problems Usually Require Retraining\n",
        "\n",
        "### The solution\n",
        "\n",
        "Bayesian inference in **score space**:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x \\mid y)\n",
        "=\n",
        "\\nabla_x \\log p(x)\n",
        "+\n",
        "\\nabla_x \\log p(y \\mid x)\n",
        "$$\n",
        "\n",
        "Implications:\n",
        "\n",
        "- Reuse unconditional score\n",
        "- Modify the drift term\n",
        "- Sample from the posterior directly\n",
        "\n",
        "Applications:\n",
        "\n",
        "- MRI\n",
        "- CT\n",
        "- Inpainting\n",
        "- Colorization\n",
        "\n",
        "### Outcome\n",
        "\n",
        "Inverse problem solving with **zero retraining**.\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Problem: Score-Based Models Appeared Separate from Diffusion Models\n",
        "\n",
        "### The solution\n",
        "\n",
        "Unification with diffusion probabilistic models.\n",
        "\n",
        "Key identity:\n",
        "\n",
        "- DDPM ELBO is equivalent to **weighted score matching**\n",
        "- Diffusion models discretize the same underlying SDE\n",
        "\n",
        "Conceptual analogy:\n",
        "\n",
        "- Wave mechanics vs matrix mechanics\n",
        "\n",
        "### Outcome\n",
        "\n",
        "A single unified diffusion framework.\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Remaining Open Challenges\n",
        "\n",
        "| Challenge | Partial Solution | Status |\n",
        "|---------|-----------------|--------|\n",
        "| Sampling speed | Probability flow ODE, distillation | Open |\n",
        "| Discrete data | Latent diffusion, autoencoders | Open |\n",
        "| Memory and compute | Adaptive solvers | Improving |\n",
        "\n",
        "---\n",
        "\n",
        "## Final Synthesis\n",
        "\n",
        "Every major obstacle was resolved by a single unifying principle:\n",
        "\n",
        "**Learn the geometry of probability via its score, then use physics (SDEs) to move through it.**\n",
        "\n",
        "Diffusion models are therefore not merely generative models, but a **geometric, dynamical theory of probability itself**.\n"
      ],
      "metadata": {
        "id": "CgbTFYt3EvtI"
      }
    }
  ]
}