{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Scientific Lineage of Diffusion Models  \n",
        "## From Physical Systems to Modern Generative AI\n",
        "\n",
        "This text presents a scientifically progressive exposition linking diffusion models in physics—equilibrium, non-equilibrium systems, system perturbations, and particle vibrations—to the mathematical pathway that ultimately led to modern diffusion models in artificial intelligence for image and video generation.\n",
        "\n",
        "We present core equations, explain the physical meaning of each, and then show how they were reinterpreted computationally within deep learning–based generative models.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Starting Point: Random Motion and Microscopic Fluctuations\n",
        "\n",
        "### 1.1 Brownian Motion  \n",
        "(Robert Brown, 1827; formalized by Norbert Wiener, 1923)\n",
        "\n",
        "A minimal stochastic model of particle motion under random molecular collisions is:\n",
        "\n",
        "$$\n",
        "dx(t) = \\sqrt{2D}\\, dW_t.\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(x(t)\\): particle position  \n",
        "- \\(D\\): diffusion coefficient (Einstein, 1905)  \n",
        "- \\(W_t\\): Wiener process (idealized white-noise-driven cumulative motion)\n",
        "\n",
        "Physical meaning: this describes memoryless (Markovian) stochastic motion with no deterministic force. It is a prototypical non-equilibrium stochastic system governed purely by randomness.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. The Diffusion Equation (Heat / Mass Transport in Density Form)\n",
        "\n",
        "(Joseph Fourier, 1822; Einstein, 1905)\n",
        "\n",
        "From Brownian motion, one obtains a density evolution equation for the probability of particle location:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p(x,t)}{\\partial t} = D \\nabla^2 p(x,t).\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(p(x,t)\\): probability density of particle location  \n",
        "- \\(\\nabla^2\\): Laplacian operator (spatial diffusion)\n",
        "\n",
        "Physical meaning: probability mass spreads from high concentration to low concentration, capturing heat conduction, molecular diffusion, and gradual convergence to equilibrium-like smoothness.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Introducing Forces: The Langevin Equation\n",
        "\n",
        "(Paul Langevin, 1908)\n",
        "\n",
        "When deterministic forces act on particles, a drift term is added:\n",
        "\n",
        "$$\n",
        "dx = f(x)\\,dt + \\sqrt{2D}\\, dW_t.\n",
        "$$\n",
        "\n",
        "Often, the force is derived from a potential energy \\(U(x)\\):\n",
        "\n",
        "$$\n",
        "f(x) = -\\nabla U(x).\n",
        "$$\n",
        "\n",
        "Physical meaning: this blends structured dynamics (force-driven motion) with thermal noise. It is foundational in non-equilibrium statistical physics and stochastic dynamical systems.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. The Fokker–Planck (Kolmogorov Forward) Equation\n",
        "\n",
        "(Adriaan Fokker, 1914; Max Planck, 1917)\n",
        "\n",
        "The probabilistic evolution corresponding to the Langevin SDE is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p(x,t)}{\\partial t}\n",
        "= -\\nabla \\cdot \\big(f(x)\\,p(x,t)\\big) + D \\nabla^2 p(x,t).\n",
        "$$\n",
        "\n",
        "Physical significance:\n",
        "\n",
        "- It bridges microscopic stochastic dynamics (sample paths) and macroscopic probability flow (density evolution).\n",
        "- Under suitable conditions, it explains convergence toward an equilibrium density of Boltzmann–Gibbs form:\n",
        "\n",
        "$$\n",
        "p_{\\mathrm{eq}}(x) \\propto e^{-U(x)/(kT)}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. The Critical Breakthrough: Reverse-Time Diffusion\n",
        "\n",
        "(Brian D. O. Anderson, 1982)\n",
        "\n",
        "Consider a forward-time Itô diffusion:\n",
        "\n",
        "$$\n",
        "dx = f(x,t)\\,dt + g(t)\\, dW_t,\n",
        "$$\n",
        "\n",
        "with marginal density \\(p(x,t)\\). Anderson’s reverse-time diffusion result gives a reverse-time SDE whose drift must include a density-gradient correction term:\n",
        "\n",
        "$$\n",
        "dx\n",
        "=\n",
        "\\Big[f(x,t) - g(t)^2 \\nabla_x \\log p(x,t)\\Big]\\,dt\n",
        "+ g(t)\\, d\\bar{W}_t.\n",
        "$$\n",
        "\n",
        "The pivotal emergence is the score function:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x,t).\n",
        "$$\n",
        "\n",
        "Physical interpretation:\n",
        "\n",
        "- The score acts as a corrective force that “points” toward higher probability mass.\n",
        "- It makes time reversal mathematically consistent for stochastic diffusions by compensating for the fact that naive time reversal does not preserve the diffusion structure.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Deep Physical Concepts: Entropy, Equilibrium, Reversibility\n",
        "\n",
        "### 6.1 Entropy\n",
        "\n",
        "(Claude Shannon, 1948; related to Boltzmann–Gibbs perspectives)\n",
        "\n",
        "A standard entropy functional is:\n",
        "\n",
        "$$\n",
        "\\mathcal{H}[p] = - \\int p(x)\\log p(x)\\,dx.\n",
        "$$\n",
        "\n",
        "Interpretation in this lineage:\n",
        "\n",
        "- Forward diffusion tends to smooth distributions, aligning with entropy increase (loss of fine detail).\n",
        "- Reverse-time diffusion corresponds to reconstruction of structure, aligning with entropy decrease.\n",
        "- Structure recovery requires directional information encoded in the score \\( \\nabla_x \\log p(x,t) \\).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. The Transition to Artificial Intelligence\n",
        "\n",
        "### 7.1 The Core Insight\n",
        "\n",
        "If we can learn the time-dependent score\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log p(x,t),\n",
        "$$\n",
        "\n",
        "then we can instantiate a reverse diffusion that maps noise back into data-like structure.\n",
        "\n",
        "This is the conceptual bridge from physics to generative modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Diffusion Models in Artificial Intelligence\n",
        "\n",
        "### 8.1 Forward Diffusion (Data Corruption)\n",
        "\n",
        "(Sohl-Dickstein et al., 2015; Ho et al., 2020)\n",
        "\n",
        "A discrete-time forward noising process is commonly written as:\n",
        "\n",
        "$$\n",
        "x_t = \\sqrt{\\alpha_t}\\,x_0 + \\sqrt{1-\\alpha_t}\\,\\varepsilon,\n",
        "\\quad \\varepsilon \\sim \\mathcal{N}(0,I).\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Progressive noise injection is a computational analogue of thermal diffusion.\n",
        "- As \\(t\\) increases, information about \\(x_0\\) is gradually erased into a simple noise distribution.\n",
        "\n",
        "### 8.2 Learning the Score (Score Matching)\n",
        "\n",
        "(Hyvärinen, 2005; Song & Ermon, 2019)\n",
        "\n",
        "Instead of learning the density \\(p(x,t)\\) directly, one learns its gradient field:\n",
        "\n",
        "$$\n",
        "s_\\theta(x,t) \\approx \\nabla_x \\log p(x,t).\n",
        "$$\n",
        "\n",
        "This yields a scalable mechanism for high-dimensional modeling because the score can be approximated by a neural network.\n",
        "\n",
        "### 8.3 Reverse Diffusion (Generation)\n",
        "\n",
        "(Anderson, 1982; Song et al., 2021)\n",
        "\n",
        "Reverse-time generation in continuous-time form is:\n",
        "\n",
        "$$\n",
        "dx\n",
        "=\n",
        "\\Big[f(x,t) - g(t)^2\\, s_\\theta(x,t)\\Big]\\,dt\n",
        "+ g(t)\\, d\\bar{W}_t.\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- The “corrective force” is learned (\\(s_\\theta\\)).\n",
        "- The generative procedure reconstructs structure from noise by following a score-corrected reverse diffusion.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. From Images to Video\n",
        "\n",
        "In video generation:\n",
        "\n",
        "- Time becomes an additional modeling axis.\n",
        "- Diffusion must be consistent across space and time, capturing motion coherence and temporal structure.\n",
        "\n",
        "One can view a video sample as a spatiotemporal field:\n",
        "\n",
        "$$\n",
        "x(\\tau, s), \\quad s = \\text{video time},\n",
        "$$\n",
        "\n",
        "where \\(\\tau\\) denotes diffusion time (the noising / denoising time parameter), and \\(s\\) denotes the intrinsic temporal axis of the video content.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Philosophical Physical Summary\n",
        "\n",
        "| Physics Concept | Artificial Intelligence |\n",
        "|---|---|\n",
        "| Particle | Pixel (or latent element) |\n",
        "| Heat / thermal agitation | Noise injection |\n",
        "| Equilibrium distribution | Clean image / data distribution |\n",
        "| Entropy increase | Detail loss under corruption |\n",
        "| Probability gradient \\( \\nabla_x \\log p \\) | Learned score network \\(s_\\theta\\) |\n",
        "| Time reversal | Generation (denoising from noise) |\n",
        "\n",
        "---\n",
        "\n",
        "## Unifying Statement\n",
        "\n",
        "Diffusion-based generative models are not a new invention, but a mathematical revival of deep physical theories of disorder, equilibrium, and time reversal—recast into computational form by learning the score field and numerically solving reverse-time dynamics.\n"
      ],
      "metadata": {
        "id": "RxpGXR8Xq2v0"
      }
    }
  ]
}