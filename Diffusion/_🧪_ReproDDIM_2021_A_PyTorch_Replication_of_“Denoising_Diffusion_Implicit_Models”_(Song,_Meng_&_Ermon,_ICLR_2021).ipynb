{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 Denoising Diffusion Implicit Models (DDIM) – Summary\n",
        "\n",
        "# https://arxiv.org/pdf/2010.02502\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 🔎 Abstract\n",
        "- Builds upon **DDPMs**, which achieve high-quality image generation without adversarial training.  \n",
        "- Limitation of DDPMs: **slow sampling** (thousands of iterative steps).  \n",
        "- DDIMs generalize DDPMs via **non-Markovian forward processes**, enabling:  \n",
        "  - 🚀 **Faster sampling** (10×–50× speedup).  \n",
        "  - 🌀 **Deterministic generation** (implicit models).  \n",
        "  - 🔄 **Interpolation and reconstruction** abilities.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Introduction\n",
        "- **GANs:** strong image quality, but instability & mode collapse.  \n",
        "- **Likelihood-based models (VAEs, flows, autoregressive):** stable, but fidelity lags.  \n",
        "- **Iterative denoising models (DDPM, NCSN):** close the gap but need long sampling chains.  \n",
        "- **DDIM:** keeps DDPM training, but introduces **more efficient inference** via non-Markovian processes.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Background\n",
        "\n",
        "### Forward Diffusion (DDPM)\n",
        "Gradually adds Gaussian noise:  \n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_0) = \\mathcal{N}(\\alpha_t x_0, (1-\\alpha_t)I).\n",
        "$$  \n",
        "\n",
        "### Reverse Process\n",
        "Trained neural network removes noise step by step.  \n",
        "\n",
        "### Training Loss\n",
        "Simplified objective:  \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\mathbb{E}_{x_0, \\epsilon, t}\n",
        "\\big[\n",
        "\\|\\epsilon - \\epsilon_\\theta(\\alpha_t x_0 + \\sqrt{1-\\alpha_t}\\,\\epsilon, t)\\|^2\n",
        "\\big].\n",
        "$$  \n",
        "\n",
        "Large \\(T \\sim 1000\\) → good approximation but slow sampling.  \n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ Methodology: DDIM\n",
        "- Training objective depends only on **marginals** \\( q(x_t \\mid x_0) \\), not the full joint process.  \n",
        "- Replace **Markovian forward diffusion** with **non-Markovian inference** → new reverse process, same training.  \n",
        "\n",
        "### Deterministic Implicit Models\n",
        "Set variance term \\( \\sigma_t = 0 \\):  \n",
        "\n",
        "$$\n",
        "x_{t-1} = \\sqrt{\\alpha_{t-1}} \\, \\hat{x}_0 \\;+\\;\n",
        "\\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2}\\,\\epsilon_\\theta(x_t, t) \\;+\\; \\sigma_t \\epsilon\n",
        "$$  \n",
        "\n",
        "- \\( \\hat{x}_0 \\): model’s prediction of the original image.  \n",
        "- \\( \\sigma_t = 0 \\): deterministic trajectory (**DDIM**).  \n",
        "- \\( \\sigma_t > 0 \\): stochastic trajectory (**DDPM**).  \n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Benefits\n",
        "- **Efficiency:** 10–50× fewer steps for similar quality.  \n",
        "- **Consistency:** same initial latent \\(x_T\\) → semantically consistent outputs across different trajectory lengths.  \n",
        "- **Interpolation:** smooth latent interpolations directly in \\(x_T\\)-space.  \n",
        "- **Reconstruction:** encode \\(x_0 \\to x_T \\to x_0\\), yielding low reconstruction error.  \n",
        "- **Connection to Neural ODEs:** deterministic DDIM resembles solving an ODE trajectory.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Experiments\n",
        "- **Datasets:** CIFAR-10 (32×32), CelebA (64×64), LSUN Bedrooms/Churches (256×256).  \n",
        "- **Metric:** FID (Fréchet Inception Distance).  \n",
        "\n",
        "### Findings\n",
        "- DDIM achieves **best FID** for short trajectories (20–100 steps).  \n",
        "- Comparable to **1000-step DDPM** using just ~50 steps.  \n",
        "- Interpolations are **smooth and semantically meaningful**.  \n",
        "- Reconstruction error decreases as steps increase.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Key Takeaways\n",
        "- **DDIM = deterministic generalization of DDPM**.  \n",
        "- Same training, but **faster sampling**.  \n",
        "- Bridges gap between **probabilistic diffusion models** and **implicit models** (GANs, flows).  \n",
        "- Opens applications in:  \n",
        "  - Efficient image generation.  \n",
        "  - Latent-space manipulation.  \n",
        "  - Reconstruction.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Core Contribution\n",
        "DDIM reframes diffusion from a **probabilistic Markov chain** into a **flexible non-Markovian process**, enabling:  \n",
        "- Faster sampling.  \n",
        "- Deterministic trajectories.  \n",
        "- Semantically consistent generation.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Reference\n",
        "Song, J., Meng, C., & Ermon, S. (2020).  \n",
        "*Denoising Diffusion Implicit Models.* arXiv preprint arXiv:2010.02502.  \n",
        "[Paper link](https://arxiv.org/abs/2010.02502)  \n"
      ],
      "metadata": {
        "id": "LBMbmIDb0BXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seXSkX8Qz840"
      },
      "outputs": [],
      "source": [
        "# 1) Setup & Imports\n",
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Cfg:\n",
        "    dataset = \"mnist\"    # \"mnist\" or \"cifar10\"\n",
        "    img_size = 32\n",
        "    channels = 1\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "    lr = 2e-4\n",
        "    T = 1000\n",
        "    beta_start = 1e-4\n",
        "    beta_end = 0.02\n",
        "    sample_n = 16\n",
        "    ddim_steps = 50       # faster sampling steps (vs T=1000)\n",
        "cfg = Cfg()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "Rx6FbuMr0f5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(cfg.img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x*2. - 1.)   # scale to [-1,1]\n",
        "])\n",
        "\n",
        "if cfg.dataset==\"mnist\":\n",
        "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "    cfg.channels = 1\n",
        "else:\n",
        "    train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=transform)\n",
        "    cfg.channels = 3\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kdb4XhkL0nT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas = torch.linspace(cfg.beta_start, cfg.beta_end, cfg.T, device=device)\n",
        "alphas = 1. - betas\n",
        "abar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "sqrt_abar = abar.sqrt()\n",
        "sqrt_one_mabar = (1 - abar).sqrt()\n",
        "\n",
        "def q_sample(x0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    return sqrt_abar[t][:,None,None,None]*x0 + sqrt_one_mabar[t][:,None,None,None]*noise, noise\n"
      ],
      "metadata": {
        "id": "qHdFFvX20q8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, c=1):\n",
        "        super().__init__()\n",
        "        self.down1 = nn.Conv2d(c,64,3,padding=1)\n",
        "        self.down2 = nn.Conv2d(64,128,3,stride=2,padding=1)\n",
        "        self.mid   = nn.Conv2d(128,128,3,padding=1)\n",
        "        self.up1   = nn.ConvTranspose2d(128,64,4,stride=2,padding=1)\n",
        "        self.out   = nn.Conv2d(64,c,3,padding=1)\n",
        "    def forward(self,x,t):\n",
        "        h1 = F.relu(self.down1(x))\n",
        "        h2 = F.relu(self.down2(h1))\n",
        "        h3 = F.relu(self.mid(h2))\n",
        "        u1 = F.relu(self.up1(h3))\n",
        "        return self.out(u1)\n"
      ],
      "metadata": {
        "id": "NPnnqe800ue0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleUNet(c=cfg.channels).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "\n",
        "history = {\"train_loss\":[],\"val_loss\":[]}\n",
        "\n",
        "for epoch in range(1,cfg.epochs+1):\n",
        "    model.train(); running=0\n",
        "    for x,_ in tqdm(train_loader,desc=f\"Epoch {epoch}/{cfg.epochs}\"):\n",
        "        x=x.to(device)\n",
        "        t=torch.randint(0,cfg.T,(x.size(0),),device=device)\n",
        "        x_t,noise=q_sample(x,t)\n",
        "        pred=model(x_t,t)\n",
        "        loss=F.mse_loss(pred,noise)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        running+=loss.item()\n",
        "    train_loss=running/len(train_loader)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "    # simple val check\n",
        "    model.eval(); vloss=0\n",
        "    with torch.no_grad():\n",
        "        for x,_ in test_loader:\n",
        "            x=x.to(device)\n",
        "            t=torch.randint(0,cfg.T,(x.size(0),),device=device)\n",
        "            x_t,noise=q_sample(x,t)\n",
        "            pred=model(x_t,t)\n",
        "            vloss+=F.mse_loss(pred,noise).item()\n",
        "    history[\"val_loss\"].append(vloss/len(test_loader))\n",
        "    print(f\"train {train_loss:.4f} val {history['val_loss'][-1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "IWIzuN0D0x_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def ddim_sample(model, n=16, ddim_steps=50):\n",
        "    model.eval()\n",
        "    step_size = cfg.T // ddim_steps\n",
        "    x = torch.randn(n, cfg.channels, cfg.img_size, cfg.img_size, device=device)\n",
        "    for t in reversed(range(0,cfg.T,step_size)):\n",
        "        t_b = torch.full((n,), t, device=device, dtype=torch.long)\n",
        "        eps = model(x, t_b)\n",
        "        abar_t = abar[t]\n",
        "        abar_prev = abar[max(t-step_size,0)]\n",
        "        x0_pred = (x - (1-abar_t).sqrt()*eps)/abar_t.sqrt()\n",
        "        x = abar_prev.sqrt()*x0_pred + (1-abar_prev).sqrt()*eps\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "MsnmMMzv01h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(hist):\n",
        "    plt.plot(hist[\"train_loss\"],label=\"Train\")\n",
        "    plt.plot(hist[\"val_loss\"],label=\"Val\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(); plt.show()\n",
        "\n",
        "def show_samples(model,n=16,ddim_steps=50):\n",
        "    samples=ddim_sample(model,n,ddim_steps).cpu().clamp(-1,1)\n",
        "    grid=utils.make_grid((samples+1)/2,nrow=int(n**0.5))\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(grid.permute(1,2,0))\n",
        "    plt.axis(\"off\"); plt.title(\"DDIM Samples\"); plt.show()\n"
      ],
      "metadata": {
        "id": "1OV-8c-G04-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)\n",
        "show_samples(model,n=16,ddim_steps=cfg.ddim_steps)\n"
      ],
      "metadata": {
        "id": "MLspWhTH08Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Results of ReproDDIM-2021  \n",
        "*A PyTorch Replication of “Denoising Diffusion Implicit Models” (Song, Meng, & Ermon, 2020)*\n",
        "\n",
        "---\n",
        "\n",
        "## 🔎 Training Dynamics\n",
        "- Training and validation losses decreased steadily:  \n",
        "  - **Epoch 1:** ~0.17 (train) / ~0.08 (val)  \n",
        "  - **Epoch 10:** ~0.04 (train) / ~0.04 (val)  \n",
        "- Close tracking of training and validation curves → **no overfitting**.  \n",
        "- Confirms the model is effectively learning the **denoising objective** across diffusion steps.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📉 Loss Curve\n",
        "- Sharp decline in early epochs → fast adaptation to denoising.  \n",
        "- Smooth convergence around **epoch 7–10**, with nearly overlapping curves.  \n",
        "- Indicates **stable training** and **well-calibrated optimization**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🖼️ Sampling Quality\n",
        "- Generated samples show **basic structural formation**, but remain **blurry and fragmented**.  \n",
        "- Reasons:  \n",
        "  - Limited training (**10 epochs vs. hundreds** in the paper).  \n",
        "  - Simplified **U-Net architecture**.  \n",
        "  - Lower dataset resolution.  \n",
        "- Early-stage training emphasizes **likelihood alignment**, not perceptual sharpness.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Interpretation\n",
        "\n",
        "### ✅ Strengths\n",
        "- Loss convergence validates the **noise-prediction objective**.  \n",
        "- Training ≈ validation losses → model **generalizes well**.  \n",
        "\n",
        "### ❌ Limitations\n",
        "- **Poor visual fidelity** due to under-training and reduced model capacity.  \n",
        "- DDIM accelerates sampling, but high-quality outputs require **well-trained denoisers**.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Conclusion\n",
        "- Replication confirms DDIM can **match DDPM training losses** while benefiting from **deterministic implicit sampling**.  \n",
        "- Current sample quality lags the original paper because of:  \n",
        "  - Short training schedule.  \n",
        "  - Smaller architecture and dataset.  \n",
        "- With **longer training** and **full-scale U-Net models**, results should approach the **sharp, high-quality images** reported in DDIM (2020).  \n"
      ],
      "metadata": {
        "id": "InYa1Jpo2C5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 Comparison of Diffusion Models (2015–2021)\n",
        "\n",
        "| **Model** | **Paper & Authors** | **Key Idea** | **Training** | **Sampling** | **Strengths** | **Limitations** |\n",
        "|-----------|----------------------|--------------|--------------|--------------|---------------|-----------------|\n",
        "| **DPMs** | *Deep Unsupervised Learning using Nonequilibrium Thermodynamics*<br>Sohl-Dickstein et al., ICML 2015 | Treat learning as reversal of a diffusion process (forward: add noise, reverse: learn denoising). | Complex, required **variational bounds** and KL minimization. | Very slow (hundreds/thousands of steps). | First formalization of **diffusion generative modeling**; strong theoretical foundation. | Intractable for large-scale images; poor sample quality. |\n",
        "| **DDPM** | *Denoising Diffusion Probabilistic Models*<br>Ho, Jain & Abbeel, NeurIPS 2020 | Reformulated diffusion with simple **noise-prediction (ε-MSE)** objective, inspired by score matching. | Scalable and stable with **U-Net architecture**. | Still slow (typically 1000 steps). | Achieved **SOTA likelihoods and fidelity** on CIFAR-10, CelebA-HQ, LSUN. | Sampling inefficiency; high inference compute cost. |\n",
        "| **DDIM** | *Denoising Diffusion Implicit Models*<br>Song, Meng & Ermon, ICLR 2021 | Introduced **non-Markovian implicit sampling** with deterministic trajectories. | Same training as DDPM (ε-MSE). | Much faster (10–50 steps possible). | Preserves DDPM quality with **accelerated inference**; deterministic generation; latent interpolation. | Some trade-offs in likelihood estimation; requires careful scheduling. |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "- **DPMs (2015):** Theory-heavy, proof-of-concept; impractical for real images.  \n",
        "- **DDPM (2020):** Breakthrough in scalability and fidelity; but sampling is slow.  \n",
        "- **DDIM (2021):** Improved efficiency and control, retaining DDPM’s strong quality.  \n"
      ],
      "metadata": {
        "id": "O0z7uMLG2jFj"
      }
    }
  ]
}