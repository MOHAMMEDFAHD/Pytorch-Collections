{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Model Methodologies — Comprehensive Comparison\n",
        "\n",
        "| Perspective | Jascha Sohl-Dickstein (2015) | Jonathan Ho (DDPM, 2020) | Yang Song (Score-Based, 2019–2021) |\n",
        "|---|---|---|---|\n",
        "| Historical role | Originator of diffusion as a generative probabilistic process | Made diffusion practical and scalable with deep learning | Unified diffusion, score matching, and SDEs |\n",
        "| Primary contribution | Showed diffusion processes can be reversed | Reframed diffusion as denoising regression | Reframed generation as learning a vector field |\n",
        "| Core question | Can entropy increase be reversed? | Can denoising be learned efficiently? | Can we learn ∇ log p(x, t) directly? |\n",
        "| View of probability | Explicit probabilistic modeling | Implicit via denoising | Probability as geometry (flow field) |\n",
        "| Mathematical object learned | Reverse transition kernel | Noise ε or clean data x₀ | Score function ∇ₓ log pₜ(x) |\n",
        "| Learning target | Reverse diffusion transitions | Conditional denoising | Denoising score matching |\n",
        "| Learning paradigm | Variational inference | Supervised regression | Score matching |\n",
        "| Forward process | Continuous-time diffusion | Discrete Gaussian noising | Continuous SDE |\n",
        "| Reverse process | Learned stochastic reverse diffusion | Iterative learned denoising | Reverse-time SDE or probability-flow ODE |\n",
        "| Time formulation | Continuous | Discrete | Continuous |\n",
        "| Noise schedule | Theoretically defined | Hand-crafted βₜ schedules | Arbitrary continuous noise levels |\n",
        "| Sampling interpretation | Reverse Markov chain | Iterative denoising | Solving differential equations |\n",
        "| Sampling type | Stochastic | Stochastic | Stochastic (SDE) or deterministic (ODE) |\n",
        "| Generative intuition | Undo entropy | Remove noise step by step | Follow probability mass flow |\n",
        "| Local vs global view | Local transitions | Local denoising | Global vector field |\n",
        "| Relation to score matching | Implicit | Implicit (discrete) | Explicit and central |\n",
        "| Relation to Langevin dynamics | Indirect | Weak | Direct and principled |\n",
        "| Likelihood computation | Theoretically tractable | Approximate (ELBO) | Exact via probability-flow ODE |\n",
        "| Engineering simplicity | Low | Very high | Medium |\n",
        "| Theoretical elegance | Very high | Medium | Very high |\n",
        "| Computational cost | High | High (many steps) | Flexible (accuracy–speed tradeoff) |\n",
        "| Typical architectures | Generic neural networks | U-Net CNNs | CNNs or Transformers |\n",
        "| Scalability | Limited (early era) | Excellent | Excellent |\n",
        "| Unifying power | Low | Medium | Very high |\n",
        "| Conceptual lens | Thermodynamics | Denoising | Differential geometry |\n",
        "| Mental model | Reverse diffusion | Predict the noise | Learn the force field |\n",
        "| Data interpretation | Samples in probability space | Noisy images | Particles moving in a flow |\n",
        "| Why it works | Gradual entropy control | Easier optimization | Smooth probability manifolds |\n",
        "| Modern descendants | DDPMs | Latent diffusion, Imagen | Flow matching, rectified flows |\n",
        "| Best described as | Probabilistic foundation | Practical algorithm | Generative dynamics framework |\n"
      ],
      "metadata": {
        "id": "G9v8WTArsgGW"
      }
    }
  ]
}