{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNT/XLOifkQIKa5RHGvglju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# “Long Short-Term Memory-Networks for Machine Reading”  \n","*Cheng, Dong, and Lapata*\n","\n","# # https://arxiv.org/pdf/1601.06733\n","\n","---\n","\n","## Abstract\n","The paper introduces Long Short-Term Memory-Networks (LSTMN), a reading simulator that enhances standard LSTMs by replacing the single memory cell with a memory network and incorporating neural attention. This design enables adaptive memory usage, weak induction of token relations, and improved handling of structured input. LSTMN is validated on language modeling, sentiment analysis, and natural language inference, achieving results on par with or surpassing state-of-the-art baselines.\n","\n","---\n","\n","## Problems\n","1. Vanishing/Exploding Gradients – Training RNNs over long sequences remains difficult despite gating mechanisms.  \n","2. Memory Compression – Standard LSTMs compress entire sequences into a single dense vector, limiting representation for long sequences.  \n","3. Lack of Structural Awareness – Sequence-level models fail to capture relations and latent structures inherent in natural language.\n","\n","---\n","\n","## Proposed Solutions\n","- Dedicated Memory Slots: Each token is stored in its own memory slot rather than being compressed into one vector.  \n","- Intra-Attention: Within recurrence, the model links current tokens with relevant past tokens.  \n","- Multi-Sequence Extension:  \n","  - Shallow Fusion: Intra-attention within encoder/decoder plus standard inter-attention.  \n","  - Deep Fusion: Inter-alignment vectors recurrently stored in target memory.\n","\n","---\n","\n","## Purpose\n","To build a general-purpose machine reading simulator that:  \n","- Incrementally processes text like humans.  \n","- Memorizes longer contexts.  \n","- Discovers latent relations.  \n","- Reasons over shallow structures without explicit supervision.  \n","\n","---\n","\n","## Methodology\n","- Architecture: Replace the LSTM memory cell with a memory tape; use intra-attention to compute token-to-token relations.  \n","\n","$$ h_t = f\\left(W_h h_{t-1} + W_x x_t + b\\right) \\quad \\Rightarrow \\quad \\text{enhanced with memory + attention} $$\n","\n","- Sequence-to-Sequence Extension:  \n","  - Shallow Fusion: intra-attention plus inter-attention.  \n","  - Deep Fusion: store inter-alignment vectors in target memory.  \n","\n","- Tasks and Datasets:  \n","  - Language Modeling: Penn Treebank.  \n","  - Sentiment Analysis: Stanford Sentiment Treebank.  \n","  - Natural Language Inference: Stanford NLI dataset.  \n","\n","---\n","\n","## Results\n","- Language Modeling:  \n","  - LSTMN achieved the lowest perplexity.  \n","  - Single-layer: PPL = 108 (vs LSTM = 115).  \n","  - Three-layer: PPL = 102 (vs Gated-Feedback LSTM = 107).  \n","\n","- Sentiment Analysis:  \n","  - Fine-grained: 47.9% (vs LSTM = 46.4%).  \n","  - Binary: 87.0% (vs LSTM = 84.9%).  \n","  - Comparable to strong CNN models.  \n","\n","- Natural Language Inference:  \n","  - LSTMN with deep fusion achieved 86.3% accuracy, surpassing LSTM attention-based models.  \n","\n","---\n","\n","## Conclusions\n","- LSTMN addresses memory compression and structural reasoning limitations of LSTMs.  \n","- The intra-attention mechanism induces soft, undirected lexical relations, improving text comprehension.  \n","- The approach of integrating memory and attention is general and extendable beyond LSTMs.  \n","\n","Future Directions:  \n","- Structured parsing.  \n","- Relation extraction.  \n","- Discovering compositionality with weak supervision.  \n","\n","---\n"],"metadata":{"id":"Wh7JMl7JwBhS"}},{"cell_type":"markdown","source":["# Mathematical and Statistical Content Summary  \n","*“Long Short-Term Memory-Networks for Machine Reading” — Cheng, Dong, and Lapata*\n","\n","---\n","\n","## 1. Standard LSTM Equations\n","\n","The LSTM updates its memory and hidden states using gates:\n","\n","$$\n","\\begin{bmatrix}\n","i_t \\\\ f_t \\\\ o_t \\\\ \\hat{c}_t\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","\\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh\n","\\end{bmatrix}\n","W \\cdot [h_{t-1}, x_t]\n","$$\n","\n","- $i_t$ → input gate (controls how much new information enters memory).  \n","- $f_t$ → forget gate (controls how much past memory is erased).  \n","- $o_t$ → output gate (controls how much memory is revealed).  \n","- $\\hat{c}_t$ → candidate memory update.  \n","\n","Memory and hidden state updates:\n","\n","$$\n","c_t = f_t \\odot c_{t-1} + i_t \\odot \\hat{c}_t\n","$$\n","\n","$$\n","h_t = o_t \\odot \\tanh(c_t)\n","$$\n","\n","This mechanism balances remembering, forgetting, and exposing information.\n","\n","---\n","\n","## 2. LSTMN Intra-Attention Equations\n","\n","LSTMN replaces a single compressed memory with multiple slots and uses attention to decide which past tokens matter.\n","\n","Attention score for past token $i$:\n","\n","$$\n","a_{it} = v^T \\tanh(W_h h_i + W_x x_t + W_{\\tilde{h}} \\tilde{h}_{t-1})\n","$$\n","\n","Softmax normalization:\n","\n","$$\n","s_{it} = \\text{softmax}(a_{it})\n","$$\n","\n","Weighted memory and hidden summaries:\n","\n","$$\n","\\begin{bmatrix}\n","\\tilde{h}_t \\\\ \\tilde{c}_t\n","\\end{bmatrix}\n","= \\sum_{i=1}^{t-1} s_{it}\n","\\begin{bmatrix}\n","h_i \\\\ c_i\n","\\end{bmatrix}\n","$$\n","\n","Updated gates:\n","\n","$$\n","\\begin{bmatrix}\n","i_t \\\\ f_t \\\\ o_t \\\\ \\hat{c}_t\n","\\end{bmatrix}\n","=\n","\\begin{bmatrix}\n","\\sigma \\\\ \\sigma \\\\ \\sigma \\\\ \\tanh\n","\\end{bmatrix}\n","W \\cdot [\\tilde{h}_t, x_t]\n","$$\n","\n","State updates:\n","\n","$$\n","c_t = f_t \\odot \\tilde{c}_t + i_t \\odot \\hat{c}_t\n","$$\n","\n","$$\n","h_t = o_t \\odot \\tanh(c_t)\n","$$\n","\n","This allows the model to reference multiple past states instead of just the previous one.\n","\n","---\n","\n","## 3. Multi-layer (Stacked) Attention\n","\n","For stacked layers:\n","\n","$$\n","a_{it}^{k+1} = v^T \\tanh(W_h h_i^{k+1} + W_l h_t^k + W_{\\tilde{h}} \\tilde{h}_{t-1}^{k+1})\n","$$\n","\n","This lets higher layers refine attention using deeper representations.\n","\n","---\n","\n","## 4. Inter-Attention (Two Sequences)\n","\n","For encoder-decoder tasks:\n","\n","$$\n","b_{jt} = u^T \\tanh(W_\\gamma \\gamma_j + W_x x_t + W_{\\tilde{\\gamma}} \\tilde{\\gamma}_{t-1})\n","$$\n","\n","$$\n","p_{jt} = \\text{softmax}(b_{jt})\n","$$\n","\n","$$\n","\\begin{bmatrix}\n","\\tilde{\\gamma}_t \\\\ \\tilde{\\alpha}_t\n","\\end{bmatrix}\n","=\n","\\sum_{j=1}^{m} p_{jt}\n","\\begin{bmatrix}\n","\\gamma_j \\\\ \\alpha_j\n","\\end{bmatrix}\n","$$\n","\n","This aligns decoder tokens with encoder tokens.\n","\n","---\n","\n","## 5. Fusion Mechanism\n","\n","Deep fusion integrates inter-attention with intra-sequence updates:\n","\n","$$\n","r_t = \\sigma(W_r \\cdot [\\tilde{\\gamma}_t, x_t])\n","$$\n","\n","$$\n","c_t = r_t \\odot \\tilde{\\alpha}_t + f_t \\odot \\tilde{c}_t + i_t \\odot \\hat{c}_t\n","$$\n","\n","$$\n","h_t = o_t \\odot \\tanh(c_t)\n","$$\n","\n","This merges encoder–decoder alignment with intra-sequence reasoning.\n","\n","---\n","\n","## 6. Evaluation Metrics\n","\n","**Perplexity (PPL):**\n","\n","$$\n","PPL = \\exp\\left(\\frac{NLL}{T}\\right)\n","$$\n","\n","- $NLL$ = negative log-likelihood of test set.  \n","- $T$ = number of tokens.  \n","- Lower perplexity = better predictive performance.\n","\n","---\n","\n","## 7. Optimization Methods\n","\n","- **Stochastic Gradient Descent (SGD):** with learning rate decay and gradient clipping.  \n","- **Adam Optimizer:** adaptive, momentum-based optimization.  \n","- **Dropout:** regularization to reduce overfitting.\n","\n","---\n","\n","## 8. Statistical Results\n","\n","- **Penn Treebank (Language Modeling):**  \n","  LSTM PPL = 115 → LSTMN PPL = 102 (3-layer).  \n","\n","- **Sentiment Analysis (SST):**  \n","  Accuracy improved over LSTMs and approached CNN performance.  \n","\n","- **Natural Language Inference (SNLI):**  \n","  LSTMN deep fusion → 86.3% accuracy, competitive with best attention-based models.  \n","\n","---\n","\n","## Final Takeaway\n","\n","The mathematical foundation of the paper is built on:  \n","\n","- Standard LSTM recurrence equations.  \n","- Intra-attention for linking tokens within a sequence.  \n","- Inter-attention for aligning across sequences.  \n","- Fusion mechanisms for combining the two attentions.  \n","- Perplexity as the main evaluation metric.  \n","- Optimization with SGD/Adam and regularization via dropout.  \n","\n","Together, these methods enable LSTMN to improve memory usage, capture structure, and achieve state-of-the-art performance in multiple NLP tasks.\n"],"metadata":{"id":"4q5lkB9UxSXi"}},{"cell_type":"markdown","source":["# Full System Flow\n","\n","```\n","Input Sequence x1...xn\n","        │\n","        ▼\n"," ┌─────────────┐\n"," │  LSTMN Core │   ← stores token-wise memory slots\n"," └─────────────┘\n","        │\n","        ▼\n"," ┌─────────────┐\n"," │ Intra-Attn  │   ← finds relations among tokens\n"," └─────────────┘\n","        │\n","        ▼\n"," ┌─────────────┐\n"," │ Encoder     │\n"," └─────────────┘\n","        │\n","        ▼\n"," ┌─────────────┐\n"," │ Decoder     │\n"," │  - Intra-attn over target\n"," │  - Inter-attn over source\n"," │  - Fusion (shallow or deep)\n"," └─────────────┘\n","        │\n","        ▼\n","  Output Sequence (translation, inference, etc.)\n","```"],"metadata":{"id":"-YjBMI3ixpgI"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAcExRH6wAtE","executionInfo":{"status":"ok","timestamp":1758752524473,"user_tz":-480,"elapsed":11965,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"edf5a629-7e8a-435d-8514-eb056d2c5486"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss=8.7238\n","Epoch 2, Loss=7.9351\n","Epoch 3, Loss=6.8637\n","Epoch 4, Loss=5.4413\n","Epoch 5, Loss=5.4554\n","Epoch 6, Loss=4.7811\n","Epoch 7, Loss=4.4558\n","Epoch 8, Loss=4.2947\n","Epoch 9, Loss=4.1061\n","Epoch 10, Loss=3.3352\n","Epoch 11, Loss=3.2487\n","Epoch 12, Loss=3.4155\n","Epoch 13, Loss=4.7039\n","Epoch 14, Loss=3.0020\n","Epoch 15, Loss=3.7901\n","Epoch 16, Loss=3.8352\n","Epoch 17, Loss=2.7812\n","Epoch 18, Loss=3.1384\n","Epoch 19, Loss=3.4388\n","Epoch 20, Loss=3.0954\n","Epoch 21, Loss=2.5091\n","Epoch 22, Loss=2.3468\n","Epoch 23, Loss=2.5982\n","Epoch 24, Loss=3.1085\n","Epoch 25, Loss=2.7143\n","Epoch 26, Loss=2.6720\n","Epoch 27, Loss=2.1767\n","Epoch 28, Loss=1.9859\n","Epoch 29, Loss=2.6051\n","Epoch 30, Loss=2.5834\n","Epoch 31, Loss=2.0447\n","Epoch 32, Loss=3.0147\n","Epoch 33, Loss=2.2219\n","Epoch 34, Loss=3.9671\n","Epoch 35, Loss=2.8967\n","Epoch 36, Loss=2.4293\n","Epoch 37, Loss=3.1520\n","Epoch 38, Loss=2.6283\n","Epoch 39, Loss=2.4988\n","Epoch 40, Loss=2.6665\n","Epoch 41, Loss=1.6662\n","Epoch 42, Loss=3.9601\n","Epoch 43, Loss=2.6165\n","Epoch 44, Loss=3.5123\n","Epoch 45, Loss=2.8546\n","Epoch 46, Loss=2.3843\n","Epoch 47, Loss=2.4685\n","Epoch 48, Loss=2.6547\n","Epoch 49, Loss=3.7775\n","Epoch 50, Loss=3.0684\n","Epoch 51, Loss=2.1883\n","Epoch 52, Loss=1.6873\n","Epoch 53, Loss=3.0686\n","Epoch 54, Loss=2.4628\n","Epoch 55, Loss=3.4912\n","Epoch 56, Loss=2.2309\n","Epoch 57, Loss=2.4637\n","Epoch 58, Loss=2.5098\n","Epoch 59, Loss=2.0580\n","Epoch 60, Loss=3.1205\n","Epoch 61, Loss=1.7112\n","Epoch 62, Loss=2.2417\n","Epoch 63, Loss=2.7969\n","Epoch 64, Loss=3.1768\n","Epoch 65, Loss=2.8383\n","Epoch 66, Loss=2.5836\n","Epoch 67, Loss=3.0636\n","Epoch 68, Loss=1.6838\n","Epoch 69, Loss=3.2003\n","Epoch 70, Loss=2.8680\n","Epoch 71, Loss=2.3938\n","Epoch 72, Loss=3.2770\n","Epoch 73, Loss=2.7425\n","Epoch 74, Loss=2.1896\n","Epoch 75, Loss=3.5402\n","Epoch 76, Loss=3.3743\n","Epoch 77, Loss=2.7043\n","Epoch 78, Loss=2.7483\n","Epoch 79, Loss=1.8123\n","Epoch 80, Loss=1.9854\n","Epoch 81, Loss=1.9049\n","Epoch 82, Loss=2.7328\n","Epoch 83, Loss=3.1328\n","Epoch 84, Loss=2.6382\n","Epoch 85, Loss=2.7131\n","Epoch 86, Loss=2.5833\n","Epoch 87, Loss=1.5919\n","Epoch 88, Loss=2.7958\n","Epoch 89, Loss=2.8079\n","Epoch 90, Loss=2.4381\n","Epoch 91, Loss=3.2658\n","Epoch 92, Loss=2.8735\n","Epoch 93, Loss=2.6816\n","Epoch 94, Loss=2.2694\n","Epoch 95, Loss=2.0730\n","Epoch 96, Loss=2.8183\n","Epoch 97, Loss=3.6414\n","Epoch 98, Loss=2.5128\n","Epoch 99, Loss=2.7673\n","Epoch 100, Loss=2.7710\n","Epoch 101, Loss=1.5872\n","Epoch 102, Loss=2.6215\n","Epoch 103, Loss=3.5397\n","Epoch 104, Loss=1.6331\n","Epoch 105, Loss=2.1718\n","Epoch 106, Loss=2.5801\n","Epoch 107, Loss=1.5723\n","Epoch 108, Loss=2.5566\n","Epoch 109, Loss=4.3915\n","Epoch 110, Loss=2.7358\n","Epoch 111, Loss=2.3899\n","Epoch 112, Loss=2.2213\n","Epoch 113, Loss=2.2113\n","Epoch 114, Loss=2.7884\n","Epoch 115, Loss=2.3949\n","Epoch 116, Loss=2.0332\n","Epoch 117, Loss=2.5947\n","Epoch 118, Loss=3.1958\n","Epoch 119, Loss=1.9858\n","Epoch 120, Loss=2.8529\n","Epoch 121, Loss=3.0100\n","Epoch 122, Loss=3.2145\n","Epoch 123, Loss=2.2057\n","Epoch 124, Loss=2.1985\n","Epoch 125, Loss=2.7266\n","Epoch 126, Loss=2.6533\n","Epoch 127, Loss=2.4349\n","Epoch 128, Loss=2.7244\n","Epoch 129, Loss=1.7654\n","Epoch 130, Loss=2.7216\n","Epoch 131, Loss=2.5528\n","Epoch 132, Loss=2.4910\n","Epoch 133, Loss=1.7351\n","Epoch 134, Loss=1.7981\n","Epoch 135, Loss=2.4239\n","Epoch 136, Loss=4.0887\n","Epoch 137, Loss=1.7136\n","Epoch 138, Loss=2.5754\n","Epoch 139, Loss=3.7196\n","Epoch 140, Loss=2.4368\n","Epoch 141, Loss=2.8765\n","Epoch 142, Loss=2.4862\n","Epoch 143, Loss=2.7328\n","Epoch 144, Loss=1.5619\n","Epoch 145, Loss=3.3835\n","Epoch 146, Loss=2.4543\n","Epoch 147, Loss=2.9759\n","Epoch 148, Loss=1.6904\n","Epoch 149, Loss=1.6636\n","Epoch 150, Loss=2.3319\n","Epoch 151, Loss=2.3628\n","Epoch 152, Loss=1.8875\n","Epoch 153, Loss=3.1895\n","Epoch 154, Loss=3.1853\n","Epoch 155, Loss=2.1568\n","Epoch 156, Loss=1.6376\n","Epoch 157, Loss=2.4920\n","Epoch 158, Loss=2.4306\n","Epoch 159, Loss=2.5945\n","Epoch 160, Loss=2.6775\n","Epoch 161, Loss=2.6476\n","Epoch 162, Loss=3.2301\n","Epoch 163, Loss=1.5301\n","Epoch 164, Loss=2.4474\n","Epoch 165, Loss=2.0433\n","Epoch 166, Loss=1.5207\n","Epoch 167, Loss=5.7294\n","Epoch 168, Loss=4.5667\n","Epoch 169, Loss=2.2727\n","Epoch 170, Loss=2.0470\n","Epoch 171, Loss=2.3921\n","Epoch 172, Loss=2.3280\n","Epoch 173, Loss=1.9744\n","Epoch 174, Loss=3.7324\n","Epoch 175, Loss=1.7411\n","Epoch 176, Loss=2.0161\n","Epoch 177, Loss=3.0592\n","Epoch 178, Loss=2.6295\n","Epoch 179, Loss=1.8550\n","Epoch 180, Loss=2.5156\n","Epoch 181, Loss=2.7652\n","Epoch 182, Loss=1.6649\n","Epoch 183, Loss=2.3586\n","Epoch 184, Loss=2.0700\n","Epoch 185, Loss=2.6423\n","Epoch 186, Loss=2.4806\n","Epoch 187, Loss=2.4442\n","Epoch 188, Loss=2.9058\n","Epoch 189, Loss=2.9519\n","Epoch 190, Loss=2.1018\n","Epoch 191, Loss=3.0078\n","Epoch 192, Loss=2.3593\n","Epoch 193, Loss=2.4316\n","Epoch 194, Loss=3.3055\n","Epoch 195, Loss=2.2912\n","Epoch 196, Loss=2.2719\n","Epoch 197, Loss=2.1710\n","Epoch 198, Loss=3.6440\n","Epoch 199, Loss=2.1666\n","Epoch 200, Loss=2.8781\n"]}],"source":["# ============================================================\n","# Educational PyTorch Implementation of LSTMN (Cheng et al.)\n","# Task: Translation (Toy Dataset EN → FR)\n","# ============================================================\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import Counter\n","import random, re\n","\n","# ------------------------------------------------------------\n","# 1. Toy parallel dataset (English → French)\n","# ------------------------------------------------------------\n","train_data = [\n","    (\"i love this movie\", \"j aime ce film\"),\n","    (\"the film was boring\", \"le film etait ennuyeux\"),\n","    (\"wonderful acting\", \"jeu merveilleux\"),\n","    (\"terrible movie\", \"film terrible\"),\n","    (\"a masterpiece\", \"un chef d oeuvre\")\n","]\n","\n","dev_data = [\n","    (\"i hated this\", \"j ai deteste cela\"),\n","    (\"great story\", \"bonne histoire\")\n","]\n","\n","# ------------------------------------------------------------\n","# 2. Tokenizer & vocab utils\n","# ------------------------------------------------------------\n","def simple_tokenizer(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-z0-9àâçéèêëîïôûùüÿñæœ]+\", \" \", text)\n","    return text.strip().split()\n","\n","def build_vocab(dataset, side=\"src\", min_freq=1):\n","    counter = Counter()\n","    for src, tgt in dataset:\n","        text = src if side == \"src\" else tgt\n","        counter.update(simple_tokenizer(text))\n","    vocab = {\"<pad>\":0,\"<unk>\":1,\"<sos>\":2,\"<eos>\":3}\n","    for w,f in counter.items():\n","        if f>=min_freq: vocab[w]=len(vocab)\n","    return vocab\n","\n","src_vocab = build_vocab(train_data,\"src\")\n","tgt_vocab = build_vocab(train_data,\"tgt\")\n","PAD_IDX, SOS_IDX, EOS_IDX = tgt_vocab[\"<pad>\"], tgt_vocab[\"<sos>\"], tgt_vocab[\"<eos>\"]\n","\n","def numericalize(text, vocab, max_len=8, add_eos=False):\n","    toks = simple_tokenizer(text)\n","    ids = [vocab.get(tok,vocab[\"<unk>\"]) for tok in toks]\n","    if add_eos: ids.append(EOS_IDX)\n","    ids = ids[:max_len]\n","    ids += [PAD_IDX]*(max_len-len(ids))\n","    return ids\n","\n","# ------------------------------------------------------------\n","# 3. LSTMN Cell (intra-attention)\n","# ------------------------------------------------------------\n","class LSTMNCell(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","        super().__init__()\n","        self.hidden_dim=hidden_dim\n","        self.gates=nn.Linear(input_dim+hidden_dim,4*hidden_dim)\n","        self.attn_w_h=nn.Linear(hidden_dim,hidden_dim,bias=False)\n","        self.attn_w_x=nn.Linear(input_dim,hidden_dim,bias=False)\n","        self.attn_w_prev=nn.Linear(hidden_dim,hidden_dim,bias=False)\n","        self.attn_v=nn.Linear(hidden_dim,1,bias=False)\n","    def forward(self,x_t,H,C):\n","        if len(H)==0:\n","            h_prev=torch.zeros(x_t.size(0),self.hidden_dim,device=x_t.device)\n","            c_prev=torch.zeros(x_t.size(0),self.hidden_dim,device=x_t.device)\n","            H,C=[h_prev],[c_prev]\n","        h_stack=torch.stack(H,dim=1); c_stack=torch.stack(C,dim=1)\n","        attn_scores=self.attn_v(torch.tanh(\n","            self.attn_w_h(h_stack)+\n","            self.attn_w_x(x_t).unsqueeze(1)+\n","            self.attn_w_prev(H[-1]).unsqueeze(1)\n","        )).squeeze(-1)\n","        attn_w=F.softmax(attn_scores,dim=1)\n","        h_tilde=torch.bmm(attn_w.unsqueeze(1),h_stack).squeeze(1)\n","        c_tilde=torch.bmm(attn_w.unsqueeze(1),c_stack).squeeze(1)\n","        gates=self.gates(torch.cat([x_t,h_tilde],dim=1))\n","        i,f,o,c_hat=gates.chunk(4,dim=1)\n","        i,f,o=torch.sigmoid(i),torch.sigmoid(f),torch.sigmoid(o)\n","        c_hat=torch.tanh(c_hat)\n","        c_t=f*c_tilde+i*c_hat\n","        h_t=o*torch.tanh(c_t)\n","        return h_t,c_t,H+[h_t],C+[c_t]\n","\n","# ------------------------------------------------------------\n","# 4. Encoder–Decoder with LSTMN\n","# ------------------------------------------------------------\n","class Encoder(nn.Module):\n","    def __init__(self,vocab_size,embed_dim,hidden_dim):\n","        super().__init__()\n","        self.embed=nn.Embedding(vocab_size,embed_dim)\n","        self.cell=LSTMNCell(embed_dim,hidden_dim)\n","    def forward(self,src):\n","        H,C=[],[]\n","        for t in range(src.size(1)):\n","            h,c,H,C=self.cell(self.embed(src[:,t]),H,C)\n","        return H,C\n","\n","class Decoder(nn.Module):\n","    def __init__(self,vocab_size,embed_dim,hidden_dim):\n","        super().__init__()\n","        self.embed=nn.Embedding(vocab_size,embed_dim)\n","        self.cell=LSTMNCell(embed_dim,hidden_dim)\n","        self.fc=nn.Linear(hidden_dim,vocab_size)\n","    def forward(self,tgt,H,C,teacher_forcing_ratio=0.5):\n","        B,T=tgt.shape; outputs=[]\n","        h,c,H_d,C_d=[],[],[],[]\n","        x=self.embed(torch.full((B,),SOS_IDX,dtype=torch.long,device=tgt.device))\n","        for t in range(T):\n","            h,c,H_d,C_d=self.cell(x,H_d,C_d)\n","            out=self.fc(h); outputs.append(out.unsqueeze(1))\n","            teacher_force=random.random()<teacher_forcing_ratio\n","            top1=out.argmax(1)\n","            inp=tgt[:,t] if teacher_force else top1\n","            x=self.embed(inp)\n","        return torch.cat(outputs,dim=1)\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self,enc,dec):\n","        super().__init__(); self.enc=enc; self.dec=dec\n","    def forward(self,src,tgt):\n","        H,C=self.enc(src)\n","        return self.dec(tgt,H,C)\n","\n","# ------------------------------------------------------------\n","# 5. Training\n","# ------------------------------------------------------------\n","device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n","enc=Encoder(len(src_vocab),32,64)\n","dec=Decoder(len(tgt_vocab),32,64)\n","model=Seq2Seq(enc,dec).to(device)\n","optimizer=optim.Adam(model.parameters(),lr=0.01)\n","criterion=nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","def batchify(data,batch_size=2,shuffle=True):\n","    if shuffle: random.shuffle(data)\n","    for i in range(0,len(data),batch_size):\n","        batch=data[i:i+batch_size]\n","        src=[numericalize(src,src_vocab,add_eos=True) for src,_ in batch]\n","        tgt=[numericalize(tgt,tgt_vocab,add_eos=True) for _,tgt in batch]\n","        yield torch.tensor(src).to(device),torch.tensor(tgt).to(device)\n","\n","for epoch in range(200):\n","    model.train(); total_loss=0\n","    for src,tgt in batchify(train_data):\n","        optimizer.zero_grad()\n","        out=model(src,tgt[:,:-1])   # predict next tokens\n","        loss=criterion(out.reshape(-1,out.size(-1)),tgt[:,1:].reshape(-1))\n","        loss.backward(); optimizer.step()\n","        total_loss+=loss.item()\n","    print(f\"Epoch {epoch+1}, Loss={total_loss:.4f}\")"]},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# 6. Translation Demo\n","# ------------------------------------------------------------\n","def translate(sentence,max_len=8):\n","    model.eval()\n","    src=torch.tensor([numericalize(sentence,src_vocab,add_eos=True)]).to(device)\n","    H,C=model.enc(src)\n","    x=torch.full((1,),SOS_IDX,dtype=torch.long,device=device)\n","    H_d,C_d=[],[]\n","    outputs=[]\n","    for _ in range(max_len):\n","        x_emb=model.dec.embed(x)\n","        h,c,H_d,C_d=model.dec.cell(x_emb,H_d,C_d)\n","        out=model.dec.fc(h)\n","        top1=out.argmax(1)\n","        if top1.item()==EOS_IDX: break\n","        outputs.append(top1.item()); x=top1\n","    inv_vocab={i:w for w,i in tgt_vocab.items()}\n","    return \" \".join(inv_vocab.get(idx,\"?\") for idx in outputs)\n","\n","print(\"EN: i love this movie\")\n","print(\"FR:\", translate(\"i love this movie\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDck-5VIyKii","executionInfo":{"status":"ok","timestamp":1758752524648,"user_tz":-480,"elapsed":17,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"f29f7aa5-5a00-48d4-c3cd-ad740aab6f5e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["EN: i love this movie\n","FR: merveilleux\n"]}]},{"cell_type":"markdown","source":["# Problem–Solution Mapping: “Long Short-Term Memory-Networks for Machine Reading”  \n","*Cheng, Dong, and Lapata*\n","\n","---\n","\n","| Problem / Research Gap | How It Limits Prior Work | Paper’s Proposed Solution |\n","|-------------------------|--------------------------|----------------------------|\n","| **Memory compression in sequence models**: single-vector compression is insufficient for long sequences. | Leads to poor generalization on long inputs and inefficient use of memory on short ones. Prior LSTMs must fit all past context into one dense state. | Replace the single LSTM cell with a **memory network (memory tape)** that stores a slot per token; use **attention** to read adaptively from past slots, avoiding over-compression. |\n","| **Lack of structural inductive bias**: sequence models process tokens linearly without explicit mechanisms for relations. | Imposes a bias misaligned with language’s inherent structure; models cannot explicitly reason over token–token relations. | Add **intra-attention inside the recurrence** to induce soft, undirected lexical relations among tokens during reading, trained end-to-end without direct supervision. |\n","| **Markovian state update in standard LSTMs**: next state depends only on current state; “unbounded memory” assumption may fail. | Long-distance information can be lost when the current state is treated as a sufficient summary; harms modeling of long dependencies. | Perform **non-Markov updates** by attending over all prior hidden/memory slots to build adaptive summaries $(\\tilde{h}_t, \\tilde{c}_t)$ before gating, directly incorporating distant context. |\n","| **Integrating structure with sequence transduction (two sequences)**: standard encoder–decoder uses only inter-attention. | Decoder may not fully exploit both intra-sequence relations (within target) and inter-sequence alignments (source↔target), limiting fusion of structural cues. | Propose two integrations: **Shallow fusion** (LSTMN for encoder/decoder plus inter-attention), and **Deep fusion** (store inter-alignment in decoder memory via a gate $r_t$). |\n","| **Need for relation induction without explicit supervision** | Obtaining gold structural annotations (e.g., dependencies) is costly; prior work may rely on external memories that are separate from recurrence. | Internalize memory within recurrence and let **soft attention weakly induce relations** during task training (no direct supervision), strengthening interaction between memory and update. |\n","| **Training stability of RNNs (vanishing/exploding gradients)** | Classic difficulty in learning long-term dependencies; prior remedies rely on gating/gradient clipping. | Retain **LSTM gating (input/forget/output, candidate)** for stable training while augmenting with attention-based memory reads, preserving proven optimization behavior. |\n","\n","---\n","\n","## Scope & Empirical Support  \n","The authors validate **LSTMN** on:  \n","- **Language modeling** (Penn Treebank): lower perplexity than LSTMs and gated-feedback models.  \n","- **Sentiment analysis** (Stanford Sentiment Treebank): higher accuracy than standard LSTMs, competitive with CNNs.  \n","- **Natural language inference** (SNLI): deep fusion achieves state-of-the-art accuracy (86.3%).  \n","\n","Overall, LSTMN demonstrates performance comparable to or better than state-of-the-art baselines, and consistently superior to vanilla LSTMs.\n","\n","---\n","\n","##\n","- Cheng, Jianpeng, Li Dong, and Mirella Lapata. *“Long Short-Term Memory-Networks for Machine Reading.”* EMNLP, 2016.  \n","- Datasets: Penn Treebank (LM), Stanford Sentiment Treebank (SA), Stanford NLI (NLI).  \n"],"metadata":{"id":"uFdDDUng0jkC"}},{"cell_type":"markdown","source":["# Related Work References\n","\n","| Author(s) | Year | Title | Venue | Connection to This Paper |\n","|-----------|------|-------|-------|---------------------------|\n","| Bahdanau, D., Cho, K., & Bengio, Y. | 2014 | Neural machine translation by jointly learning to align and translate | ICLR | Introduces attention in encoder–decoder models to mitigate memory compression in RNNs; inspires LSTMN’s intra-attention mechanism. |\n","| Rush, A. M., Chopra, S., & Weston, J. | 2015 | A neural attention model for abstractive sentence summarization | EMNLP | Example of sequence-to-sequence RNNs with attention, motivating LSTMN’s ability to uncover lexical relations. |\n","| Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., & Blunsom, P. | 2015 | Teaching machines to read and comprehend | NeurIPS | Applies attention-based RNNs for reading comprehension; provides a benchmark task for models like LSTMN. |\n","| Bengio, Y., Simard, P., & Frasconi, P. | 1994 | Learning long-term dependencies with gradient descent is difficult | IEEE Trans. Neural Networks | Classic paper on vanishing/exploding gradients; motivates gated RNNs and LSTM. |\n","| Hochreiter, S., & Schmidhuber, J. | 1997 | Long short-term memory | Neural Computation | Foundational LSTM work; LSTMN extends this by integrating memory networks. |\n","| Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. | 2014 | Learning phrase representations using RNN encoder–decoder for statistical machine translation | EMNLP | Introduces GRUs; influential in advancing gated architectures compared to vanilla RNNs. |\n","| Koutník, J., Greff, K., Gomez, F., & Schmidhuber, J. | 2014 | A clockwork RNN | ICML | Enhances information flow in recurrent networks; motivates architectural modifications beyond standard LSTMs. |\n","| Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. | 2015 | Gated feedback recurrent neural networks | ICML | Proposes gating mechanisms to improve RNN memory handling; precursor to richer memory integration. |\n","| Yao, K., Cohn, T., Vylomova, K., Duh, K., & Dyer, C. | 2015 | Depth-gated recurrent neural networks | arXiv | Introduces depth-gated RNNs; inspires comparison with LSTMN as another LSTM variant. |\n","| Zaremba, W., & Sutskever, I. | 2014 | Learning to execute | arXiv | Notes RNN memory limitations; contextualizes the need for attention/memory augmentation. |\n","| Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., & Potts, C. | 2013 | Recursive deep models for semantic compositionality over a sentiment treebank | EMNLP | Example of recursive neural networks leveraging structure; contrasts with LSTMN’s soft intra-attention. |\n","| Das, S., Giles, C. L., & Sun, G. Z. | 1992 | Learning context-free grammars with a recurrent neural network and external stack memory | CogSci | Early attempt to add structural bias with external memory; LSTMN builds on this idea. |\n","| Weston, J., Chopra, S., & Bordes, A. | 2015 | Memory networks | ICLR | Proposes external memory for reasoning; LSTMN adapts this by embedding memory inside the recurrence. |\n","| Sukhbaatar, S., Weston, J., Fergus, R., et al. | 2015 | End-to-end memory networks | NeurIPS | Extends memory networks with differentiable addressing; parallels LSTMN’s attention-based memory addressing. |\n","| Meng, F., Lu, Z., Tu, Z., Li, H., & Liu, Q. | 2015 | A deep memory-based architecture for sequence-to-sequence learning | ICLR Workshop | Applies memory networks to translation; motivates LSTMN’s encoder–decoder integration. |\n","| Grefenstette, E., Hermann, K. M., Suleyman, M., & Blunsom, P. | 2015 | Learning to transduce with unbounded memory | NeurIPS | Proposes differentiable memory structures; foundational to LSTMN’s memory design. |\n","| Tran, K., Bisazza, A., & Monz, C. | 2016 | Recurrent memory network for language modeling | NAACL | Combines RNNs with external memory; directly comparable to LSTMN’s approach. |\n","| Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ondruska, P., Gulrajani, I., & Socher, R. | 2016 | Ask me anything: Dynamic memory networks for NLP | ICML | Uses episodic memory modules for QA; related to LSTMN’s memory-driven reading. |\n","| Xiong, C., Merity, S., & Socher, R. | 2016 | Dynamic memory networks for visual and textual question answering | ICML | Extends memory networks to multimodal tasks; demonstrates broader relevance of memory-based reasoning. |\n","| Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. | 2015 | Transition-based dependency parsing with stack LSTMs | ACL | Uses hard structural decisions in parsing; contrasts with LSTMN’s soft, differentiable attention. |\n","| Bowman, S. R., Gauthier, J., Rastogi, A., Gupta, R., Manning, C. D., & Potts, C. | 2016 | A fast unified model for parsing and sentence understanding | ACL | Example of shift-reduce neural models with hard decisions; compared against LSTMN’s soft induction. |\n","| Klein, D., & Manning, C. D. | 2004 | Corpus-based induction of syntactic structure | ACL | Classic work on grammar induction; cited to distinguish LSTMN’s undirected lexical relations from directed head-modifier ones. |\n"],"metadata":{"id":"0QGnKCRf1Ci3"}}]}