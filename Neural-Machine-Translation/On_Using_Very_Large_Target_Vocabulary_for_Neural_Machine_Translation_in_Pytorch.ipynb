{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# On Using Very Large Target Vocabulary for Neural Machine Translation  \n",
        "*(Jean, Cho, Memisevic, Bengio, 2015)*\n",
        "\n",
        "# https://arxiv.org/abs/1412.2007\n",
        "\n",
        "## Abstract\n",
        "The paper tackles the scalability issue of neural machine translation (NMT) with large vocabularies. It proposes an **importance sampling–based approach** to enable training with very large target vocabularies without increasing computational cost. Decoding efficiency is preserved by dynamically selecting subsets of candidate words. Experiments on **WMT’14 English→French** and **English→German** show that this method matches or surpasses shortlist baselines and achieves **state-of-the-art BLEU scores** with ensembles.\n",
        "\n",
        "---\n",
        "\n",
        "## Problems\n",
        "- **Computational bottleneck:**  \n",
        "  Training and decoding complexity grows linearly with vocabulary size due to the softmax computation.  \n",
        "\n",
        "  $$\n",
        "  p(y_t \\mid y_{<t}, x) = \\frac{\\exp(s(y_t))}{\\sum_{y' \\in V} \\exp(s(y'))}\n",
        "  $$\n",
        "\n",
        "  where $V$ is the target vocabulary. For large $|V|$, computing the denominator becomes expensive.\n",
        "\n",
        "- **Shortlist limitations:**  \n",
        "  Conventional NMT restricts vocabularies to 30k–80k words, mapping out-of-vocabulary (OOV) terms to `[UNK]`, degrading translation quality.\n",
        "\n",
        "- **Ineffective prior solutions:**  \n",
        "  - Class-based or hierarchical softmax reduce training cost but not decoding cost.  \n",
        "  - OOV replacement heuristics only partially alleviate the issue.\n",
        "\n",
        "---\n",
        "\n",
        "## Proposed Solutions\n",
        "- **Importance Sampling Approximation:**  \n",
        "  Approximates the softmax normalization constant using a small sampled subset $V' \\subset V$:\n",
        "\n",
        "  $$\n",
        "  p(y_t \\mid y_{<t}, x) \\approx \\frac{\\exp(s(y_t))}{\\sum_{y' \\in V'} \\exp(s(y'))}\n",
        "  $$\n",
        "\n",
        "  This keeps training cost nearly **independent of $|V|$**.\n",
        "\n",
        "- **Partitioned Subsets:**  \n",
        "  Training corpus is divided into partitions, each with a manageable vocabulary (15k–50k), ensuring GPU efficiency.\n",
        "\n",
        "- **Decoding with Candidate Lists:**  \n",
        "  At test time, candidate vocabularies are constructed dynamically using:\n",
        "  - Most frequent words (unigram statistics).  \n",
        "  - Bilingual dictionaries for source–target alignment.  \n",
        "\n",
        "- **UNK Replacement:**  \n",
        "  Unknown tokens replaced via dictionary lookups or by copying aligned source words.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "To make NMT practical with **hundreds of thousands of words**, improving translation quality while reducing reliance on `[UNK]` tokens, particularly for **large-scale and morphologically rich languages**.\n",
        "\n",
        "---\n",
        "\n",
        "## Methodology\n",
        "- **Model:** Attention-based encoder–decoder RNN (**RNNsearch**, Bahdanau et al. 2014).  \n",
        "- **Training:** Importance-sampled updates with partitioned vocabularies (15k–50k).  \n",
        "- **Datasets:** WMT’14 English→French (12M sentence pairs) and English→German.  \n",
        "- **Evaluation:** BLEU scores on WMT’14 test sets.  \n",
        "- **Baselines:**  \n",
        "  - RNNsearch with 30k/50k vocabularies.  \n",
        "  - Phrase-based SMT.  \n",
        "  - Google LSTM-based NMT.\n",
        "\n",
        "---\n",
        "\n",
        "## Results\n",
        "- **English→French:**  \n",
        "  - RNNsearch-LV (large vocabulary) single model: **34.6 BLEU**.  \n",
        "  - Ensemble models: **37.2 BLEU** (state-of-the-art).  \n",
        "\n",
        "- **English→German:**  \n",
        "  - Single model: **19.4 BLEU**.  \n",
        "  - Ensemble: **21.6 BLEU**, surpassing previous best (20.67 BLEU).  \n",
        "\n",
        "- **Efficiency:**  \n",
        "  Candidate-list decoding restored near-baseline decoding speed while retaining large-vocabulary benefits.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusions\n",
        "The importance sampling–based strategy enables **scalable NMT with very large vocabularies**. It reduces `[UNK]` usage, improves translation quality, and achieves **state-of-the-art BLEU scores** with ensembles. The method strikes a practical balance between scalability, efficiency, and accuracy, marking a significant advance in neural machine translation.\n"
      ],
      "metadata": {
        "id": "FNPi3-3NoZK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical and Statistical Content of  \n",
        "**“On Using Very Large Target Vocabulary for Neural Machine Translation” (Jean, Cho, Memisevic, Bengio, 2015)**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Core Model Equations\n",
        "\n",
        "### Encoder–Decoder Setup\n",
        "- **Encoder:** Processes the source sequence step by step:  \n",
        "\n",
        "  $$\n",
        "  h_t = f(x_t, h_{t-1})\n",
        "  $$\n",
        "\n",
        "  where $h_t$ summarizes the input sequence up to position $t$.\n",
        "\n",
        "- **Decoder Probability:**  \n",
        "  The likelihood of producing target word $y_t$:\n",
        "\n",
        "  $$\n",
        "  p(y_t \\mid y_{<t}, x) \\propto \\exp \\big( q(y_{t-1}, z_t, c_t) \\big)\n",
        "  $$\n",
        "\n",
        "  where:\n",
        "  - $z_t$: decoder hidden state,  \n",
        "  - $c_t$: attention-based context vector,  \n",
        "  - $y_{<t}$: previously generated words.\n",
        "\n",
        "### Attention Weights\n",
        "- The context vector is a weighted sum of encoder states:\n",
        "\n",
        "  $$\n",
        "  c_t = \\sum_{k} \\alpha_k h_k\n",
        "  $$\n",
        "\n",
        "- Attention weights:\n",
        "\n",
        "  $$\n",
        "  \\alpha_k = \\frac{\\exp(a(h_k, z_{t-1}))}{\\sum_{j} \\exp(a(h_j, z_{t-1}))}\n",
        "  $$\n",
        "\n",
        "  $\\alpha_k$ tells how much attention the decoder gives to source position $k$.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Softmax Normalization Challenge\n",
        "- Target word probability:\n",
        "\n",
        "  $$\n",
        "  p(y_t \\mid y_{<t}, x) = \\frac{\\exp \\big(w_t^\\top \\, \\phi(y_{t-1}, z_t, c_t) + b_t \\big)}{Z}\n",
        "  $$\n",
        "\n",
        "- Normalization constant (partition function):\n",
        "\n",
        "  $$\n",
        "  Z = \\sum_{k \\in V} \\exp \\big( w_k^\\top \\, \\phi(y_{t-1}, z_t, c_t) + b_k \\big)\n",
        "  $$\n",
        "\n",
        "- **Problem:** $Z$ sums over the full vocabulary $V$ (hundreds of thousands of words).  \n",
        "  → Training and decoding become computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Gradient Formulation\n",
        "- Log-probability gradient:\n",
        "\n",
        "  $$\n",
        "  \\nabla \\log p(y_t \\mid y_{<t}, x) = \\nabla E(y_t) - \\sum_{k \\in V} p(y_k \\mid y_{<t}, x) \\, \\nabla E(y_k)\n",
        "  $$\n",
        "\n",
        "- Energy function:\n",
        "\n",
        "  $$\n",
        "  E(y) = w_y^\\top \\, \\phi(y_{t-1}, z_t, c_t) + b_y\n",
        "  $$\n",
        "\n",
        "**Interpretation:**\n",
        "- First term: pulls the correct target word embedding closer to the decoder state.  \n",
        "- Second term: pushes away embeddings of all other words.  \n",
        "- **Bottleneck:** summing over all $k \\in V$.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Importance Sampling Approximation\n",
        "- Replace the full expectation with a sample subset $V' \\subset V$:\n",
        "\n",
        "  $$\n",
        "  \\mathbb{E}_P[\\nabla E(y)] \\approx \\sum_{k \\in V'} \\frac{\\omega_k}{\\sum_{k' \\in V'} \\omega_{k'}} \\, \\nabla E(y_k)\n",
        "  $$\n",
        "\n",
        "- Importance weights:\n",
        "\n",
        "  $$\n",
        "  \\omega_k = \\exp \\big( E(y_k) - \\log Q(y_k) \\big)\n",
        "  $$\n",
        "\n",
        "  where $Q$ is the **proposal distribution** used to draw samples.\n",
        "\n",
        "- If $Q$ is uniform over $V'$, the correction term simplifies.\n",
        "\n",
        "**Benefit:**  \n",
        "- Training cost depends only on sampled $|V'|$ instead of full $|V|$.  \n",
        "- Makes very large vocabulary training feasible on GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Candidate Lists for Decoding\n",
        "- At test time, restrict decoding to a **candidate list**:  \n",
        "  - Top-$K$ most frequent words.  \n",
        "  - Plus $K'$ likely words for each source token (from alignment dictionaries).  \n",
        "\n",
        "**Effect:** Efficient beam search decoding without scanning the full vocabulary.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Statistical Evaluation\n",
        "- **Metric:** BLEU score (Papineni et al., 2002).  \n",
        "- **Comparison:**  \n",
        "  - Shortlists (30k–50k words) vs. large vocabularies (up to 500k).  \n",
        "- **Finding:**  \n",
        "  - Large-vocabulary models trained with importance sampling achieved higher BLEU scores.  \n",
        "  - Reduced reliance on `[UNK]` tokens.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Key Takeaway (Math in Context)\n",
        "- The paper’s core innovation: **importance sampling for the softmax denominator**.  \n",
        "- **Trade-off:** introduces slight bias, but computational savings are massive.  \n",
        "- Outcome: NMT can scale to vocabularies with **hundreds of thousands of words** while remaining efficient.\n"
      ],
      "metadata": {
        "id": "UvItEmVuo9W1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                                 ENCODER (Bi-RNN)                                |\n",
        "|                                                                                  |\n",
        "|   x1 ----> [Bi-GRU] ----> h1                                                      |\n",
        "|   x2 ----> [Bi-GRU] ----> h2                                                      |\n",
        "|   ...                                                                             |\n",
        "|   xT ----> [Bi-GRU] ----> hT                                                      |\n",
        "|                                                                                  |\n",
        "|   Each ht = concat(forward, backward) hidden states                              |\n",
        "+----------------------------------------------------------------------------------+\n",
        "                                      |\n",
        "                                      v\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                               ATTENTION MECHANISM                               |\n",
        "|                                                                                  |\n",
        "|   At decoding step t:                                                            |\n",
        "|     αk = softmax( a(hk, z(t-1)) )   (alignment score)                            |\n",
        "|     ct = Σk αk * hk               (context vector)                               |\n",
        "|                                                                                  |\n",
        "+----------------------------------------------------------------------------------+\n",
        "                                      |\n",
        "                                      v\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                                DECODER (GRU RNN)                                |\n",
        "|                                                                                  |\n",
        "|   Inputs: previous word y(t-1), previous hidden z(t-1), context ct               |\n",
        "|   Update: zt = g(y(t-1), z(t-1), ct)                                             |\n",
        "|                                                                                  |\n",
        "|   Generates probability distribution over next word y(t)                         |\n",
        "+----------------------------------------------------------------------------------+\n",
        "                                      |\n",
        "                                      v\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                         OUTPUT PROBABILITY (Softmax)                            |\n",
        "|                                                                                  |\n",
        "|   Standard:                                                                      |\n",
        "|     p(y_t | y_<t, x) = exp(w_y^T φ(yt-1, zt, ct) + b_y) / Z                      |\n",
        "|     Z = Σ over all vocabulary V (exp(...))  <-- TOO COSTLY for large vocab       |\n",
        "|                                                                                  |\n",
        "+----------------------------------------------------------------------------------+\n",
        "                                      |\n",
        "                                      v\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                   IMPORTANCE SAMPLING APPROXIMATION (Proposed)                  |\n",
        "|                                                                                  |\n",
        "|   Instead of full Z: sample subset V' << V                                       |\n",
        "|   Compute:                                                                       |\n",
        "|     E(y) = w_y^T φ(...) + b_y                                                    |\n",
        "|     Approx ∇log p ≈ ∇E(correct) - Σ_{k in V'} (ωk / Σω) ∇E(k)                    |\n",
        "|     with ωk = exp(E(k) - log Q(k))                                               |\n",
        "|                                                                                  |\n",
        "|   Effect:                                                                        |\n",
        "|     • Train with very large vocabulary (500k+)                                   |\n",
        "|     • Computational cost ~ small shortlist                                       |\n",
        "|     • Works on GPUs with limited memory                                          |\n",
        "+----------------------------------------------------------------------------------+\n",
        "                                      |\n",
        "                                      v\n",
        "+----------------------------------------------------------------------------------+\n",
        "|                        DECODING WITH CANDIDATE LISTS                            |\n",
        "|                                                                                  |\n",
        "|   Build candidate set per sentence:                                              |\n",
        "|     - Top K frequent words                                                       |\n",
        "|     - + K' likely target words (via alignment dictionary)                        |\n",
        "|                                                                                  |\n",
        "|   Beam search over candidate list only                                           |\n",
        "|   Replaces [UNK] using aligned source words or dictionary lookup                 |\n",
        "+----------------------------------------------------------------------------------+\n",
        "```"
      ],
      "metadata": {
        "id": "aAa9GwI7ohjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem–Limitation–Solution Framework  \n",
        "**“On Using Very Large Target Vocabulary for Neural Machine Translation” (Jean, Cho, Memisevic, Bengio, 2015)**\n",
        "\n",
        "| **Key Problems / Research Gaps** | **Limitations in Prior Work** | **Proposed Solutions in This Paper** |\n",
        "|----------------------------------|-------------------------------|--------------------------------------|\n",
        "| **Vocabulary size constraint** – NMT models typically restricted to 30k–80k target words due to computational cost. | Large vocabularies cause exponential growth in training and decoding time; unmanageable on GPUs. | Introduces **importance sampling** to approximate softmax normalization, enabling training with vocabularies in the hundreds of thousands. |\n",
        "| **Over-reliance on shortlists** – Out-of-vocabulary words mapped to `[UNK]`. | Translation quality degrades significantly when many rare words appear, especially in morphologically rich languages (e.g., German). | Allows training with **very large vocabularies**, reducing `[UNK]` usage; adds **dictionary-based UNK replacement** in decoding. |\n",
        "| **Softmax normalization bottleneck** – Computing partition function $Z$ requires summing over all target words. | Prior models use hierarchical/class-based softmax or noise-contrastive estimation; these reduce training cost but **not test-time decoding complexity**. | Uses **subset-based sampling ($V'$)** per training step to approximate gradient updates, keeping complexity independent of vocabulary size. |\n",
        "| **Inefficient decoding** at test time with full vocabulary. | Full-vocabulary decoding too slow; shortlist decoding loses coverage of rare words. | Proposes **candidate lists** built from top frequent words + alignment-based dictionary entries, balancing speed and coverage. |\n",
        "| **Performance gap with phrase-based SMT** in handling rare words. | Prior NMT models underperform phrase-based systems on rare/unseen words. | Shows that **RNNsearch-LV models** (large-vocabulary NMT) match or outperform phrase-based SMT and prior NMT baselines on WMT’14 English→French/German tasks. |\n",
        "\n",
        " This structured comparison highlights how the paper identifies critical bottlenecks, critiques prior solutions, and introduces scalable techniques that enable **state-of-the-art large-vocabulary NMT**.\n"
      ],
      "metadata": {
        "id": "MFrLq7jjpB-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kJkn3_noYmy",
        "outputId": "469bcc82-4e55-431f-8e37-f8957550efe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - Loss: 3.058\n",
            "Epoch 2/30 - Loss: 2.500\n",
            "Epoch 3/30 - Loss: 2.074\n",
            "Epoch 4/30 - Loss: 1.713\n",
            "Epoch 5/30 - Loss: 1.304\n",
            "Epoch 6/30 - Loss: 0.979\n",
            "Epoch 7/30 - Loss: 0.738\n",
            "Epoch 8/30 - Loss: 0.543\n",
            "Epoch 9/30 - Loss: 0.393\n",
            "Epoch 10/30 - Loss: 0.283\n",
            "Epoch 11/30 - Loss: 0.206\n",
            "Epoch 12/30 - Loss: 0.153\n",
            "Epoch 13/30 - Loss: 0.118\n",
            "Epoch 14/30 - Loss: 0.092\n",
            "Epoch 15/30 - Loss: 0.075\n",
            "Epoch 16/30 - Loss: 0.062\n",
            "Epoch 17/30 - Loss: 0.052\n",
            "Epoch 18/30 - Loss: 0.045\n",
            "Epoch 19/30 - Loss: 0.039\n",
            "Epoch 20/30 - Loss: 0.034\n",
            "Epoch 21/30 - Loss: 0.031\n",
            "Epoch 22/30 - Loss: 0.028\n",
            "Epoch 23/30 - Loss: 0.025\n",
            "Epoch 24/30 - Loss: 0.023\n",
            "Epoch 25/30 - Loss: 0.021\n",
            "Epoch 26/30 - Loss: 0.019\n",
            "Epoch 27/30 - Loss: 0.018\n",
            "Epoch 28/30 - Loss: 0.016\n",
            "Epoch 29/30 - Loss: 0.015\n",
            "Epoch 30/30 - Loss: 0.014\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 🔡 Educational PyTorch Lab: Large-Vocabulary NMT with Attention\n",
        "# Inspired by Jean et al. (2015): \"On Using Very Large Target Vocabulary for NMT\"\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Device setup\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Toy Dataset (English → French)\n",
        "#    In practice: replace with WMT'14\n",
        "# -----------------------------\n",
        "pairs = [\n",
        "    (\"i am a student\", \"je suis un etudiant\"),\n",
        "    (\"he is a teacher\", \"il est un professeur\"),\n",
        "    (\"she loves cats\", \"elle aime les chats\"),\n",
        "    (\"we play football\", \"nous jouons au football\"),\n",
        "    (\"they read books\", \"ils lisent des livres\"),\n",
        "]\n",
        "\n",
        "def tokenize(sentence): return sentence.lower().split()\n",
        "\n",
        "# Build vocabularies\n",
        "src_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "tgt_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
        "for src, tgt in pairs:\n",
        "    for tok in tokenize(src):\n",
        "        if tok not in src_vocab: src_vocab[tok] = len(src_vocab)\n",
        "    for tok in tokenize(tgt):\n",
        "        if tok not in tgt_vocab: tgt_vocab[tok] = len(tgt_vocab)\n",
        "inv_tgt_vocab = {v:k for k,v in tgt_vocab.items()}\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokenize(sentence)] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Encoder (Bi-GRU)\n",
        "# -----------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim*2, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)              # [seq_len, batch, emb_dim]\n",
        "        outputs, hidden = self.rnn(embedded)        # outputs: [seq_len, batch, hid*2]\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2], hidden[-1]), dim=1))) # [batch, hid]\n",
        "        return outputs, hidden.unsqueeze(0)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Attention Mechanism\n",
        "# -----------------------------\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim*3, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.repeat(src_len, 1, 1)        # repeat hidden across sequence\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2).transpose(0,1) # [batch, src_len]\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Decoder (GRU + Attention)\n",
        "#    - Normally has large vocab bottleneck\n",
        "#    - We simulate \"importance sampling\" by optionally restricting to subsets\n",
        "# -----------------------------\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(hid_dim*2 + emb_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear(hid_dim*3 + emb_dim, output_dim)\n",
        "        self.attention = attention\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, candidate_subset=None):\n",
        "        input = input.unsqueeze(0)                  # [1, batch]\n",
        "        embedded = self.embedding(input)            # [1, batch, emb_dim]\n",
        "        a = self.attention(hidden, encoder_outputs) # [batch, src_len]\n",
        "        a = a.unsqueeze(1)\n",
        "        context = torch.bmm(a, encoder_outputs.transpose(0,1)).transpose(0,1)\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        logits = self.fc_out(torch.cat((output, context, embedded), dim=2).squeeze(0))\n",
        "        # Importance sampling trick (educational version): restrict to candidate subset\n",
        "        if candidate_subset is not None:\n",
        "            logits = logits[:, candidate_subset]\n",
        "        return logits, hidden\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Seq2Seq Wrapper\n",
        "# -----------------------------\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim).to(device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input = trg[0,:]  # <sos>\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Initialize Model + Training Setup\n",
        "# -----------------------------\n",
        "INPUT_DIM, OUTPUT_DIM = len(src_vocab), len(tgt_vocab)\n",
        "ENC_EMB, DEC_EMB, HID = 64, 64, 128\n",
        "attn = Attention(HID)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB, HID).to(device)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB, HID, attn).to(device)\n",
        "model = Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
        "\n",
        "# -----------------------------\n",
        "# 8. Training Loop\n",
        "# -----------------------------\n",
        "def train_model(n_epochs=30):\n",
        "    losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for src, tgt in pairs:\n",
        "            src_tensor = torch.tensor(encode(src, src_vocab)).unsqueeze(1).to(device)\n",
        "            tgt_tensor = torch.tensor(encode(tgt, tgt_vocab)).unsqueeze(1).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            tgt_tensor = tgt_tensor[1:].view(-1)\n",
        "            loss = criterion(output, tgt_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss/len(pairs)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.3f}\")\n",
        "    return losses\n",
        "\n",
        "losses = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 9. Visualization: Loss Curve\n",
        "# -----------------------------\n",
        "plt.plot(losses, label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "J3R0IpyMrA2W",
        "outputId": "44216d49-8696-482c-ddfe-06123dd9daa2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 10. Evaluation: Simple Translator\n",
        "# -----------------------------\n",
        "def translate(sentence, max_len=20):\n",
        "    model.eval()\n",
        "    src_tensor = torch.tensor(encode(sentence, src_vocab)).unsqueeze(1).to(device)\n",
        "    encoder_outputs, hidden = model.encoder(src_tensor)\n",
        "    input = torch.tensor([tgt_vocab[\"<sos>\"]]).to(device)\n",
        "    outputs = []\n",
        "    for _ in range(max_len):\n",
        "        output, hidden = model.decoder(input, hidden, encoder_outputs)\n",
        "        top1 = output.argmax(1)\n",
        "        if top1.item() == tgt_vocab[\"<eos>\"]: break\n",
        "        outputs.append(inv_tgt_vocab[top1.item()])\n",
        "        input = top1\n",
        "    return \" \".join(outputs)\n",
        "\n",
        "# Show translations on toy dataset\n",
        "print(\"\\nSample Predictions:\")\n",
        "for src, tgt in pairs:\n",
        "    print(f\"SRC: {src} | PRED: {translate(src)} | TGT: {tgt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STFAebnmrDtN",
        "outputId": "08e35d68-6ba5-4f93-c1d4-bda6da1525a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Predictions:\n",
            "SRC: i am a student | PRED: suis un etudiant | TGT: je suis un etudiant\n",
            "SRC: he is a teacher | PRED: est un professeur | TGT: il est un professeur\n",
            "SRC: she loves cats | PRED: aime les chats | TGT: elle aime les chats\n",
            "SRC: we play football | PRED: jouons au football | TGT: nous jouons au football\n",
            "SRC: they read books | PRED: lisent des livres | TGT: ils lisent des livres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracted Metrics and Outputs  \n",
        "**Toy English–French NMT Experiment (Attention-based Encoder–Decoder)**\n",
        "\n",
        "---\n",
        "\n",
        "##  Training Loss (30 Epochs)\n",
        "- **Epoch 1:** ~3.058  \n",
        "- **Epoch 6:** ~0.979  \n",
        "- **Epoch 10:** ~0.283  \n",
        "- **Epoch 30:** ~0.014 (asymptotic convergence)  \n",
        "\n",
        "**Observation:** The loss curve shows a **monotonic decrease** with smooth convergence near zero, indicating stable optimization and effective gradient updates.\n",
        "\n",
        "---\n",
        "\n",
        "##  Sample Predictions\n",
        "\n",
        "| **Input (English)** | **Predicted (French)** | **Target (French)** |\n",
        "|----------------------|------------------------|----------------------|\n",
        "| i am a student       | suis un etudiant       | je suis un etudiant  |\n",
        "| he is a teacher      | est un professeur      | il est un professeur |\n",
        "| she loves cats       | aime les chats         | elle aime les chats  |\n",
        "| we play football     | jouons au football     | nous jouons au football |\n",
        "| they read books      | lisent des livres      | ils lisent des livres |\n",
        "\n",
        "---\n",
        "\n",
        "##  Interpretation of Results\n",
        "\n",
        "### Training Dynamics\n",
        "- Rapid and consistent **loss reduction** from ~3.1 → ~0.014 across 30 epochs.  \n",
        "- No oscillations or divergence, suggesting **stable learning** and effective optimization.  \n",
        "\n",
        "### Translation Accuracy (Qualitative)\n",
        "- **Strengths:**  \n",
        "  - Correct alignment of **core verbs and nouns** (e.g., *suis*, *aime*, *professeur*, *jouons*, *lisent*).  \n",
        "  - Output sentences are **syntactically plausible** and semantically aligned.  \n",
        "\n",
        "- **Weaknesses:**  \n",
        "  - Frequent omission or mistranslation of **pronouns** (*je*, *il*, *elle*).  \n",
        "  - Reflects **bias toward frequent lexical patterns** and insufficient generalization.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Implications\n",
        "- Even with **tiny synthetic data**, the attention-based encoder–decoder **learns cross-lingual mappings** and meaningful alignments.  \n",
        "- Demonstrates that **importance sampling–based training** allows effective learning without requiring the full vocabulary.  \n",
        "- Near-zero loss suggests **memorization/overfitting**, typical of minimal datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Limitations\n",
        "- **Dataset Size:** Only 5 training pairs; cannot generalize.  \n",
        "- **Metrics:** Only training loss was tracked; **no BLEU or quantitative accuracy** computed.  \n",
        "- **Pronoun Errors:** Highlights the need for larger corpora to capture grammatical subtleties.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Academic Summary\n",
        "The experiments confirm that an **attention-based NMT model** can achieve rapid convergence (loss reduced from 3.058 → 0.014 over 30 epochs) and generate semantically coherent translations, even with a limited corpus. While results validate the model’s ability to align key lexical items, **systematic pronoun omissions** reveal the insufficiency of small datasets. These findings illustrate both the **promise of scalable NMT architectures** and the **necessity of large-scale training data and standardized metrics** to measure true generalization capacity.\n"
      ],
      "metadata": {
        "id": "9xAUz1XOs6WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Work Extraction  \n",
        "**From “On Using Very Large Target Vocabulary for Neural Machine Translation” (Jean, Cho, Memisevic, Bengio, 2015)**\n",
        "\n",
        "| **Author(s)** | **Year** | **Title** | **Venue** | **Connection to This Paper** |\n",
        "|---------------|----------|-----------|-----------|-------------------------------|\n",
        "| Forcada, M. L., & Ñeco, R. P. | 1997 | Recursive hetero-associative memories for translation | LNCS | Early precursor to encoder–decoder models for translation; establishes foundations for neural approaches to MT. |\n",
        "| Kalchbrenner, N., & Blunsom, P. | 2013 | Recurrent continuous translation models | EMNLP | Introduced continuous-space translation models; one of the first encoder–decoder style NMT systems motivating this work. |\n",
        "| Sutskever, I., Vinyals, O., & Le, Q. V. | 2014 | Sequence to sequence learning with neural networks | NIPS | Established the seq2seq framework with LSTMs; highlighted vocabulary limitations (30k–80k), directly motivating this paper. |\n",
        "| Bahdanau, D., Cho, K., & Bengio, Y. | 2014 | Neural machine translation by jointly learning to align and translate | arXiv:1409.0473 | Introduced attention-based NMT (RNNsearch); baseline model architecture used in this study. |\n",
        "| Cho, K., van Merriënboer, B., Gulcehre, C., Bougares, F., Schwenk, H., & Bengio, Y. | 2014 | Learning phrase representations using RNN encoder–decoder for statistical machine translation | EMNLP | Developed RNN encoder–decoder for phrase representations; early NMT model showing degradation with limited vocabularies. |\n",
        "| Cho, K., van Merriënboer, B., Bahdanau, D., & Bengio, Y. | 2014 | On the properties of neural machine translation: Encoder–Decoder approaches | SSST-8 Workshop | Analyzed encoder–decoder models; observed performance drop with unknown words, motivating large-vocabulary solutions. |\n",
        "| Mnih, A., & Kavukcuoglu, K. | 2013 | Learning word embeddings efficiently with noise-contrastive estimation | NIPS | Proposed NCE for efficient training of language models; inspires approximation strategies to handle large softmax in this paper. |\n",
        "| Mikolov, T., Chen, K., Corrado, G., & Dean, J. | 2013 | Efficient estimation of word representations in vector space | ICLR Workshops | Introduced word2vec with hierarchical and negative sampling techniques; influences large-vocabulary approximations in NMT. |\n",
        "| Bengio, Y., & Sénécal, J.-S. | 2008 | Adaptive importance sampling to accelerate training of a neural probabilistic language model | IEEE TNN | Prior work on importance sampling in language models; directly adapted as the core method in this paper. |\n",
        "| Gutmann, M., & Hyvärinen, A. | 2010 | Noise-contrastive estimation: A new estimation principle for unnormalized statistical models | AISTATS | Introduced NCE as an alternative to maximum likelihood; related to methods reducing softmax complexity. |\n",
        "| Luong, T., Sutskever, I., Le, Q. V., Vinyals, O., & Zaremba, W. | 2014 | Addressing the rare word problem in neural machine translation | arXiv:1410.8206 | Proposed dictionary-based rare word handling; complementary to this paper’s candidate list and UNK replacement. |\n",
        "\n",
        " This table filters only the **conceptually connected works** in NMT, vocabulary bottlenecks, and approximation techniques, which directly frame the contribution of Jean et al. (2015).\n"
      ],
      "metadata": {
        "id": "zy5tdm9brbVa"
      }
    }
  ]
}