{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundational Papers in Word Embeddings\n",
        "\n",
        "---\n",
        "\n",
        "## 1. A Neural Probabilistic Language Model — Bengio et al. (2003 / NIPS 2000)\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Traditional n-gram models suffered from the **curse of dimensionality** — unseen word sequences received zero probability. Bengio et al. proposed a neural network that **learns distributed word representations** (embeddings) to generalize better across unseen combinations.\n",
        "\n",
        "### Core Idea\n",
        "\n",
        "Each word \\( w \\) is represented by a dense vector embedding \\( C(w) \\in \\mathbb{R}^m \\).  \n",
        "A neural network predicts the next word given a context of \\( n-1 \\) words:\n",
        "\n",
        "$$\n",
        "P(w_t | w_{t-1}, ..., w_{t-n+1}) = \\text{Softmax}(f(C(w_{t-1}), ..., C(w_{t-n+1})))\n",
        "$$\n",
        "\n",
        "The embeddings and the network are learned jointly by maximizing log-likelihood over the corpus.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "- Embedding matrix \\( C \\in \\mathbb{R}^{|V| \\times m} \\)\n",
        "- Context word embeddings concatenated or averaged\n",
        "- One or more hidden layers with nonlinear activation\n",
        "- Output layer: softmax over vocabulary\n",
        "\n",
        "### Significance\n",
        "\n",
        "- Unified **embedding learning** and **language modeling**.\n",
        "- Allowed **generalization to unseen n-grams**.\n",
        "- Introduced **continuous-space representations** of words.\n",
        "- Foundation for all subsequent embedding models.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- High computational cost for large vocabularies.\n",
        "- Fixed-length context window.\n",
        "- Static embeddings (one vector per word).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Word2Vec — Mikolov et al. (2013)\n",
        "\n",
        "### Motivation\n",
        "\n",
        "To make training faster and scalable, Word2Vec simplifies Bengio’s model while retaining semantic quality.\n",
        "\n",
        "### Models\n",
        "\n",
        "- **Skip-gram**: predict surrounding context words from a target word.\n",
        "- **CBOW (Continuous Bag-of-Words)**: predict a target word from surrounding context.\n",
        "\n",
        "### Objective (Skip-gram with Negative Sampling)\n",
        "\n",
        "For observed pairs \\( (w, c) \\):\n",
        "\n",
        "$$\n",
        "\\log \\sigma(v_c' \\cdot v_w) + \\sum_{i=1}^k \\mathbb{E}_{c_i \\sim P_n} [ \\log \\sigma(-v_{c_i}' \\cdot v_w) ]\n",
        "$$\n",
        "\n",
        "where \\( P_n \\) is the noise distribution for negative samples.\n",
        "\n",
        "### Innovations\n",
        "\n",
        "- **Negative Sampling** and **Subsampling** of frequent words for efficiency.\n",
        "- Training billions of tokens on a single machine.\n",
        "- Learned **linear semantic relationships** such as:\n",
        "\n",
        "$$\n",
        "\\text{vec(\"king\")} - \\text{vec(\"man\")} + \\text{vec(\"woman\")} \\approx \\text{vec(\"queen\")}\n",
        "$$\n",
        "\n",
        "### Impact\n",
        "\n",
        "- Fast, scalable, widely adopted.\n",
        "- Embeddings capture syntactic and semantic analogies.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. GloVe — Pennington, Socher & Manning (2014)\n",
        "\n",
        "### Motivation\n",
        "\n",
        "Combine **global co-occurrence statistics** (like in LSA) with **local predictive modeling** (like Word2Vec).\n",
        "\n",
        "### Model\n",
        "\n",
        "Let \\( X_{ij} \\) = number of times word \\( j \\) appears in the context of word \\( i \\).\n",
        "\n",
        "They propose:\n",
        "\n",
        "$$\n",
        "w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j = \\log X_{ij}\n",
        "$$\n",
        "\n",
        "Minimized using a weighted least-squares loss with weighting function \\( f(X_{ij}) \\).\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- Captures **global corpus structure**.\n",
        "- Deterministic and symmetric.\n",
        "- Combines strengths of count-based and predictive models.\n",
        "\n",
        "### Pros and Cons\n",
        "\n",
        "| Pros | Cons |\n",
        "|------|------|\n",
        "| Strong global semantics | Still static embeddings |\n",
        "| Simple and interpretable | Co-occurrence matrix can be large |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. fastText — Bojanowski et al. (2017)\n",
        "\n",
        "### Problem\n",
        "\n",
        "Static word embeddings struggle with **rare words** and **morphologically rich languages**.\n",
        "\n",
        "### Solution\n",
        "\n",
        "Represent each word as the sum of its **character n-gram embeddings**:\n",
        "\n",
        "$$\n",
        "v(w) = \\frac{1}{|G(w)|} \\sum_{g \\in G(w)} z_g\n",
        "$$\n",
        "\n",
        "where \\( G(w) \\) is the set of n-grams for word \\( w \\), and \\( z_g \\) is the embedding of subword \\( g \\).\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Handles OOV (out-of-vocabulary) words gracefully.\n",
        "- Encodes morphological patterns.\n",
        "- Integrated within Skip-gram or CBOW objectives.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "- More parameters due to n-grams.\n",
        "- Loses some idiomatic meaning precision.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. ELMo — Peters et al. (2018)\n",
        "\n",
        "### Paradigm Shift: Contextual Embeddings\n",
        "\n",
        "Unlike static embeddings, ELMo produces **context-dependent vectors**.\n",
        "\n",
        "### Model\n",
        "\n",
        "Pretrain a **bidirectional language model (biLM)** using LSTMs:\n",
        "\n",
        "$$\n",
        "P(w_1, ..., w_T) = \\prod_{t=1}^T P(w_t | w_{1:t-1}) P(w_t | w_{t+1:T})\n",
        "$$\n",
        "\n",
        "ELMo embedding for a word = weighted sum of hidden layers from both directions.\n",
        "\n",
        "### Features\n",
        "\n",
        "- Learns **different embeddings for the same word** depending on context.\n",
        "- Dramatically improved NLP task performance.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Based on LSTMs → limited parallelism.\n",
        "- Restricted to local sequence modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. BERT — Devlin et al. (2018)\n",
        "\n",
        "### Innovation\n",
        "\n",
        "Introduced **Transformers** for **bidirectional contextual embeddings**.\n",
        "\n",
        "### Pretraining Tasks\n",
        "\n",
        "1. **Masked Language Modeling (MLM):** Predict masked tokens using context.\n",
        "2. **Next Sentence Prediction (NSP):** Predict if sentence B follows sentence A.\n",
        "\n",
        "### Transformer-based Contextualization\n",
        "\n",
        "Tokens are embedded, then refined through self-attention layers:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "### Significance\n",
        "\n",
        "- Captures **deep bidirectional context**.\n",
        "- Massive performance jump across NLP benchmarks.\n",
        "- Foundation for modern language models (RoBERTa, GPT, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Later Developments\n",
        "\n",
        "- **SynGCN / SemGCN:** Integrate syntactic/semantic graphs.\n",
        "- **Multilingual embeddings:** Align cross-lingual spaces.\n",
        "- **Distillation:** Compress contextual models for efficiency.\n",
        "- **Dynamic embeddings:** Adapt continuously as language evolves.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Model | Year | Contextual | Architecture | Main Contribution |\n",
        "|--------|------|-------------|---------------|--------------------|\n",
        "| Bengio et al. | 2003 | No | MLP | Neural LM with embeddings |\n",
        "| Word2Vec | 2013 | No | Shallow NN | Scalable predictive embeddings |\n",
        "| GloVe | 2014 | No | Matrix factorization | Global co-occurrence modeling |\n",
        "| fastText | 2017 | No | Subword NN | Morphology-aware embeddings |\n",
        "| ELMo | 2018 | Yes | BiLSTM | Contextual word vectors |\n",
        "| BERT | 2018 | Yes | Transformer | Deep bidirectional contextualization |\n"
      ],
      "metadata": {
        "id": "ch0uX8iPks-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chronological Evolution of Word Embedding Paradigms\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Bengio et al. (2003) — Neural Probabilistic Language Model\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2003  \n",
        "Core Architecture: Feedforward Neural Language Model  \n",
        "Representation Type: Distributed Static Embeddings  \n",
        "Objective Function: Next-word prediction via softmax  \n",
        "Context Modeling: Fixed window (n-gram)  \n",
        "Innovation Vector: Introduced continuous word representations; joint training of embeddings and language model  \n",
        "Limitation Vector: Expensive softmax; short context window; static embeddings  \n",
        "Legacy: Foundation for Word2Vec; marked the renaissance of neural language models  \n",
        "Mathematical Form:  \n",
        "$$\n",
        "p(w_t | w_{t-n+1}, …, w_{t-1}) = \\text{Softmax}(W·h + b)\n",
        "$$  \n",
        "Citation Impact: ≈ 20k+  \n",
        "Conceptual Distance to BERT: 0.9  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Mikolov et al. (2013) — Word2Vec: Skip-Gram & CBOW\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2013  \n",
        "Core Architecture: Shallow Neural Network  \n",
        "Representation Type: Static Distributed Embeddings  \n",
        "Objective Function: Predict context (Skip-gram) or predict target (CBOW)  \n",
        "Optimization Tricks: Negative Sampling, Hierarchical Softmax, Subsampling of frequent words  \n",
        "Innovation Vector: Scalable training; vector arithmetic semantics  \n",
        "Limitation Vector: Context-independent; ignores global corpus statistics  \n",
        "Legacy: Dominant paradigm from 2013–2016; foundation for GloVe and FastText  \n",
        "Key Equation:  \n",
        "$$\n",
        "\\log \\sigma(v'_c·v_w) + \\sum_i \\mathbb{E}[ \\log \\sigma(-v'_{c_i}·v_w) ]\n",
        "$$  \n",
        "Geometric Property: Linear analogies emerge as vector offsets  \n",
        "Conceptual Distance to BERT: 0.8  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Pennington et al. (2014) — GloVe: Global Vectors for Word Representation\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2014  \n",
        "Core Architecture: Matrix Factorization + Log-Bilinear Model  \n",
        "Representation Type: Static Global Embeddings  \n",
        "Objective Function: Weighted least-squares on log co-occurrence counts  \n",
        "Data Basis: Global word–word co-occurrence matrix  \n",
        "Innovation Vector: Bridged LSA and Word2Vec; leveraged global corpus statistics  \n",
        "Limitation Vector: Memory heavy for large vocabularies; context-independent  \n",
        "Legacy: Stable pretrained embeddings for NLP tasks; mathematically interpretable  \n",
        "Key Equation:  \n",
        "$$\n",
        "w_i^\\top w_j + b_i + b_j \\approx \\log X_{ij}\n",
        "$$  \n",
        "Conceptual Distance to BERT: 0.75  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Bojanowski et al. (2017) — fastText: Subword Information\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2017  \n",
        "Core Architecture: Subword-augmented Skip-gram  \n",
        "Representation Type: Compositional Static Embeddings  \n",
        "Objective Function: Negative sampling with character n-gram vectors  \n",
        "Data Basis: Word and subword n-grams  \n",
        "Innovation Vector: Handles rare and out-of-vocabulary words; models morphology  \n",
        "Limitation Vector: Ignores context; increased training cost  \n",
        "Legacy: Multilingual embedding robustness; de facto model for morphologically rich languages  \n",
        "Composition Function:  \n",
        "$$\n",
        "v(w) = \\sum_{g \\in G(w)} z_g\n",
        "$$  \n",
        "Conceptual Distance to BERT: 0.65  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Peters et al. (2018) — ELMo: Deep Contextualized Word Representations\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2018  \n",
        "Core Architecture: Bidirectional LSTM Language Model  \n",
        "Representation Type: Dynamic Contextual Embeddings  \n",
        "Objective Function: Language modeling (forward + backward)  \n",
        "Innovation Vector: Context-dependent vectors; layer-weighted embedding fusion  \n",
        "Limitation Vector: Sequential RNN architecture; limited long-range context  \n",
        "Legacy: Introduced contextual embeddings; bridge to the transformer era  \n",
        "Embedding Function:  \n",
        "$$\n",
        "ELMo(word_t) = \\sum_j s_j \\times h_j(word_t)\n",
        "$$  \n",
        "Transferability: High — used across diverse NLP tasks  \n",
        "Conceptual Distance to BERT: 0.35  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Devlin et al. (2018) — BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "**Vector Representation (Conceptual Embedding):**\n",
        "\n",
        "Year: 2018  \n",
        "Core Architecture: Transformer Encoder (Multi-Head Self-Attention)  \n",
        "Representation Type: Deep Contextual Embeddings  \n",
        "Objective Function: Masked Language Modeling and Next Sentence Prediction  \n",
        "Innovation Vector: Bidirectional contextualization; parallel attention; massive pretraining  \n",
        "Limitation Vector: Resource-intensive; context window limited to 512 tokens  \n",
        "Legacy: Unified pretraining–finetuning paradigm; backbone of modern LLMs  \n",
        "Key Equation:  \n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$  \n",
        "Tokenization: Subword (WordPiece)  \n",
        "Conceptual Distance to Bengio (2003): 0.1  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evolution Vector Trajectory (Simplified)\n",
        "\n",
        "| Era | Representative Paper | Representation Type | Context Handling | Main Innovation | Vector Shift Direction |\n",
        "|------|----------------------|----------------------|------------------|------------------|------------------------|\n",
        "| 2003 | Bengio et al. | Static | Local (n-gram) | Continuous word representations | → Distributed space |\n",
        "| 2013 | Mikolov et al. | Static | Local (window) | Skip-gram & CBOW | → Efficient scaling |\n",
        "| 2014 | Pennington et al. (GloVe) | Static | Global | Co-occurrence matrix factorization | → Global semantics |\n",
        "| 2017 | Bojanowski et al. (fastText) | Static | Local + Subword | Morphological composition | → Morphological axis |\n",
        "| 2018 | Peters et al. (ELMo) | Contextual | Sentence | BiLSTM contextual dynamics | → Contextualization axis |\n",
        "| 2018 | Devlin et al. (BERT) | Contextual | Bidirectional | Transformer attention | → Deep contextual manifold |\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Conceptual Embedding Space (Qualitative Map)\n",
        "\n",
        "**Principal Components of Embedding Evolution:**\n",
        "\n",
        "| Principal Component (Axis) | Interpretation | Dominant Models |\n",
        "|-----------------------------|----------------|-----------------|\n",
        "| PC₁ — Local → Global Semantics | From word-window context to full corpus co-occurrence | Word2Vec → GloVe |\n",
        "| PC₂ — Static → Contextual | From fixed vectors to context-dependent embeddings | fastText → ELMo → BERT |\n",
        "| PC₃ — Shallow → Deep | From single-layer networks to deep transformers | Word2Vec → ELMo → BERT |\n",
        "| PC₄ — Symbolic → Subword | From whole-word tokens to character/subword units | fastText → BERT |\n",
        "| PC₅ — Autoregressive → Masked/Bidirectional | From unidirectional to fully bidirectional modeling | ELMo → BERT |\n",
        "\n",
        "Each model represents a displacement in this conceptual manifold:\n",
        "\n",
        "$$\n",
        "\\vec{BERT} - \\vec{Word2Vec} = Δ_{contextuality} + Δ_{depth} + Δ_{attention}\n",
        "$$\n",
        "\n",
        "This equation symbolizes the paradigm shift from static, local, and shallow models to deep, bidirectional, and context-sensitive embeddings that define the foundation of modern language understanding.\n"
      ],
      "metadata": {
        "id": "S2sbVD1_l72r"
      }
    }
  ]
}