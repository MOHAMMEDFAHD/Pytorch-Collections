# ğŸ§  Transformers: From *Attention Is All You Need* to Foundation Models (2017â€“2025)

Welcome to the **Transformer Replication Labs** of the **Programming Ocean Academy** ğŸŒŠ  
This repository is a **pedagogical and experimental collection** tracing the evolution of Transformer-based models â€” from the 2017 breakthrough paper to todayâ€™s multimodal foundation giants.

---

## ğŸ“˜ What This Repo Offers

- âœ… **Faithful replications** of major Transformer papers  
- ğŸ§‘â€ğŸ« **Clean PyTorch implementations** designed for education and experimentation  
- ğŸ“Š **Training, evaluation, and visualization** pipelines  
- ğŸ”­ **Historical perspective** on how attention reshaped AI â€” in NLP, CV, and multimodal learning

---

## ğŸ“– Background: Why Transformers Changed Everything

Before 2017, sequence modeling relied heavily on **RNNs** (LSTM, GRU) and **CNNs** (ConvS2S). These architectures struggled with:

- Long-range dependencies  
- Lack of parallelism  
- Memory and computation limits  

Then came the game-changer:

### âœ¨ *Attention Is All You Need* (Vaswani et al., 2017)

- Replaced recurrence/convolutions with **multi-head self-attention**  
- Introduced **positional encoding** and **layer normalization**  
- Enabled **massive parallelism** in training  
- Sparked models like **BERT, GPT, ViT, T5, and LLaMA**

---

## ğŸ“‚ Repository Structure

```bash
transformers/
â”œâ”€â”€ AttentionIsAllYouNeed/       # Original Transformer replication
â”œâ”€â”€ BERT/                        # Bidirectional encoding (Devlin et al.)
â”œâ”€â”€ GPT/                         # Generative Pre-Training (GPT-1 â†’ GPT-3)
â”œâ”€â”€ ViT/                         # Vision Transformers (Dosovitskiy et al.)
â”œâ”€â”€ ALBERT/                      # Lite BERT variant
â”œâ”€â”€ T5/                          # Text-to-text multitask transformer
â”œâ”€â”€ LLaMA/                       # Efficient open foundation models
â”œâ”€â”€ Transformer++/              # Architectural refinements
â”œâ”€â”€ Utilities/                   # Tokenizers, training scripts, visualization
â””â”€â”€ README.md


## ğŸ“‘ Key Replicated Papers

| Year | Paper                        | Authors                | Highlights                                              |
|------|------------------------------|------------------------|---------------------------------------------------------|
| 2017 | *Attention Is All You Need*  | Vaswani et al.         | Introduced self-attention, transformer encoder-decoder  |
| 2018 | *GPT-1*                      | Radford & Narasimhan   | First autoregressive transformer                        |
| 2018 | *BERT*                       | Devlin et al.          | Bidirectional masked pre-training                       |
| 2020 | *GPT-3*                      | Brown et al.           | Massive model enabling few-shot learning                |
| 2020 | *ViT*                        | Dosovitskiy et al.     | First successful vision transformer                     |
| 2019 | *ALBERT*                     | Lan et al.             | Parameter-sharing for efficient BERT                    |
| 2020 | *T5*                         | Raffel et al.          | Unified text-to-text pretraining                        |
| 2021 | *PVT*                        | Wang et al.            | Hierarchical vision transformer                         |
| 2023 | *LLaMA*                      | Touvron et al.         | Open-access foundation models for efficient fine-tuning |

---

## ğŸ”¬ Methodology

Each notebook follows a pedagogical development cycle:

### ğŸ”¢ Mathematical Foundations

- **Self-Attention**  
  \[
  \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V
  \]

- **Multi-Head Attention**  
- **Position-wise Feedforward Networks**  
- **Layer Normalization**  
- **Positional Encoding**

---

### ğŸ§° Implementation

- PyTorch-first, clean, modular code  
- Core components: `MultiHeadAttention`, `EncoderBlock`, `TransformerEncoder`  
- Reproducible training loops with flexible configs

---

### ğŸ“ˆ Evaluation

- **BLEU** â€” Translation performance  
- **Accuracy** â€” Classification models  
- **Perplexity** â€” Language modeling  

---

### ğŸ” Visualization

- Attention heatmaps  
- Token-level saliency maps  
- PCA/t-SNE of hidden states  

---

## ğŸ“Š Results Snapshot

- ğŸš€ Transformer (2017) achieves **SOTA BLEU** on WMTâ€™14 Enâ€“De & Enâ€“Fr  
- ğŸ’¬ GPT-3 enables **few-shot and in-context learning**  
- ğŸ§  BERT pretraining creates **universal NLP representations**  
- ğŸ–¼ï¸ ViT competes with or exceeds **ResNet** on ImageNet  

---

## ğŸŒ Impact & Legacy

Transformers are now the **core infrastructure of modern AI** across domains:

| Domain     | Key Models                              |
|------------|------------------------------------------|
| NLP        | BERT, GPT, T5, LLaMA, PaLM               |
| Vision     | ViT, DETR, Swin, PVT                     |
| Multimodal | CLIP, Flamingo, GPT-4, Gemini            |
| Code       | Codex, CodeT5, StarCoder                 |
| Agents     | ReAct, Toolformer, Voyager               |

---

## ğŸš€ Getting Started

Clone the repo and install dependencies:

```bash
git clone https://github.com/Programming-Ocean-Academy/deep-learning/transformers
cd transformers
pip install -r requirements.txt

## ğŸ“š References

- Vaswani et al. (2017) â€” *Attention Is All You Need*  
- Devlin et al. (2019) â€” *BERT*  
- Radford et al. (2018â€“2020), Brown et al. â€” *GPT-1 â†’ GPT-3*  
- Dosovitskiy et al. (2020) â€” *Vision Transformer (ViT)*  
- Lan et al. (2019) â€” *ALBERT*  
- Raffel et al. (2020) â€” *T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*  
- Touvron et al. (2023) â€” *LLaMA: Open and Efficient Foundation Models*

ğŸ—‚ï¸ *See [`docs/related_work.md`](docs/related_work.md) for the full list of papers.*

---

## âœ¨ Mission

This repository is both a **research chronicle** and an **educational playground**.

We aim to:

- ğŸ” Demystify attention mechanisms and Transformer architecture  
- ğŸ§  Provide **clear, first-principles implementations**  
- ğŸš€ Empower learners and researchers to build on these foundations

---

ğŸ‘©â€ğŸ« **Mohammed Fahd Abrah** â€” *Teaching AI by Recreating Its History*  
**Replicate. Understand. Extend.** ğŸ”¨ğŸ¤–ğŸ”§

