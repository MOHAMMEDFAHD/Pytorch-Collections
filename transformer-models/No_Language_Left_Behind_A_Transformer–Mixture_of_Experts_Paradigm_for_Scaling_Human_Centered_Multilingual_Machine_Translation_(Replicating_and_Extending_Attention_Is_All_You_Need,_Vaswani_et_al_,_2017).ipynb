{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# No Language Left Behind: Scaling Human-Centered Machine Translation\n",
        "\n",
        "# https://arxiv.org/abs/2207.04672\n",
        "\n",
        "# Abstract\n",
        "\n",
        "The No Language Left Behind project addresses the inequity in machine translation (MT) by expanding support to over 200 languages, most of which are low-resource. Through novel data creation, modeling, and evaluation, the project achieves a 44% BLEU improvement over prior state-of-the-art, while prioritizing human-centered design, safety, and open access\n",
        "\n",
        "**No Language Left Behind**\n",
        "\n",
        "---\n",
        "\n",
        "# Problems\n",
        "\n",
        "- **Unequal benefits:** MT progress has disproportionately favored high-resource languages, leaving low-resource communities marginalized.  \n",
        "- **Data scarcity:** Training and evaluation resources for low-resource languages are limited, costly, and inconsistent.  \n",
        "- **Quality and safety issues:** Existing systems produce poor, biased, or even toxic translations for low-resource languages, eroding user trust.  \n",
        "- **Ethical and cultural risks:** Without careful design, MT may reinforce digital inequalities and cultural displacement  \n",
        "\n",
        "**No Language Left Behind**\n",
        "\n",
        "---\n",
        "\n",
        "# Purposes\n",
        "\n",
        "- To break the 200-language barrier while ensuring safe, high-quality translations.  \n",
        "- To design translation systems that are human-centered, inclusive of underserved communities, and ethically responsible.  \n",
        "- To create open-source datasets, benchmarks, and models that advance multilingual MT research and practical use.  \n",
        "\n",
        "---\n",
        "\n",
        "# Methodologies\n",
        "\n",
        "### Human-Centered Design\n",
        "- Conducted 44 interviews with native speakers of 36 low-resource languages to guide principles and identify needs.  \n",
        "\n",
        "### Data Creation\n",
        "- Built **Flores-200**, a benchmark with 40k translation directions in 204 languages.  \n",
        "- Developed **NLLB-Seed** and **NLLB-MD** professionally translated datasets.  \n",
        "- Introduced **LASER3 encoders** and large-scale mining pipelines for low-resource bitexts.  \n",
        "\n",
        "### Modeling Innovations\n",
        "- Used **Sparsely Gated Mixture of Experts** (conditional compute).  \n",
        "- Applied **self-supervised training, backtranslation, and curriculum learning**.  \n",
        "- Mitigated overfitting with **data augmentation and bootstrap strategies**.  \n",
        "\n",
        "### Evaluation\n",
        "- Combined **automatic metrics** (BLEU, etc.) with **human evaluation** across thousands of directions.  \n",
        "- Built **Toxicity-200**, a multilingual toxicity benchmark to assess harmful outputs.  \n",
        "\n",
        "---\n",
        "\n",
        "# Results\n",
        "\n",
        "- **Coverage:** Translation expanded to 202 languages, doubling prior benchmarks.  \n",
        "- **Performance:** NLLB models achieved **44% BLEU gains** over the prior state-of-the-art on Flores benchmarks.  \n",
        "- **Safety:** Toxicity detection reduced harmful outputs across languages.  \n",
        "- **Accessibility:** Released models from large (54.5B MoE) to distilled smaller variants for broader usability.  \n",
        "- **Impact:** Deployment improved translation support for low-resource communities, e.g., enabling Wikipedia content creation  \n",
        "\n",
        "**No Language Left Behind**\n",
        "\n",
        "---\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "The NLLB initiative demonstrates that scaling multilingual MT beyond 200 languages is feasible when guided by human-centered, ethical, and open-source principles. By integrating sociotechnical insights, robust datasets, innovative modeling, and rigorous evaluation, NLLB sets a foundation for universal, safe, and equitable translation systems. Future work must continue balancing scale, inclusivity, and responsibility, recognizing MT as not just a technical challenge but a cultural and social one  \n",
        "\n",
        "**No Language Left Behind**"
      ],
      "metadata": {
        "id": "0zAfsXSrrFwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ Mathematical & Statistical Equations in *No Language Left Behind (NLLB)*\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Evaluation Metrics\n",
        "\n",
        "### BLEU (Bilingual Evaluation Understudy)\n",
        "Used as the core automatic metric for translation quality.\n",
        "\n",
        "$$\n",
        "BLEU = BP \\cdot \\exp \\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
        "$$\n",
        "\n",
        "- \\(p_n\\): n-gram precision  \n",
        "- \\(w_n\\): weight for each n-gram (typically \\(w_n = \\frac{1}{N}\\))  \n",
        "- \\(BP\\): brevity penalty  \n",
        "\n",
        "Brevity penalty:\n",
        "\n",
        "$$\n",
        "BP =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } c > r \\\\\n",
        "e^{(1-r/c)}, & \\text{if } c \\leq r\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where \\(c\\) = candidate translation length, \\(r\\) = reference length.\n",
        "\n",
        "---\n",
        "\n",
        "### ChrF (Character F-score)\n",
        "Character-level evaluation (robust for morphologically rich and low-resource languages).\n",
        "\n",
        "$$\n",
        "ChrF = \\frac{(1+\\beta^2) \\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}\n",
        "$$\n",
        "\n",
        "with \\(\\beta = 2\\) commonly used.\n",
        "\n",
        "---\n",
        "\n",
        "### COMET / Human Agreement Metrics\n",
        "While not explicit equations, they rely on **correlation coefficients** (Pearson/Spearman):\n",
        "\n",
        "$$\n",
        "\\rho = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_i (y_i - \\bar{y})^2}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Mixture of Experts (MoE) â€“ Conditional Compute\n",
        "\n",
        "NLLB models use Sparsely Gated Mixture of Experts:\n",
        "\n",
        "$$\n",
        "y = \\sum_{i=1}^{k} g_i(x) \\cdot f_i(x)\n",
        "$$\n",
        "\n",
        "- \\(f_i(x)\\): expert function (neural sub-network)  \n",
        "- \\(g_i(x)\\): gating probability (sparse, softmaxed)  \n",
        "\n",
        "Gating mechanism:\n",
        "\n",
        "$$\n",
        "g(x) = Softmax(W_g x)\n",
        "$$\n",
        "\n",
        "Only top-\\(k\\) experts are activated, reducing compute.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Self-Supervised Objectives\n",
        "\n",
        "### Masked Language Modeling (MLM)\n",
        "\n",
        "$$\n",
        "L_{MLM} = - \\sum_{t \\in M} \\log P(x_t \\mid x \\setminus M)\n",
        "$$\n",
        "\n",
        "- \\(M\\): set of masked tokens  \n",
        "- \\(x_t\\): true token  \n",
        "- \\(x \\setminus M\\): observed sequence with masked positions  \n",
        "\n",
        "---\n",
        "\n",
        "### Backtranslation\n",
        "\n",
        "Synthetic parallel data generation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = MT(x), \\quad D_{bt} = \\{ (x, \\hat{y}) \\}\n",
        "$$\n",
        "\n",
        "where a source-side monolingual corpus \\(x\\) is translated into synthetic target \\(\\hat{y}\\).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Data Mining â€“ LASER Sentence Similarity\n",
        "\n",
        "Parallel sentence mining uses cosine similarity in multilingual embedding space:\n",
        "\n",
        "$$\n",
        "sim(x, y) = \\frac{\\langle h(x), h(y) \\rangle}{\\|h(x)\\| \\cdot \\|h(y)\\|}\n",
        "$$\n",
        "\n",
        "- \\(h(x)\\): sentence embedding of \\(x\\)  \n",
        "- \\(h(y)\\): sentence embedding of \\(y\\)  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Toxicity Scoring\n",
        "\n",
        "Defined as proportion of toxic tokens flagged:\n",
        "\n",
        "$$\n",
        "Toxicity(x) = \\frac{count\\_toxic\\_tokens(x)}{total\\_tokens(x)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Statistical Sampling â€“ FLORES-200\n",
        "\n",
        "Dataset sampling from multilingual Wikipedia follows **stratified sampling**:\n",
        "\n",
        "$$\n",
        "P(select \\; s_i) = \\frac{1}{|S|}\n",
        "$$\n",
        "\n",
        "ensuring balanced representation across Wikinews, Wikijunior, Wikivoyage.\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Summary\n",
        "\n",
        "The main mathematics in the NLLB project centers on:\n",
        "\n",
        "- **Evaluation metrics:** BLEU, ChrF, correlation measures  \n",
        "- **Modeling equations:** Mixture-of-Experts, masked language modeling, backtranslation  \n",
        "- **Data mining:** cosine similarity in multilingual embeddings  \n",
        "- **Safety metrics:** toxicity ratio  \n",
        "\n",
        "These are primarily **statistical and empirical** â€” designed for **benchmarking, scaling, and validation** of multilingual MT rather than theoretical model proofs."
      ],
      "metadata": {
        "id": "zru2Wt3csqsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Evaluation Metrics for Machine Translation Models\n",
        "\n",
        "| Metric | Type | Formula / Core Idea | Strengths | Limitations | Best Use |\n",
        "|--------|------|----------------------|-----------|-------------|----------|\n",
        "| **BLEU (Papineni et al., 2002)** | n-gram precision | $$BLEU = BP \\cdot \\exp \\left(\\sum_{n=1}^{N} w_n \\log p_n \\right)$$ with brevity penalty \\(BP\\) | Widely adopted, simple, interpretable n-gram match | Insensitive to synonyms/paraphrases, favors shorter outputs, surface-level only | Benchmarking high-resource MT; legacy SOTA reporting |\n",
        "| **ChrF / ChrF++ (PopoviÄ‡, 2015)** | Character n-gram F-score | $$F_\\beta = \\frac{(1+\\beta^2)\\cdot P \\cdot R}{\\beta^2 P + R}$$ over char n-grams | Captures morphology and inflections; language-agnostic | Less intuitive than BLEU; slower on long sequences | Evaluating morphologically rich or low-resource languages |\n",
        "| **METEOR (Banerjee & Lavie, 2005)** | Word alignment with stemming & synonyms | Harmonic mean of unigram precision & recall + penalty | Handles synonyms & word order better than BLEU | More complex, language resources needed | Research on fluency/adequacy; English-focused corpora |\n",
        "| **ROUGE (Lin, 2004)** | Recall-oriented n-gram overlap | $$ROUGE\\text{-}N = \\frac{\\sum \\# \\text{overlap n-grams}}{\\sum \\# \\text{ref n-grams}}$$ | Good for summarization; recall-focused | Biased toward recall, ignores precision, less common in MT | MT-summarization hybrids; adequacy-sensitive tasks |\n",
        "| **TER (Translation Edit Rate) (Snover et al., 2006)** | Edit distance | $$TER = \\frac{\\#edits}{\\#reference\\;words}$$ | Intuitive: number of edits to match ref | Harsh on valid paraphrases; does not reward fluency | Post-editing cost estimation, human-in-the-loop MT |\n",
        "| **COMET (Rei et al., 2020)** | Neural learned metric | Pretrained encoder (XLM-R, etc.) + regression to human judgments | Correlates best with human evaluations | Requires GPU; less interpretable | SOTA academic/industrial MT evaluation |\n",
        "| **BLEURT (Sellam et al., 2020)** | Neural, fine-tuned on human judgments | BERT-based model scoring reference â†” hypothesis | High correlation with human ratings | Black-box, resource heavy | MT research where human-like evaluation is critical |\n",
        "| **Human Evaluation** | Subjective adequacy + fluency | Direct human judgments on scales (1â€“5) or pairwise | Gold standard; captures nuances | Expensive, slow, inconsistent across annotators | Benchmarking final production systems (NLLB, Google Translate, DeepL) |\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… Academic Insights\n",
        "\n",
        "- **BLEU** remains the de facto legacy benchmark but is now often complemented by **ChrF** (for morphology) and **COMET/BLEURT** (for correlation with human ratings).  \n",
        "- **TER** is practical in industry for estimating post-editing effort.  \n",
        "- **Human evaluation** is still the ultimate **gold standard**, especially for low-resource languages where automatic metrics may fail.  \n",
        "- **NLLB (Meta, 2022)** reported results across **BLEU, ChrF, COMET, and human judgments** to capture both automatic alignment and real-world adequacy/fluency."
      ],
      "metadata": {
        "id": "IPSdZDrpuAeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Machine Translation Evaluation Metrics â€” Academic Summary\n",
        "\n",
        "- **BLEU:** Measures n-gram precision between system output and references, with a brevity penalty. Legacy standard, but surface-level.  \n",
        "\n",
        "- **ChrF / ChrF++:** Uses character n-gram F-scores to capture morphology and inflection differences; better for rich languages.  \n",
        "\n",
        "- **METEOR:** Aligns words with stemming and synonym matching; combines precision, recall, and fragmentation penalty for fluency.  \n",
        "\n",
        "- **ROUGE:** Recall-based n-gram overlap metric, originally for summarization, sometimes used to check adequacy in MT.  \n",
        "\n",
        "- **TER (Translation Edit Rate):** Counts the minimum edits (insertions, deletions, substitutions, shifts) needed to match a reference.  \n",
        "\n",
        "- **COMET:** Neural metric using multilingual encoders (e.g., XLM-R) trained to predict human adequacy/fluency judgments.  \n",
        "\n",
        "- **BLEURT:** BERT-based learned metric fine-tuned on human ratings, designed to reflect semantic adequacy and fluency.  \n",
        "\n",
        "- **Human Evaluation:** Direct human judgment of adequacy (meaning preserved) and fluency (naturalness), considered the gold standard."
      ],
      "metadata": {
        "id": "75asK5VHuZ8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ Mini NLLB Replication (PyTorch, toy ENâ†’DE translation)\n",
        "import math, random, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Synthetic Parallel Data\n",
        "# ----------------------------\n",
        "subjects = [\"i\",\"you\",\"he\",\"she\",\"we\",\"they\"]\n",
        "verbs = {\"like\":\"mag\",\"see\":\"sehe\",\"eat\":\"esse\",\"want\":\"will\",\"know\":\"kenne\"}\n",
        "objects = {\"apples\":\"Ã¤pfel\",\"bread\":\"brot\",\"books\":\"bÃ¼cher\",\"music\":\"musik\",\"coffee\":\"kaffee\"}\n",
        "\n",
        "def make_pair():\n",
        "    s = random.choice(subjects)\n",
        "    v = random.choice(list(verbs.keys()))\n",
        "    o = random.choice(list(objects.keys()))\n",
        "    src = f\"{s} {v} {o}\"\n",
        "    tgt = f\"{ {'i':'ich','you':'du','he':'er','she':'sie','we':'wir','they':'sie'}[s] } {verbs[v]} {objects[o]}\"\n",
        "    return src, tgt\n",
        "\n",
        "pairs = [make_pair() for _ in range(500)]\n",
        "train, test = pairs[:400], pairs[400:]\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Vocab\n",
        "# ----------------------------\n",
        "PAD,BOS,EOS = \"<pad>\",\"<bos>\",\"<eos>\"\n",
        "def build_vocab(sents):\n",
        "    toks = set(w for s in sents for w in s.split())\n",
        "    itos = [PAD,BOS,EOS] + sorted(list(toks))\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    return itos, stoi\n",
        "\n",
        "src_itos, src_stoi = build_vocab([s for s,_ in train])\n",
        "tgt_itos, tgt_stoi = build_vocab([t for _,t in train])\n",
        "\n",
        "def encode(s,stoi): return [stoi[w] for w in s.split()]\n",
        "def detok(ids,itos): return \" \".join([itos[i] for i in ids if i<len(itos) and itos[i] not in [PAD,BOS,EOS]])\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Dataset\n",
        "# ----------------------------\n",
        "class Parallel(Dataset):\n",
        "    def __init__(self,pairs): self.p=pairs\n",
        "    def __len__(self): return len(self.p)\n",
        "    def __getitem__(self,i):\n",
        "        s,t=self.p[i]\n",
        "        s=[src_stoi[w] for w in s.split()]\n",
        "        t=[tgt_stoi[BOS]]+[tgt_stoi[w] for w in t.split()]+[tgt_stoi[EOS]]\n",
        "        return torch.tensor(s),torch.tensor(t)\n",
        "\n",
        "def collate(batch):\n",
        "    src,tgt=zip(*batch)\n",
        "    sl=max(len(s) for s in src); tl=max(len(t) for t in tgt)\n",
        "    src_out=torch.full((len(batch),sl),src_stoi[PAD])\n",
        "    tgt_out=torch.full((len(batch),tl),tgt_stoi[PAD])\n",
        "    for i,(s,t) in enumerate(zip(src,tgt)):\n",
        "        src_out[i,:len(s)]=s; tgt_out[i,:len(t)]=t\n",
        "    return src_out,tgt_out\n",
        "\n",
        "train_loader=DataLoader(Parallel(train),batch_size=32,shuffle=True,collate_fn=collate)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Model (Transformer+MoE)\n",
        "# ----------------------------\n",
        "class MoE(nn.Module):\n",
        "    def __init__(self,d,num_experts=2):\n",
        "        super().__init__(); self.experts=nn.ModuleList([nn.Linear(d,d) for _ in range(num_experts)])\n",
        "        self.gate=nn.Linear(d,num_experts)\n",
        "    def forward(self,x):\n",
        "        g=F.softmax(self.gate(x),-1) # (B,T,E)\n",
        "        outs=sum(g[...,i:i+1]*self.experts[i](x) for i in range(len(self.experts)))\n",
        "        return outs\n",
        "\n",
        "class MT(nn.Module):\n",
        "    def __init__(self,srcV,tgtV,d=64,h=2,L=2):\n",
        "        super().__init__()\n",
        "        self.srcE=nn.Embedding(srcV,d); self.tgtE=nn.Embedding(tgtV,d)\n",
        "        self.trans=nn.Transformer(d_model=d,nhead=h,num_encoder_layers=L,num_decoder_layers=L,batch_first=True)\n",
        "        self.moe=MoE(d); self.fc=nn.Linear(d,tgtV)\n",
        "    def forward(self,src,tgt):\n",
        "        s=self.srcE(src); t=self.tgtE(tgt)\n",
        "        out=self.trans(s,t); out=self.moe(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "model=MT(len(src_itos),len(tgt_itos)).to(\"cpu\")\n",
        "opt=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "pad=tgt_stoi[PAD]\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Train\n",
        "# ----------------------------\n",
        "for ep in range(10):\n",
        "    model.train(); tot=0\n",
        "    for s,t in train_loader:\n",
        "        dec_in,targ=t[:,:-1],t[:,1:]\n",
        "        opt.zero_grad()\n",
        "        out=model(s,dec_in)\n",
        "        loss=F.cross_entropy(out.reshape(-1,out.size(-1)),targ.reshape(-1),ignore_index=pad)\n",
        "        loss.backward(); opt.step()\n",
        "        tot+=loss.item()\n",
        "    print(f\"Epoch {ep+1}, loss {tot/len(train_loader):.3f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Predict\n",
        "# ----------------------------\n",
        "def translate(sent):\n",
        "    model.eval()\n",
        "    src=torch.tensor([encode(sent,src_stoi)])\n",
        "    ys=torch.tensor([[tgt_stoi[BOS]]])\n",
        "    for _ in range(10):\n",
        "        out=model(src,ys)\n",
        "        next=out[0,-1].argmax().unsqueeze(0).unsqueeze(0)\n",
        "        ys=torch.cat([ys,next],1)\n",
        "        if next.item()==tgt_stoi[EOS]: break\n",
        "    return detok(ys[0].tolist(),tgt_itos)\n",
        "\n",
        "for s,t in test[:5]:\n",
        "    print(\"SRC:\",s)\n",
        "    print(\"REF:\",t)\n",
        "    print(\"HYP:\",translate(s))\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmqJUIbqEaU",
        "outputId": "59dbb1bc-0068-4be3-ccc1-65f3b03da942"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, loss 1.881\n",
            "Epoch 2, loss 0.698\n",
            "Epoch 3, loss 0.205\n",
            "Epoch 4, loss 0.063\n",
            "Epoch 5, loss 0.029\n",
            "Epoch 6, loss 0.017\n",
            "Epoch 7, loss 0.013\n",
            "Epoch 8, loss 0.010\n",
            "Epoch 9, loss 0.008\n",
            "Epoch 10, loss 0.007\n",
            "SRC: you know bread\n",
            "REF: du kenne brot\n",
            "HYP: du kenne brot\n",
            "----\n",
            "SRC: he want music\n",
            "REF: er will musik\n",
            "HYP: er will musik\n",
            "----\n",
            "SRC: he want books\n",
            "REF: er will bÃ¼cher\n",
            "HYP: er will bÃ¼cher\n",
            "----\n",
            "SRC: she know coffee\n",
            "REF: sie kenne kaffee\n",
            "HYP: sie kenne kaffee\n",
            "----\n",
            "SRC: she like books\n",
            "REF: sie mag bÃ¼cher\n",
            "HYP: sie mag bÃ¼cher\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– Academic Explanation of Results\n",
        "\n",
        "## 1. Training Behavior\n",
        "- The model begins with a loss â‰ˆ1.88 and steadily converges to â‰ˆ0.007 by epoch 10.  \n",
        "- The curve is **monotonic and smooth**, showing stable optimization without divergence.  \n",
        "- The near-zero loss suggests the model has almost perfectly learned the training distribution â€” unsurprising given the **synthetic and deterministic dataset**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Prediction Quality\n",
        "\n",
        "### Lexical Fidelity\n",
        "Hypotheses (HYP) closely mirror the reference translations (REF). For instance:  \n",
        "\n",
        "- â€œhe want booksâ€ â†’ **REF:** â€œer will bÃ¼cherâ€, **HYP:** â€œer will bÃ¼cherâ€ (*perfect match*).  \n",
        "- â€œshe like booksâ€ â†’ **REF:** â€œsie mag bÃ¼cherâ€, **HYP:** â€œsie mag bÃ¼cherâ€ (*perfect match*).  \n",
        "\n",
        "### Systematic Errors\n",
        "Minor discrepancies are visible, e.g.:  \n",
        "\n",
        "- â€œyou know breadâ€ â†’ **REF:** â€œdu kenne brotâ€, **HYP:** â€œdu kenne brotâ€  \n",
        "  â†’ same output, but linguistically *â€œkennstâ€* would be the correct German verb form.  \n",
        "\n",
        "This reflects that the model **memorized synthetic mappings** rather than acquiring full grammar.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Generalization and Overfitting\n",
        "- The predictions align well with references, indicating **good generalization** across the held-out test set.  \n",
        "- However, since the dataset is **synthetic and constrained**, this generalization is more about memorization of deterministic patterns than robust linguistic competence.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Metric Implications\n",
        "- **BLEU and ChrF scores** (not shown in screenshot but computable) would be near **100**, because of exact matches in many sentences.  \n",
        "- This highlights a **ceiling effect**: with a small, regular dataset, the model saturates quickly, unlike real-world multilingual translation where performance differences are more nuanced.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Theoretical Framing\n",
        "- The results confirm the **inductive strength of Transformer architectures with Mixture-of-Experts (MoE):** even a miniature version captures alignment and translation mapping very efficiently.  \n",
        "- At the same time, they underscore the **limitations of synthetic setups:** success here cannot be directly extrapolated to real-world multilingual challenges like those addressed in *No Language Left Behind*.  \n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Conclusion\n",
        "The loss trajectory and translation outputs jointly demonstrate that the **mini-NLLB replication converged effectively**, producing accurate translations on the toy dataset.  \n",
        "The experiment validates the architectureâ€™s ability to learn **structured mappings**, while also exposing the gap between **synthetic memorization** and **genuine multilingual generalization**.  "
      ],
      "metadata": {
        "id": "6RQbIlAhsR8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ Mini NLLB Replication with Visualization\n",
        "import math, random, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Synthetic Parallel Data\n",
        "# ----------------------------\n",
        "subjects = [\"i\",\"you\",\"he\",\"she\",\"we\",\"they\"]\n",
        "verbs = {\"like\":\"mag\",\"see\":\"sehe\",\"eat\":\"esse\",\"want\":\"will\",\"know\":\"kenne\"}\n",
        "objects = {\"apples\":\"Ã¤pfel\",\"bread\":\"brot\",\"books\":\"bÃ¼cher\",\"music\":\"musik\",\"coffee\":\"kaffee\"}\n",
        "\n",
        "def make_pair():\n",
        "    s = random.choice(subjects)\n",
        "    v = random.choice(list(verbs.keys()))\n",
        "    o = random.choice(list(objects.keys()))\n",
        "    src = f\"{s} {v} {o}\"\n",
        "    tgt = f\"{ {'i':'ich','you':'du','he':'er','she':'sie','we':'wir','they':'sie'}[s] } {verbs[v]} {objects[o]}\"\n",
        "    return src, tgt\n",
        "\n",
        "pairs = [make_pair() for _ in range(500)]\n",
        "train, val = pairs[:400], pairs[400:450]\n",
        "test = pairs[450:]\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Vocab\n",
        "# ----------------------------\n",
        "PAD,BOS,EOS = \"<pad>\",\"<bos>\",\"<eos>\"\n",
        "def build_vocab(sents):\n",
        "    toks = set(w for s in sents for w in s.split())\n",
        "    itos = [PAD,BOS,EOS] + sorted(list(toks))\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    return itos, stoi\n",
        "\n",
        "src_itos, src_stoi = build_vocab([s for s,_ in train])\n",
        "tgt_itos, tgt_stoi = build_vocab([t for _,t in train])\n",
        "\n",
        "def encode(s,stoi): return [stoi[w] for w in s.split()]\n",
        "def detok(ids,itos): return \" \".join([itos[i] for i in ids if i<len(itos) and itos[i] not in [PAD,BOS,EOS]])\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Dataset\n",
        "# ----------------------------\n",
        "class Parallel(Dataset):\n",
        "    def __init__(self,pairs): self.p=pairs\n",
        "    def __len__(self): return len(self.p)\n",
        "    def __getitem__(self,i):\n",
        "        s,t=self.p[i]\n",
        "        s=[src_stoi[w] for w in s.split()]\n",
        "        t=[tgt_stoi[BOS]]+[tgt_stoi[w] for w in t.split()]+[tgt_stoi[EOS]]\n",
        "        return torch.tensor(s),torch.tensor(t)\n",
        "\n",
        "def collate(batch):\n",
        "    src,tgt=zip(*batch)\n",
        "    sl=max(len(s) for s in src); tl=max(len(t) for t in tgt)\n",
        "    src_out=torch.full((len(batch),sl),src_stoi[PAD])\n",
        "    tgt_out=torch.full((len(batch),tl),tgt_stoi[PAD])\n",
        "    for i,(s,t) in enumerate(zip(src,tgt)):\n",
        "        src_out[i,:len(s)]=s; tgt_out[i,:len(t)]=t\n",
        "    return src_out,tgt_out\n",
        "\n",
        "train_loader=DataLoader(Parallel(train),batch_size=32,shuffle=True,collate_fn=collate)\n",
        "val_loader=DataLoader(Parallel(val),batch_size=32,shuffle=False,collate_fn=collate)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Model (Transformer+MoE)\n",
        "# ----------------------------\n",
        "class MoE(nn.Module):\n",
        "    def __init__(self,d,num_experts=2):\n",
        "        super().__init__(); self.experts=nn.ModuleList([nn.Linear(d,d) for _ in range(num_experts)])\n",
        "        self.gate=nn.Linear(d,num_experts)\n",
        "    def forward(self,x):\n",
        "        g=F.softmax(self.gate(x),-1) # (B,T,E)\n",
        "        outs=sum(g[...,i:i+1]*self.experts[i](x) for i in range(len(self.experts)))\n",
        "        return outs\n",
        "\n",
        "class MT(nn.Module):\n",
        "    def __init__(self,srcV,tgtV,d=64,h=2,L=2):\n",
        "        super().__init__()\n",
        "        self.srcE=nn.Embedding(srcV,d); self.tgtE=nn.Embedding(tgtV,d)\n",
        "        self.trans=nn.Transformer(d_model=d,nhead=h,num_encoder_layers=L,num_decoder_layers=L,batch_first=True)\n",
        "        self.moe=MoE(d); self.fc=nn.Linear(d,tgtV)\n",
        "    def forward(self,src,tgt):\n",
        "        s=self.srcE(src); t=self.tgtE(tgt)\n",
        "        out=self.trans(s,t); out=self.moe(out)\n",
        "        return self.fc(out)\n",
        "\n",
        "model=MT(len(src_itos),len(tgt_itos))\n",
        "opt=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "pad=tgt_stoi[PAD]\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Train + Track Losses\n",
        "# ----------------------------\n",
        "train_losses,val_losses=[],[]\n",
        "for ep in range(10):\n",
        "    model.train(); tot=0\n",
        "    for s,t in train_loader:\n",
        "        dec_in,targ=t[:,:-1],t[:,1:]\n",
        "        opt.zero_grad()\n",
        "        out=model(s,dec_in)\n",
        "        loss=F.cross_entropy(out.reshape(-1,out.size(-1)),targ.reshape(-1),ignore_index=pad)\n",
        "        loss.backward(); opt.step(); tot+=loss.item()\n",
        "    train_losses.append(tot/len(train_loader))\n",
        "\n",
        "    # validation\n",
        "    model.eval(); vtot=0\n",
        "    with torch.no_grad():\n",
        "        for s,t in val_loader:\n",
        "            dec_in,targ=t[:,:-1],t[:,1:]\n",
        "            out=model(s,dec_in)\n",
        "            vloss=F.cross_entropy(out.reshape(-1,out.size(-1)),targ.reshape(-1),ignore_index=pad)\n",
        "            vtot+=vloss.item()\n",
        "    val_losses.append(vtot/len(val_loader))\n",
        "    print(f\"Epoch {ep+1}: train={train_losses[-1]:.3f}, val={val_losses[-1]:.3f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Visualization\n",
        "# ----------------------------\n",
        "plt.plot(train_losses,label=\"Train Loss\")\n",
        "plt.plot(val_losses,label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training/Validation Loss Curves\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Translate Samples\n",
        "# ----------------------------\n",
        "def translate(sent):\n",
        "    model.eval()\n",
        "    src=torch.tensor([encode(sent,src_stoi)])\n",
        "    ys=torch.tensor([[tgt_stoi[BOS]]])\n",
        "    for _ in range(10):\n",
        "        out=model(src,ys)\n",
        "        next=out[0,-1].argmax().unsqueeze(0).unsqueeze(0)\n",
        "        ys=torch.cat([ys,next],1)\n",
        "        if next.item()==tgt_stoi[EOS]: break\n",
        "    return detok(ys[0].tolist(),tgt_itos)\n",
        "\n",
        "for s,t in test[:5]:\n",
        "    print(\"SRC:\",s)\n",
        "    print(\"REF:\",t)\n",
        "    print(\"HYP:\",translate(s))\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "soOqezbPqvUW",
        "outputId": "df285231-7799-4b9a-b48a-70f779c8fd6e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train=2.011, val=1.139\n",
            "Epoch 2: train=0.785, val=0.327\n",
            "Epoch 3: train=0.240, val=0.086\n",
            "Epoch 4: train=0.075, val=0.030\n",
            "Epoch 5: train=0.032, val=0.015\n",
            "Epoch 6: train=0.020, val=0.010\n",
            "Epoch 7: train=0.013, val=0.007\n",
            "Epoch 8: train=0.010, val=0.006\n",
            "Epoch 9: train=0.008, val=0.005\n",
            "Epoch 10: train=0.007, val=0.004\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAa8FJREFUeJzt3Xd4FOXexvHv7qb3UNIEktBbKFIioILHKE0UbMiLh2I7KhYOYsECWLHLUVHs5XhAEBUL3SjYkF4VECRAKAk1HdJ23j+WrCxJaEl2kuz9ua65sjv7zMxvWDE3M888j8UwDAMRERERD2I1uwARERERd1MAEhEREY+jACQiIiIeRwFIREREPI4CkIiIiHgcBSARERHxOApAIiIi4nEUgERERMTjKACJiIiIx1EAEqkiI0aMIC4u7py2nThxIhaLpXILMpHFYmHixInO9x9++CEWi4UdO3acdtu4uDhGjBhRqfVU5LsRkdpBAUg8jsViOaNl8eLFZpfqNq+99hqhoaHccccdWCwWtm3bVm7bRx55BIvFwvr1691Y4dnbu3cvEydOZO3atWaX4rRjxw4sFgsvvvii2aWckfT0dMaOHUvLli0JCAggMDCQTp068dRTT5GRkWF2eSIV4mV2ASLu9t///tfl/ccff8yiRYtKrW/VqlWFjvPOO+9gt9vPadtHH32Uhx56qELHPxtz5szh8ssvZ8SIEUydOpVp06Yxfvz4MttOnz6dhIQE2rVrd87H++c//8kNN9yAr6/vOe/jdPbu3cvjjz9OXFwcHTp0cPmsIt+Np1ixYgX9+vUjJyeHG2+8kU6dOgGwcuVKnn32WX788UcWLlxocpUi504BSDzOjTfe6PL+t99+Y9GiRaXWnywvL4+AgIAzPo63t/c51Qfg5eWFl5d7/nrm5eWxZMkS3nzzTRITE2natCnTp08vMwAtXbqUlJQUnn322Qod02azYbPZKrSPiqjId+MJMjIyGDRoEDabjTVr1tCyZUuXz59++mneeeedSjlWbm4ugYGBlbIvkbOhW2AiZejVqxdt27Zl1apVXHzxxQQEBPDwww8D8NVXX9G/f39iYmLw9fWlSZMmPPnkkxQXF7vs4+R+Jife/nj77bdp0qQJvr6+dOnShRUrVrhsW1YfIIvFwl133cXs2bNp27Ytvr6+tGnThvnz55eqf/HixXTu3Bk/Pz+aNGnCW2+9VW6/ouTkZPLz8+nbty8AQ4cOZfPmzaxevbpU22nTpmGxWBgyZAgFBQWMHz+eTp06ERoaSmBgIBdddBE//PDDaf98y+oDZBgGTz31FA0aNCAgIIBLLrmE33//vdS2hw8fZuzYsSQkJBAUFERISAh9+/Zl3bp1LuffpUsXAEaOHOm8rfnhhx8CZfcBys3N5b777qNhw4b4+vrSokULXnzxRQzDcGl3Nt/Dudq/fz8333wzkZGR+Pn50b59ez766KNS7T799FM6depEcHAwISEhJCQk8J///Mf5eWFhIY8//jjNmjXDz8+PunXrcuGFF7Jo0aJTHv+tt95iz549vPzyy6XCD0BkZCSPPvqo8/3JfbxKnNx/q+R7X7JkCXfeeScRERE0aNCAWbNmOdeXVYvFYmHjxo3OdZs3b+baa6+lTp06+Pn50blzZ77++muX7c713MVz6AqQSDkOHTpE3759ueGGG7jxxhuJjIwEHP8TDwoKYsyYMQQFBfH9998zfvx4srKyeOGFF06732nTppGdnc2//vUvLBYLzz//PFdffTXbt28/7ZWJn3/+mS+++II777yT4OBgXn31Va655hp27dpF3bp1AVizZg19+vQhOjqaxx9/nOLiYp544gnq169f5j7nzp1Lp06dnOc3dOhQHn/8caZNm8b555/vbFdcXMzMmTO56KKLaNSoEQcPHuTdd99lyJAh3HrrrWRnZ/Pee+/Ru3dvli9fXuq20+mMHz+ep556in79+tGvXz9Wr17N5ZdfTkFBgUu77du3M3v2bK677jri4+NJT0/nrbfeomfPnvzxxx/ExMTQqlUrnnjiCcaPH89tt93GRRddBED37t3LPLZhGFx55ZX88MMP3HzzzXTo0IEFCxZw//33s2fPHl555ZWz/h7O1dGjR+nVqxfbtm3jrrvuIj4+ns8++4wRI0aQkZHBvffeC8CiRYsYMmQIl156Kc899xwAmzZt4pdffnG2mThxIpMmTeKWW26ha9euZGVlsXLlSlavXs1ll11Wbg1ff/01/v7+XHvttRU6l/Lceeed1K9fn/Hjx5Obm0v//v0JCgpi5syZ9OzZ06XtjBkzaNOmDW3btgXg999/p0ePHpx33nk89NBDBAYGMnPmTAYOHMjnn3/OoEGDKnTu4kEMEQ83atQo4+S/Cj179jQAY+rUqaXa5+XllVr3r3/9ywgICDCOHTvmXDd8+HAjNjbW+T4lJcUAjLp16xqHDx92rv/qq68MwPjmm2+c6yZMmFCqJsDw8fExtm3b5ly3bt06AzBee+0157oBAwYYAQEBxp49e5zrtm7danh5eZXap2EYRqNGjYwJEya4rOvSpYvRoEEDo7i42Llu/vz5BmC89dZbhmEYRlFRkZGfn++y3ZEjR4zIyEjjpptuKlX7icf44IMPDMBISUkxDMMw9u/fb/j4+Bj9+/c37Ha7s93DDz9sAMbw4cOd644dO+ZSl2E4/mx9fX2NJ554wrluxYoVBmB88MEHpc755O9m9uzZBmA89dRTLu2uvfZaw2KxuPyZn+n3UJaS/wZeeOGFcttMnjzZAIxPPvnEua6goMDo1q2bERQUZGRlZRmGYRj33nuvERISYhQVFZW7r/bt2xv9+/c/ZU1lCQ8PN9q3b3/G7U/+fkvExsa6fHcl3/uFF15Yqu4hQ4YYERERLuv37dtnWK1Wl+/10ksvNRISElz+rtntdqN79+5Gs2bNnOvO9dzFc+gWmEg5fH19GTlyZKn1/v7+ztfZ2dkcPHiQiy66iLy8PDZv3nza/Q4ePJjw8HDn+5KrE9u3bz/ttklJSTRp0sT5vl27doSEhDi3LS4u5rvvvmPgwIHExMQ42zVt2tR5i+tEGzduZNeuXfTv399l/Y033sju3bv58ccfneumTZuGj48P1113HeDox+Pj4wOA3W7n8OHDFBUV0blz5zJvn53Kd999R0FBAXfffbfLbbrRo0eXauvr64vVanWe76FDhwgKCqJFixZnfdwSc+fOxWazcc8997isv++++zAMg3nz5rmsP933UBFz584lKiqKIUOGONd5e3tzzz33kJOT47xNFBYWRm5u7ilv6YSFhfH777+zdevWs6ohKyuL4ODgczuBM3DrrbeW6gM2ePBg9u/f7/L05axZs7Db7QwePBhw3P78/vvvuf76651/9w4ePMihQ4fo3bs3W7duZc+ePcC5n7t4DgUgkXKcd955zl/wJ/r9998ZNGgQoaGhhISEUL9+fWcH6szMzNPut1GjRi7vS8LQkSNHznrbku1Ltt2/fz9Hjx6ladOmpdqVtW7OnDlERkbSuXNnl/U33HADNpuNadOmAXDs2DG+/PJL+vbt6xLePvroI9q1a+fsY1G/fn3mzJlzRn8OJ9q5cycAzZo1c1lfv359l+OBI2y98sorNGvWDF9fX+rVq0f9+vVZv379WR/3xOPHxMSU+qVf8iRgSX0lTvc9VMTOnTtp1qyZM+SVV8udd95J8+bN6du3Lw0aNOCmm24q1Q/piSeeICMjg+bNm5OQkMD9999/RsMXhISEkJ2dXeFzKU98fHypdX369CE0NJQZM2Y4182YMYMOHTrQvHlzALZt24ZhGDz22GPUr1/fZZkwYQLg+DsA537u4jkUgETKceKVnhIZGRn07NmTdevW8cQTT/DNN9+waNEiZx+MM3m0urynn4yTOttW9rZlmTt3Ln369CnVOToiIoLLLruMzz//nMLCQr755huys7MZOnSos80nn3zCiBEjaNKkCe+99x7z589n0aJF/OMf/6jSR8yfeeYZxowZw8UXX8wnn3zCggULWLRoEW3atHHbo+2V/T2ci4iICNauXcvXX3/t7L/Ut29fhg8f7mxz8cUX89dff/H+++/Ttm1b3n33Xc4//3zefffdU+67ZcuW/Pnnn6X6X52tkx8MKFHW3y1fX18GDhzIl19+SVFREXv27OGXX35xXv2Bv/9+jR07lkWLFpW5lAT9cz138RzqBC1yFhYvXsyhQ4f44osvuPjii53rU1JSTKzqbxEREfj5+ZU5kOHJ6zIyMvj111+56667ytzX0KFDmT9/PvPmzWPatGmEhIQwYMAA5+ezZs2icePGfPHFFy4BquRf4mcjNjYWgK1bt9K4cWPn+gMHDpS6qjJr1iwuueQS3nvvvVLnU69ePef7sxlJOzY2lu+++47s7GyXq0AltzRL6nOH2NhY1q9fj91ud7kKVFYtPj4+DBgwgAEDBmC327nzzjt56623eOyxx5xBoE6dOowcOZKRI0eSk5PDxRdfzMSJE7nlllvKrWHAgAEsXbqUzz//3OVWXHnCw8NLDYxYUFDAvn37zubUGTx4MB999BHJycls2rQJwzBcAlDJfxve3t4kJSWddn/ncu7iOXQFSOQslPzL/8R/6RcUFPDGG2+YVZILm81GUlISs2fPZu/evc7127ZtK9WPpWQQu8svv7zMfQ0cOJCAgADeeOMN5s2bx9VXX42fn5/LscD1z2LZsmUsXbr0rOtOSkrC29ub1157zWV/kydPLvMcT77S8tlnnzn7fpQoGVvmTEYs7tevH8XFxbz++usu61955RUsFkuZ/aeqSr9+/UhLS3O5FVRUVMRrr71GUFCQ8ympQ4cOuWxntVqdg1Pm5+eX2SYoKIimTZs6Py/P7bffTnR0NPfddx9//vlnqc/379/PU0895XzfpEkTl/5iAG+//Xa5V4DKk5SURJ06dZgxYwYzZsyga9euLrfLIiIi6NWrF2+99VaZ4erAgQPO1+d67uI5dAVI5Cx0796d8PBwhg8fzj333IPFYuG///2vW299nM7EiRNZuHAhPXr04I477nD+Ym/btq3LtBBz5szhwgsvJDQ0tMz9BAUFMXDgQGc/oBNvfwFcccUVfPHFFwwaNIj+/fuTkpLC1KlTad26NTk5OWdVc/369Rk7diyTJk3iiiuuoF+/fqxZs4Z58+a5XNUpOe4TTzzByJEj6d69Oxs2bOB///ufy5UjcPxSDgsLY+rUqQQHBxMYGEhiYmKZ/U8GDBjAJZdcwiOPPMKOHTto3749Cxcu5KuvvmL06NEuHZ4rQ3JyMseOHSu1fuDAgdx222289dZbjBgxglWrVhEXF8esWbP45ZdfmDx5svMK1S233MLhw4f5xz/+QYMGDdi5cyevvfYaHTp0cPYXat26Nb169aJTp07UqVOHlStXMmvWrHKv+pUIDw/nyy+/pF+/fnTo0MFlJOjVq1czffp0unXr5mx/yy23cPvtt3PNNddw2WWXsW7dOhYsWFDquzsdb29vrr76aj799FNyc3PLnDJkypQpXHjhhSQkJHDrrbfSuHFj0tPTWbp0Kbt373aOB3Wu5y4exKSnz0SqjfIeg2/Tpk2Z7X/55RfjggsuMPz9/Y2YmBjjgQceMBYsWGAAxg8//OBsV95j8GU9As1JjxGX9xj8qFGjSm178qPGhmEYycnJRseOHQ0fHx+jSZMmxrvvvmvcd999hp+fn2EYjseGIyIijOeff77McywxZ84cAzCio6NLPXput9uNZ555xoiNjTV8fX2Njh07Gt9++22p8y7r/E5+DN4wDKO4uNh4/PHHjejoaMPf39/o1auXsXHjxlLnd+zYMeO+++5ztuvRo4exdOlSo2fPnkbPnj1djvvVV18ZrVu3dg4BUPJIfFk1ZmdnG//+97+NmJgYw9vb22jWrJnxwgsvuDyWX3IuZ/o9nKzkv4Hylv/+97+GYRhGenq6MXLkSKNevXqGj4+PkZCQUOpx/lmzZhmXX365ERERYfj4+BiNGjUy/vWvfxn79u1ztnnqqaeMrl27GmFhYYa/v7/RsmVL4+mnnzYKCgpOWWeJvXv3Gv/+97+N5s2bG35+fkZAQIDRqVMn4+mnnzYyMzOd7YqLi40HH3zQqFevnhEQEGD07t3b2LZtW7mPwa9YsaLcYy5atMgADIvFYqSmppbZ5q+//jKGDRtmREVFGd7e3sZ5551nXHHFFcasWbMq7dyl9rMYRjX6p6uIVJmBAwc6Hwtevnw5iYmJ/P7777Ru3drs0kRE3E59gERqoaNHj7q837p1K3PnzqVXr17Odc8884zCj4h4LF0BEqmFoqOjGTFiBI0bN2bnzp28+eab5Ofns2bNmlJj7YiIeCJ1ghaphfr06cP06dNJS0vD19eXbt268cwzzyj8iIgcpytAIiIi4nHUB0hEREQ8jgKQiIiIeBz1ASqD3W5n7969BAcHn9Vw+iIiImIewzDIzs4mJiam1ITCJ1MAKsPevXtp2LCh2WWIiIjIOUhNTaVBgwanbKMAVIaSoeZTU1MJCQkxuRoRERE5E1lZWTRs2NBlUuPyKACVoeS2V0hIiAKQiIhIDXMm3VfUCVpEREQ8jgKQiIiIeBwFIBEREfE46gMkIiK1XnFxMYWFhWaXIRXk7e2NzWarlH0pAImISK1lGAZpaWlkZGSYXYpUkrCwMKKioio8Tp8CkIiI1Fol4SciIoKAgAANbluDGYZBXl4e+/fvByA6OrpC+1MAEhGRWqm4uNgZfurWrWt2OVIJ/P39Adi/fz8REREVuh2mTtAiIlIrlfT5CQgIMLkSqUwl32dF+3QpAImISK2m2161S2V9nwpAIiIi4nEUgERERGq5uLg4Jk+ebHYZ1YoCkIiISDVhsVhOuUycOPGc9rtixQpuu+22CtXWq1cvRo8eXaF9VCemBqBJkybRpUsXgoODiYiIYODAgWzZsuW023322We0bNkSPz8/EhISmDt3rsvnhmEwfvx4oqOj8ff3Jykpia1bt1bVaZwxwzDYdSiPPRlHzS5FRESqoX379jmXyZMnExIS4rJu7NixzraGYVBUVHRG+61fv746g5/E1AC0ZMkSRo0axW+//caiRYsoLCzk8ssvJzc3t9xtfv31V4YMGcLNN9/MmjVrGDhwIAMHDmTjxo3ONs8//zyvvvoqU6dOZdmyZQQGBtK7d2+OHTvmjtMq11NzNnHxCz/w0a87TK1DRESqp6ioKOcSGhqKxWJxvt+8eTPBwcHMmzePTp064evry88//8xff/3FVVddRWRkJEFBQXTp0oXvvvvOZb8n3wKzWCy8++67DBo0iICAAJo1a8bXX39dodo///xz2rRpg6+vL3Fxcbz00ksun7/xxhs0a9YMPz8/IiMjufbaa52fzZo1i4SEBPz9/albty5JSUmnzAKVwqhG9u/fbwDGkiVLym1z/fXXG/3793dZl5iYaPzrX/8yDMMw7Ha7ERUVZbzwwgvOzzMyMgxfX19j+vTpZ1RHZmamARiZmZnncBbl+3xVqhH74LfGla//XKn7FRGR0o4ePWr88ccfxtGjR53r7Ha7kZtf6PbFbrefdf0ffPCBERoa6nz/ww8/GIDRrl07Y+HChca2bduMQ4cOGWvXrjWmTp1qbNiwwfjzzz+NRx991PDz8zN27tzp3DY2NtZ45ZVXnO8Bo0GDBsa0adOMrVu3Gvfcc48RFBRkHDp0qNx6evbsadx7771lfrZy5UrDarUaTzzxhLFlyxbjgw8+MPz9/Y0PPvjAMAzDWLFihWGz2Yxp06YZO3bsMFavXm385z//MQzDMPbu3Wt4eXkZL7/8spGSkmKsX7/emDJlipGdnV3mscr6Xkucze/vajUQYmZmJgB16tQpt83SpUsZM2aMy7revXsze/ZsAFJSUkhLSyMpKcn5eWhoKImJiSxdupQbbrih1D7z8/PJz893vs/KyqrIaZQrsbFjIK6NezLJyS8iyLda/fGLiNR6RwuLaT1+gduP+8cTvQnwqZz/5z/xxBNcdtllzvd16tShffv2zvdPPvkkX375JV9//TV33XVXufsZMWIEQ4YMAeCZZ57h1VdfZfny5fTp0+esa3r55Ze59NJLeeyxxwBo3rw5f/zxBy+88AIjRoxg165dBAYGcsUVVxAcHExsbCwdO3YEHLf9ioqKuPrqq4mNjQUgISHhrGs4W9WmE7Tdbmf06NH06NGDtm3bltsuLS2NyMhIl3WRkZGkpaU5Py9ZV16bk02aNInQ0FDn0rBhw4qcSrnOC/PnvDB/iu0Gq3ceqZJjiIhI7da5c2eX9zk5OYwdO5ZWrVoRFhZGUFAQmzZtYteuXafcT7t27ZyvAwMDCQkJcU4zcbY2bdpEjx49XNb16NGDrVu3UlxczGWXXUZsbCyNGzfmn//8J//73//Iy8sDoH379lx66aUkJCRw3XXX8c4773DkSNX/jqw2lyBGjRrFxo0b+fnnn91+7HHjxrlcVcrKyqqyEJTYuA5frN7DspRDXNy8fpUcQ0REyubvbeOPJ3qbctzKEhgY6PJ+7NixLFq0iBdffJGmTZvi7+/PtddeS0FBwSn34+3t7fLeYrFgt9srrc4TBQcHs3r1ahYvXszChQsZP348EydOZMWKFYSFhbFo0SJ+/fVXFi5cyGuvvcYjjzzCsmXLiI+Pr5J6oJpcAbrrrrv49ttv+eGHH2jQoMEp20ZFRZGenu6yLj09naioKOfnJevKa3MyX19fQkJCXJaqkhjvuL23POVwlR1DRETKZrFYCPDxcvtSlaNR//LLL4wYMYJBgwaRkJBAVFQUO3bsqLLjlaVVq1b88ssvpepq3ry5c74uLy8vkpKSeP7551m/fj07duzg+++/BxzfS48ePXj88cdZs2YNPj4+fPnll1Vas6lXgAzD4O677+bLL79k8eLFZ5T0unXrRnJysstYBIsWLaJbt24AxMfHExUVRXJyMh06dAAcV3SWLVvGHXfcURWncVYS4x39gNalZnKssBi/SvxXgYiIeJ5mzZrxxRdfMGDAACwWC4899liVXck5cOAAa9eudVkXHR3NfffdR5cuXXjyyScZPHgwS5cu5fXXX+eNN94A4Ntvv2X79u1cfPHFhIeHM3fuXOx2Oy1atGDZsmUkJydz+eWXExERwbJlyzhw4ACtWrWqknMoYWoAGjVqFNOmTeOrr74iODjY2UcnNDTUOePrsGHDOO+885g0aRIA9957Lz179uSll16if//+fPrpp6xcuZK3334bcKTI0aNH89RTT9GsWTPi4+N57LHHiImJYeDAgaac54li6wYQEezL/ux81uzKoFsTzVAsIiLn7uWXX+amm26ie/fu1KtXjwcffLDKHuaZNm0a06ZNc1n35JNP8uijjzJz5kzGjx/Pk08+SXR0NE888QQjRowAICwsjC+++IKJEydy7NgxmjVrxvTp02nTpg2bNm3ixx9/ZPLkyWRlZREbG8tLL71E3759q+QcSliOPw5nivIuCX7wwQfOP7RevXoRFxfHhx9+6Pz8s88+49FHH2XHjh00a9aM559/nn79+jk/NwyDCRMm8Pbbb5ORkcGFF17IG2+8QfPmzc+orqysLEJDQ8nMzKyS22F3T1/DN+v2MjqpGaOTzqwmERE5O8eOHSMlJYX4+Hj8/PzMLkcqyam+17P5/W1qAKquqjoA/fe3nTw2eyPdm9Rl2q0XVPr+RUREAai2qqwAVC06QXuako7Qq3cdoaCoau7TioiISPkUgEzQLCKIOoE+HCu0s2FPhtnliIiIeBwFIBNYLBa6xIUD8Nt2PQ4vIiLibgpAJil5HF7jAYmIiLifApBJuh7vB7Rq5xGKitUPSERExJ0UgEzSKjqEYD8vcvKL+GNf1YzXICIiImVTADKJzWqhS5ymxRARETGDApCJSh6HV0doERER91IAMlFJP6AVOw5jt2s8ShERqRy9evVymTNTSlMAMlHb80IJ8LGRebSQLenZZpcjIiImGzBgAH369Cnzs59++gmLxcL69esrfJwPP/yQsLCwCu+nJlMAMpG3zUqnWMd4QOoHJCIiN998M4sWLWL37t2lPvvggw/o3Lkz7dq1M6Gy2kcByGQl/YCWpRwyuRIRETHbFVdcQf369V0mAAfIycnhs88+4+abb+bQoUMMGTKE8847j4CAABISEpg+fXql1rFr1y6uuuoqgoKCCAkJ4frrryc9Pd35+bp167jkkksIDg4mJCSETp06sXLlSgB27tzJgAEDCA8PJzAwkDZt2jB37txKra8yeJldgKfresKAiIZhYLFYTK5IRKQWMwwozHP/cb0D4Az+/+7l5cWwYcP48MMPeeSRR5y/Ez777DOKi4sZMmQIOTk5dOrUiQcffJCQkBDmzJnDP//5T5o0aULXrl0rXKrdbneGnyVLllBUVMSoUaMYPHgwixcvBmDo0KF07NiRN998E5vNxtq1a/H29gZg1KhRFBQU8OOPPxIYGMgff/xBUFBQheuqbApAJmvfMBQfLysHcwr460AuTSOq338kIiK1RmEePBPj/uM+vBd8As+o6U033cQLL7zAkiVL6NWrF+C4/XXNNdcQGhpKaGgoY8eOdba/++67WbBgATNnzqyUAJScnMyGDRtISUmhYcOGAHz88ce0adOGFStW0KVLF3bt2sX9999Py5YtAWjWrJlz+127dnHNNdeQkJAAQOPGjStcU1XQLTCT+XrZ6NgwDFA/IBERgZYtW9K9e3fef/99ALZt28ZPP/3EzTffDEBxcTFPPvkkCQkJ1KlTh6CgIBYsWMCuXbsq5fibNm2iYcOGzvAD0Lp1a8LCwti0aRMAY8aM4ZZbbiEpKYlnn32Wv/76y9n2nnvu4amnnqJHjx5MmDChUjptVwVdAaoGEhvXZVnKYZalHOL/EhuZXY6ISO3lHeC4GmPGcc/CzTffzN13382UKVP44IMPaNKkCT179gTghRde4D//+Q+TJ08mISGBwMBARo8eTUFBQVVUXqaJEyfyf//3f8yZM4d58+YxYcIEPv30UwYNGsQtt9xC7969mTNnDgsXLmTSpEm89NJL3H333W6r70zoClA14OwIvd3RD0hERKqIxeK4FeXu5Sz7d15//fVYrVamTZvGxx9/zE033eTsD/TLL79w1VVXceONN9K+fXsaN27Mn3/+WWl/RK1atSI1NZXU1FTnuj/++IOMjAxat27tXNe8eXP+/e9/s3DhQq6++mo++OAD52cNGzbk9ttv54svvuC+++7jnXfeqbT6KouuAFUD5zcKx8tqIS3rGKmHj9Ko7tn9S0FERGqXoKAgBg8ezLhx48jKymLEiBHOz5o1a8asWbP49ddfCQ8P5+WXXyY9Pd0lnJyJ4uJi1q5d67LO19eXpKQkEhISGDp0KJMnT6aoqIg777yTnj170rlzZ44ePcr999/PtddeS3x8PLt372bFihVcc801AIwePZq+ffvSvHlzjhw5wg8//ECrVq0q+kdS6RSAqgF/HxvtGoSyelcGy1IOKQCJiAg333wz7733Hv369SMm5u+O248++ijbt2+nd+/eBAQEcNtttzFw4EAyMzPPav85OTl07NjRZV2TJk3Ytm0bX331FXfffTcXX3wxVquVPn368NprrwFgs9k4dOgQw4YNIz09nXr16nH11Vfz+OOPA45gNWrUKHbv3k1ISAh9+vThlVdeqeCfRuWzGLrnUkpWVhahoaFkZmYSEhLilmM+O28zU5f8xXWdGvDCde3dckwRkdrs2LFjpKSkEB8fj5+fn9nlSCU51fd6Nr+/1QeomkhsXDIgop4EExERqWoKQNVE59hwrBbYdTiPfZlHzS5HRESkVlMAqiaC/bxpExMKaDwgERGRqqYAVI10jddtMBEREXdQAKpG/h4PSBOjiohUFj3rU7tU1vepAFSNdIlzBKC/DuRyMCff5GpERGq2ksk58/JMmPxUqkzJ91ny/Z4rjQNUjYQH+tAyKpjNadksTzlMv4Ros0sSEamxbDYbYWFh7N+/H4CAgADnaMpS8xiGQV5eHvv37ycsLAybzVah/SkAVTNd4+soAImIVJKoqCgAZwiSmi8sLMz5vVaEAlA1kxhfl4+X7uQ39QMSEakwi8VCdHQ0ERERFBYWml2OVJC3t3eFr/yUUACqZrrEhwOwJT2bjLwCwgJ8TK5IRKTms9lslfaLU2oHUztB//jjjwwYMICYmBgsFguzZ88+ZfsRI0ZgsVhKLW3atHG2mThxYqnPW7ZsWcVnUnkigv1oXD8Qw4AVO46YXY6IiEitZGoAys3NpX379kyZMuWM2v/nP/9h3759ziU1NZU6depw3XXXubRr06aNS7uff/65KsqvMiWPwy9P0W0wERGRqmDqLbC+ffvSt2/fM24fGhpKaGio8/3s2bM5cuQII0eOdGnn5eVVKR2kzJIYX5fpy1M1IKKIiEgVqdHjAL333nskJSURGxvrsn7r1q3ExMTQuHFjhg4dyq5du0yq8NyUjAi9cU8mOflFJlcjIiJS+9TYALR3717mzZvHLbfc4rI+MTGRDz/8kPnz5/Pmm2+SkpLCRRddRHZ2drn7ys/PJysry2UxU0yYPw3r+GM3YNVO9QMSERGpbDU2AH300UeEhYUxcOBAl/V9+/bluuuuo127dvTu3Zu5c+eSkZHBzJkzy93XpEmTnLfXQkNDadiwYRVXf3pd4+oCmhZDRESkKtTIAGQYBu+//z7//Oc/8fE59WPiYWFhNG/enG3btpXbZty4cWRmZjqX1NTUyi75rCU2LukIrX5AIiIila1GBqAlS5awbds2br755tO2zcnJ4a+//iI6uvxRlX19fQkJCXFZzFbyJNi63RkcLSg2uRoREZHaxdQAlJOTw9q1a1m7di0AKSkprF271tlpedy4cQwbNqzUdu+99x6JiYm0bdu21Gdjx45lyZIl7Nixg19//ZVBgwZhs9kYMmRIlZ5LZWtUJ4DIEF8Kiw3WpKofkIiISGUyNQCtXLmSjh070rFjRwDGjBlDx44dGT9+PAD79u0r9QRXZmYmn3/+eblXf3bv3s2QIUNo0aIF119/PXXr1uW3336jfv36VXsylcxisZAYX9IPSLfBREREKpPFMAzD7CKqm6ysLEJDQ8nMzDT1dtgnv+3k0dkb6da4LtNvu8C0OkRERGqCs/n9XSP7AHmKC453hF696wj5ReoHJCIiUlkUgKqxJvWDqBvoQ36RnQ27M80uR0REpNZQAKrGLBaLc1RoTYshIiJSeRSAqjkFIBERkcqnAFTNlTwJtmrHYYqK7SZXIyIiUjsoAFVzLaKCCfHzIregmN/3mjtHmYiISG2hAFTN2awn9gPSvGAiIiKVQQGoBigJQJoXTEREpHIoANUAJf2Alqccxm7XuJUiIiIVpQBUA7SJCSHQx0bWsSI2p2WbXY6IiEiNpwBUA3jZrHSKK7kNpn5AIiIiFaUAVEMkajwgERGRSqMAVEMkntARWvPXioiIVIwCUA2R0CAUXy8rh3IL+OtAjtnliIiI1GgKQDWEr5eN8xuFA7oNJiIiUlEKQDWIc0DE7QpAIiIiFaEAVIOoH5CIiEjlUACqQTo2CsfbZiEt6xi7DueZXY6IiEiNpQBUg/j72GjXIAxQPyAREZGKUACqYRLVD0hERKTCFIBqGOfEqDs0IrSIiMi5UgCqYTrH1cFqgdTDR9mbcdTsckRERGokBaAaJsjXi7bnhQKOp8FERETk7CkA1UCaF0xERKRiFIBqoK7xdQFYppnhRUREzokCUA3UNa4OFgtsP5DLgex8s8sRERGpcRSAaqDQAG9aRAYD6gckIiJyLhSAaqgLGjtugy3XbTAREZGzpgBUQ3VVR2gREZFzpgBUQ5UEoM1p2WTkFZhcjYiISM2iAFRD1QvypUn9QED9gERERM6WqQHoxx9/ZMCAAcTExGCxWJg9e/Yp2y9evBiLxVJqSUtLc2k3ZcoU4uLi8PPzIzExkeXLl1fhWZgn0dkPSAFIRETkbJgagHJzc2nfvj1Tpkw5q+22bNnCvn37nEtERITzsxkzZjBmzBgmTJjA6tWrad++Pb1792b//v2VXb7pNCCiiIjIufEy8+B9+/alb9++Z71dREQEYWFhZX728ssvc+uttzJy5EgApk6dypw5c3j//fd56KGHKlJutVPSD+j3vZlkHysk2M/b5IpERERqhhrZB6hDhw5ER0dz2WWX8csvvzjXFxQUsGrVKpKSkpzrrFYrSUlJLF261IxSq1R0qD+N6gRgN2DlziNmlyMiIlJj1KgAFB0dzdSpU/n888/5/PPPadiwIb169WL16tUAHDx4kOLiYiIjI122i4yMLNVP6ET5+flkZWW5LDVFyVUg9QMSERE5c6beAjtbLVq0oEWLFs733bt356+//uKVV17hv//97znvd9KkSTz++OOVUaLbJcbXYdaq3SzbrgERRUREzlSNugJUlq5du7Jt2zYA6tWrh81mIz093aVNeno6UVFR5e5j3LhxZGZmOpfU1NQqrbkyJR6fGHX97kyOFhSbXI2IiEjNUOMD0Nq1a4mOjgbAx8eHTp06kZyc7PzcbreTnJxMt27dyt2Hr68vISEhLktN0bCOP9GhfhTZDdbsUj8gERGRM2HqLbCcnBzn1RuAlJQU1q5dS506dWjUqBHjxo1jz549fPzxxwBMnjyZ+Ph42rRpw7Fjx3j33Xf5/vvvWbhwoXMfY8aMYfjw4XTu3JmuXbsyefJkcnNznU+F1TYWi4Wu8XX4au1efks5TPem9cwuSUREpNozNQCtXLmSSy65xPl+zJgxAAwfPpwPP/yQffv2sWvXLufnBQUF3HfffezZs4eAgADatWvHd99957KPwYMHc+DAAcaPH09aWhodOnRg/vz5pTpG1yaJ8XX5au1eTYwqIiJyhiyGYRhmF1HdZGVlERoaSmZmZo24HbZtfw5JLy/B18vK+omX4+tlM7skERERtzub3981vg+QQJP6gdQL8iG/yM763ZlmlyMiIlLtKQDVAiX9gAA9Di8iInIGFIBqiZLH4TUvmIiIyOkpANUSJVeAVu08QmGx3eRqREREqjcFoFqiRWQwof7e5BUU8/vemjOVh4iIiBkUgGoJq9VClzj1AxIRETkTCkC1yAWNNTGqiIjImVAAqkWcM8PvOEyxXcM7iYiIlEcBqBZpHR1CkK8X2ceK2JymfkAiIiLlUQCqRbxsVjrFhgOwbLtug4mIiJRHAaiWcd4GUz8gERGRcikA1TLOjtA7DqNp3kRERMqmAFTLJJwXhp+3lcO5BWzbn2N2OSIiItWSAlAt4+Nl5fxGx/sB6TaYiIhImRSAaiHnxKgKQCIiImVSAKqFSiZGXZ5ySP2AREREyqAAVAt1bBSGj81KelY+Ow/lmV2OiIhItaMAVAv5edto3zAU0OPwIiIiZVEAqqVK+gH9lqKJUUVERE6mAFRL/d0PSFeARERETqYAVEudHxuOzWph95Gj7Mk4anY5IiIi1YoCUC0V5OtF2/NK+gHpNpiIiMiJFIBqscSS8YA0MaqIiIgLBaBaLFETo4qIiJRJAagW6xxXB4sFth/MZX/WMbPLERERqTYUgGqxUH9vWkWFAI7Z4UVERMRBAaiW66rbYCIiIqUoANVyFzRWR2gREZGTKQDVcl3iHAFoS3o2R3ILTK5GRESkelAAquXqBvnSNCIIUD8gERGREgpAHkCPw4uIiLgyNQD9+OOPDBgwgJiYGCwWC7Nnzz5l+y+++ILLLruM+vXrExISQrdu3ViwYIFLm4kTJ2KxWFyWli1bVuFZVH8lHaGXaURoERERwOQAlJubS/v27ZkyZcoZtf/xxx+57LLLmDt3LqtWreKSSy5hwIABrFmzxqVdmzZt2Ldvn3P5+eefq6L8GqNkYtQ/9maRdazQ5GpERETM52Xmwfv27Uvfvn3PuP3kyZNd3j/zzDN89dVXfPPNN3Ts2NG53svLi6ioqMoqs8aLCvUjtm4AOw/lsWrHES5pGWF2SSIiIqaq0X2A7HY72dnZ1KlTx2X91q1biYmJoXHjxgwdOpRdu3adcj/5+flkZWW5LLWNc14w9QMSERGp2QHoxRdfJCcnh+uvv965LjExkQ8//JD58+fz5ptvkpKSwkUXXUR2dna5+5k0aRKhoaHOpWHDhu4o3626Hr8Npn5AIiIiNTgATZs2jccff5yZM2cSEfH3LZ2+ffty3XXX0a5dO3r37s3cuXPJyMhg5syZ5e5r3LhxZGZmOpfU1FR3nIJblVwB2rA7k7yCIpOrERERMVeNDECffvopt9xyCzNnziQpKemUbcPCwmjevDnbtm0rt42vry8hISEuS23TINyfmFA/iuwGq3dmmF2OiIiIqWpcAJo+fTojR45k+vTp9O/f/7Ttc3Jy+Ouvv4iOjnZDddWXxWIhsbHjNthy3QYTEREPZ2oAysnJYe3ataxduxaAlJQU1q5d6+y0PG7cOIYNG+ZsP23aNIYNG8ZLL71EYmIiaWlppKWlkZmZ6WwzduxYlixZwo4dO/j1118ZNGgQNpuNIUOGuPXcqqOu6ggtIiICmByAVq5cSceOHZ2PsI8ZM4aOHTsyfvx4APbt2+fyBNfbb79NUVERo0aNIjo62rnce++9zja7d+9myJAhtGjRguuvv566devy22+/Ub9+ffeeXDVU0g9oTWoGxwqLTa5GRETEPBbDMAyzi6husrKyCA0NJTMzs1b1BzIMgy5PJ3MwJ5+Z/+rmvCIkIiJSG5zN7+8a1wdIzp2jH9Dx22Db1Q9IREQ8lwKQh3FOjKqZ4UVExIMpAHmYknnBVu08QmGx3eRqREREzKEA5GGaRQQRFuBNXkExG/dknn4DERGRWkgByMNYrRa6xulxeBER8WwKQB6o5Omv5QpAIiLioRSAPFBJP6AVKYcptmsUBBER8TwKQB6odUwIQb5eZOcXsWlfltnliIiIuJ0CkAeyWS10jgsH1A9IREQ8kwKQhyq5DaaJUUVExBMpALlbRirkHDC7CpeO0Hb1AxIREQ+jAORO302EyQmw/G2zKyHhvFD8vW0cyStk24Ecs8sRERFxKwUgd4psCxiw/lOwmzsKs4+XlfNjwwD1AxIREc+jAOROLfqBTzBk7IJdv5pdjbMfkCZGFRERT6MA5E4+AdBmoOP1uummlgKu/YAMQ/2ARETEcygAuVv7IY6fv38FBXmmltKhYRg+Niv7s/PZccjcWkRERNxJAcjdGnWDsEZQkA2b55haip+3jQ4NwwA9Di8iIp5FAcjdrNa/rwJVg9tgiY2PT4y6XR2hRUTEcygAmaHdYMfP7T9A1j5TSynpB6QnwURExJMoAJmhbhNoeAEYdtgw09RSOsWG42W1sCfjKLuPqB+QiIh4BgUgs7S/wfFz7XQw8QmsAB8v2p4XCjieBhMREfEECkBmaTMIbL5wYBOkrTe1FPUDEhERT3NOASg1NZXdu3c73y9fvpzRo0fz9tvmT/FQY/iHQct+jtdrze0MnVgyHtAOBSAREfEM5xSA/u///o8ffvgBgLS0NC677DKWL1/OI488whNPPFGpBdZqJU+DbfgMigtNK6NzXB0sFkg5mMv+rGOm1SEiIuIu5xSANm7cSNeuXQGYOXMmbdu25ddff+V///sfH374YWXWV7s1+QcE1oe8g7DtO9PKCPHzpnV0CKCnwURExDOcUwAqLCzE19cXgO+++44rr7wSgJYtW7Jvn7mPddcoNm9IuN7x2uQxgf5+HF4DIoqISO13TgGoTZs2TJ06lZ9++olFixbRp08fAPbu3UvdunUrtcBar+RpsC3z4OgR08oomRhVT4KJiIgnOKcA9Nxzz/HWW2/Rq1cvhgwZQvv27QH4+uuvnbfG5AxFJUBEGygugI1fmFZGyRWgP9NzOJxbYFodIiIi7nBOAahXr14cPHiQgwcP8v777zvX33bbbUydOrXSivMIFgt0KJka41PTyqgT6EPzyCBAV4FERKT2O6cAdPToUfLz8wkPDwdg586dTJ48mS1bthAREVGpBXqEhOvAYoXdy+HQX6aVUXIVSAFIRERqu3MKQFdddRUff/wxABkZGSQmJvLSSy8xcOBA3nzzzUot0CMER0GTSx2vTewMXdIPSB2hRUSktjunALR69WouuugiAGbNmkVkZCQ7d+7k448/5tVXXz3j/fz4448MGDCAmJgYLBYLs2fPPu02ixcv5vzzz8fX15emTZuW+dj9lClTiIuLw8/Pj8TERJYvX37GNZmmpDP0uhlgt5tSQsmAiH/syyLrmHnjEomIiFS1cwpAeXl5BAcHA7Bw4UKuvvpqrFYrF1xwATt37jzj/eTm5tK+fXumTJlyRu1TUlLo378/l1xyCWvXrmX06NHccsstLFiwwNlmxowZjBkzhgkTJrB69Wrat29P79692b9//9mdpLu17A++IZC5C3b9akoJESF+xNcLxDBgpUaFFhGRWuycAlDTpk2ZPXs2qampLFiwgMsvvxyA/fv3ExIScsb76du3L0899RSDBg06o/ZTp04lPj6el156iVatWnHXXXdx7bXX8sorrzjbvPzyy9x6662MHDmS1q1bM3XqVAICAlw6a1dL3v7QZqDjtYlTY3SNKxkPSAFIRERqr3MKQOPHj2fs2LHExcXRtWtXunXrBjiuBnXs2LFSCzzR0qVLSUpKclnXu3dvli5dCkBBQQGrVq1yaWO1WklKSnK2KUt+fj5ZWVkuiylKpsb4YzYU5JlSgiZGFRERT3BOAejaa69l165drFy50uX206WXXupyNaaypaWlERkZ6bIuMjKSrKwsjh49ysGDBykuLi6zTVpaWrn7nTRpEqGhoc6lYcOGVVL/aTW8AMJioSAHNn9rSgklT4Jt3JNJbn6RKTWIiIhUtXMKQABRUVF07NiRvXv3OmeG79q1Ky1btqy04txl3LhxZGZmOpfU1FRzCrFa/74KZNLTYA3CAzgvzJ8iu8HqXeaNTC0iIlKVzikA2e12nnjiCUJDQ4mNjSU2NpawsDCefPJJ7FX4BFNUVBTp6eku69LT0wkJCcHf35969ephs9nKbBMVFVXufn19fQkJCXFZTNN+sOPn9sWQtdeUEhI1HpCIiNRy5xSAHnnkEV5//XWeffZZ1qxZw5o1a3jmmWd47bXXeOyxxyq7Rqdu3bqRnJzssm7RokXOPkg+Pj506tTJpY3dbic5OdnZptqr0xgadQPDDutnmlKC+gGJiEht53UuG3300Ue8++67zlngAdq1a8d5553HnXfeydNPP31G+8nJyWHbtm3O9ykpKaxdu5Y6derQqFEjxo0bx549e5yDLt5+++28/vrrPPDAA9x00018//33zJw5kzlz5jj3MWbMGIYPH07nzp3p2rUrkydPJjc3l5EjR57LqZqj/Q2wa6njNliPex3TZbhR1+MDIq5NzeBYYTF+3ja3Hl9ERKSqndMVoMOHD5fZ16dly5YcPnzmVw1WrlxJx44dnU+OjRkzho4dOzJ+/HgA9u3bx65du5zt4+PjmTNnDosWLaJ9+/a89NJLvPvuu/Tu3dvZZvDgwbz44ouMHz+eDh06sHbtWubPn1+qY3S11nog2HzhwGbYt87th4+rG0BEsC8FxXbWpma4/fgiIiJVzWIYhnG2GyUmJpKYmFhq1Oe7776b5cuXs2zZskor0AxZWVmEhoaSmZlpXn+gz0bC719A4u3Q9zm3H/6uaav5dv0+xlzWnHsubeb244uIiJyts/n9fU63wJ5//nn69+/Pd9995+xbs3TpUlJTU5k7d+657FJO1n6IIwBt+Awufwps3m49fGLjuny7fp86QouISK10TrfAevbsyZ9//smgQYPIyMggIyODq6++mt9//53//ve/lV2jZ2ryDwiMgLxDsO07tx++5EmwVTuPUFhsztxkIiIiVeWcboGVZ926dZx//vkUFxdX1i5NUS1ugQEseASWvg6troTB7g2WdrtBp6cWcSSvkC/u7M75jcLdenwREZGzdTa/v895IERxg5IZ4v+cD3nuvRVltVqco0LrcXgREaltFICqs6gEiGwLxQWO/kBuVvI4/PKUQ24/toiISFVSAKrunFNjfOr2Q5f0A1q54wjF9kq7UyoiImK6s3oK7Oqrrz7l5xkZGRWpRcqScB0sGg+7V8DBbVCvqdsO3So6hGA/L7KPFbFpXxZtzwt127FFRESq0lldATpxxvSyltjYWIYNG1ZVtXqm4EhoeqnjtZsnSLVZLXSJc1wF+m27boOJiEjtcVZXgD744IOqqkNOpf0NsHUhrJ8BlzzimDXeTbrG1+H7zftZnnKYWy5q7LbjioiIVCX1AaoJWvQD31DITIWdv7j10M6Z4Xccxq5+QCIiUksoANUE3v7QZqDjtZtvg7U9L5QAHxsZeYVs3Z/j1mOLiIhUFQWgmqLkabA/voKCXLcd1ttmpVOsYxDEZXocXkREagkFoJqi0QUQHgcFObDpW7ceuuvxjtDLNC+YiIjUEgpANYXFcsKYQO69DZbY2DEg4rLth6nEmVNERERMowBUk7Qb7Pi5fTFk7XXfYRuE4uNl5WBOPikH3Xf7TUREpKooANUkdeKhUTfAcDwS7yZ+3jY6NgwDYLlug4mISC2gAFTTnDg1hhtvR5U8Dq9+QCIiUhsoANU0bQaClx8c2Az71rrtsCX9gHQFSEREagMFoJrGLxRa9ne8Xuu+ztAdG4XhZbWwJ+MoqYfz3HZcERGRqqAAVBOV3AbbOAuKCtxyyAAfL9o1cEyGqqtAIiJS0ykA1USNL4GgSMg7BNu+c9thu8YffxxeAyKKiEgNpwBUE9m8IOE6x+t109x2WOe8YLoCJCIiNZwCUE1Vchtsy3zIc08g6RQXjtUCOw7lkZ51zC3HFBERqQoKQDVVVFuITAB7IWz83C2HDPHzpnVMCKDH4UVEpGZTAKrJOpwwJpCbJJb0A9qufkAiIlJzKQDVZAnXgcUGe1bCwa1uOWRX9QMSEZFaQAGoJguKgKZJjtdumiC1ZGb4rftzOJST75ZjioiIVDYFoJqu/Q2On+tmgN1e5YcLD/ShRWQwACt26CqQiIjUTApANV2LfuAbClm7YefPbjlkYmPHVaDftisAiYhIzaQAVNN5+0HbQY7Xbpoao0fTegB8vno3R3LdMxK1iIhIZVIAqg1KxgT64ysoyK3ywyW1iqRlVDDZx4p4/YdtVX48ERGRylYtAtCUKVOIi4vDz8+PxMREli9fXm7bXr16YbFYSi39+/d3thkxYkSpz/v06eOOUzFHw0QIj4fCXNj0bZUfzma18HC/VgB8vHQHuw5pclQREalZTA9AM2bMYMyYMUyYMIHVq1fTvn17evfuzf79+8ts/8UXX7Bv3z7nsnHjRmw2G9ddd51Luz59+ri0mz7dfTOnu53F8vdVIDdNjXFx8/pc1KwehcUGLyzc4pZjioiIVBbTA9DLL7/MrbfeysiRI2ndujVTp04lICCA999/v8z2derUISoqyrksWrSIgICAUgHI19fXpV14eLg7Tsc87a53/Ny+BDL3uOWQ4/q2wmKBb9btZV1qhluOKSIiUhlMDUAFBQWsWrWKpKQk5zqr1UpSUhJLly49o32899573HDDDQQGBrqsX7x4MREREbRo0YI77riDQ4fKH7k4Pz+frKwsl6XGqRMPjboDBqyf4ZZDto4J4eqODQB4Zu4mDMNwy3FFREQqytQAdPDgQYqLi4mMjHRZHxkZSVpa2mm3X758ORs3buSWW25xWd+nTx8+/vhjkpOTee6551iyZAl9+/aluLi4zP1MmjSJ0NBQ59KwYcNzPykznTg1hpvCyH2XN8fXy8qylMMkbyr7tqWIiEh1Y/otsIp47733SEhIoGvXri7rb7jhBq688koSEhIYOHAg3377LStWrGDx4sVl7mfcuHFkZmY6l9TUVDdUXwVaXwVefnBwC+xd45ZDxoT5c9OF8QBMmreJouKqH4xRRESkokwNQPXq1cNms5Genu6yPj09naioqFNum5uby6effsrNN9982uM0btyYevXqsW1b2Y9s+/r6EhIS4rLUSH6h0PIKx2s3TY0BcEevJoQHePPXgVxmrtzttuOKiIicK1MDkI+PD506dSI5Odm5zm63k5ycTLdu3U657WeffUZ+fj433njjaY+ze/duDh06RHR0dIVrrvZKngbbMAuK3DNIYYifN/dc2gyAlxf9SW5+kVuOKyIicq5MvwU2ZswY3nnnHT766CM2bdrEHXfcQW5uLiNHjgRg2LBhjBs3rtR27733HgMHDqRu3bou63Nycrj//vv57bff2LFjB8nJyVx11VU0bdqU3r17u+WcTNW4FwRFwtHDsG2R2w47NDGW2LoBHMzJ5+0ft7vtuCIiIufC9AA0ePBgXnzxRcaPH0+HDh1Yu3Yt8+fPd3aM3rVrF/v27XPZZsuWLfz8889l3v6y2WysX7+eK6+8kubNm3PzzTfTqVMnfvrpJ3x9fd1yTqayef39SPxa94wJBODjZeWB3i0BeOen7ezPOua2Y4uIiJwti6Fnl0vJysoiNDSUzMzMmtkfKP13eLM7WL1h7J8QUMcthzUMg6vf/JU1uzIY0rURk65OcMtxRURE4Ox+f5t+BUiqQGQbiEoAeyFs/Nxth7VY/p4iY8aKXWzbn+22Y4uIiJwNBaDaqv3/OX668WkwgC5xdbi8dSR2A56dt9mtxxYRETlTCkC1VcK1YLHBnlVw4E+3HvrBvi2xWS18t2k/v20vfwRuERERsygA1VZBEdDsMsdrN18FalI/iCFdHaNpPzN3E3a7upmJiEj1ogBUm7W/wfFz/Qywu3eE5nsvbU6gj431uzP5dsO+028gIiLiRgpAtVnzvo7RobP2wI6f3Hro+sG+3N6zCQAvLNhMflHZ87CJiIiYQQGoNvP2gzZXO167+TYYwM0XxRMR7Evq4aP8d+lOtx9fRESkPApAtV3J1Bh/fA35OW49dICPF/dd3hyA177fRmZeoVuPLyIiUh4FoNquYVeo0xgKc2Hzt24//LWdGtI8MojMo4W8sbjsyWhFRETcTQGotrNY/r4K5MapMUrYrBbG9XUMjvjBrzvYfSTP7TWIiIicTAHIE5TMDZbyI2Tudvvhe7WoT/cmdSkosvPSQveOSSQiIlIWBSBPEB4HsT0Aw/FIvJtZLH9fBfpyzR427sl0ew0iIiInUgDyFCW3wdZ9CibMf5vQIJSBHWIAx+CImoNXRETMpADkKVpfBV7+cPBP2LvalBLG9m6Bj83Kr38dYvGfB0ypQUREBBSAPIdfCLS6wvF6rfvHBAJoEB7AiB5xADw7dzPFmiJDRERMogDkSUqmxtg4C4oKTClhVK+mhPp7syU9m1mrUk2pQURERAHIkzS+BIKi4OgR2LrQlBJCA7y5+x9NAXh50Z/kFRSZUoeIiHg2BSBPYrX9/Ui8CVNjlPhnt1ga1vEnPSuf935KMa0OERHxXApAnqbkabA/F0DuIVNK8PWycX/vlgBMXfIXB3PyTalDREQ8lwKQp4lsDVHtwF4Iv39hWhlXJETTrkEouQXF/Oe7rabVISIinkkByBN1+D/HTxOmxihhtVp4uJ9jcMRpy3fx1wH3TtQqIiKeTQHIE7W9Fiw2x3hAB7aYVsYFjeuS1CqCYrvB8/M3m1aHiIh4HgUgTxRUH5pd5nhtYmdogAf7tMRqgQW/p7Nix2FTaxEREc+hAOSpSjpDr58J9mLTymgWGczgLo0ATZEhIiLuowDkqZr3Ab9QyNoDO34ytZR/JzUjwMfGml0ZzNuYZmotIiLiGRSAPJW3H7S9xvHapKkxSkSE+HHrRY0BeG7+ZgqK7KbWIyIitZ8CkCcruQ226WvIN/cprNsubky9IF92Hspj2rKdptYiIiK1nwKQJ2vQBeo0gcI82PSNqaUE+nrx78uaAfCf5K1kHSs0tR4REandFIA8mcXy91WgdeaNCVRicOeGNKkfyJG8QqYu/svsckREpBZTAPJ0JXODpfwEGebOzu5ls/JQX8fgiO/9nMLejKOm1iMiIrWXApCnC4+F2AsBAzbMNLsaklpF0DW+DvlFdl5a+KfZ5YiISC1VLQLQlClTiIuLw8/Pj8TERJYvX15u2w8//BCLxeKy+Pn5ubQxDIPx48cTHR2Nv78/SUlJbN2q+abK1eH4bbC108HkcXgslr+nyPhizW7+2Jtlaj0iIlI7mR6AZsyYwZgxY5gwYQKrV6+mffv29O7dm/3795e7TUhICPv27XMuO3e6PjX0/PPP8+qrrzJ16lSWLVtGYGAgvXv35tixY1V9OjVTqyvByx8ObYU9q82uhg4Nw7iiXTSGAZPmbTK7HBERqYVMD0Avv/wyt956KyNHjqR169ZMnTqVgIAA3n///XK3sVgsREVFOZfIyEjnZ4ZhMHnyZB599FGuuuoq2rVrx8cff8zevXuZPXu2G86oBvILgVYDHK+rQWdogAd6t8TbZuGnrQf58c8DZpcjIiK1jKkBqKCggFWrVpGUlORcZ7VaSUpKYunSpeVul5OTQ2xsLA0bNuSqq67i999/d36WkpJCWlqayz5DQ0NJTEwsd5/5+flkZWW5LB6n/Q2Onxs/h6J8c2sBGtUNYFi3OAAmzdtMsV1TZIiISOUxNQAdPHiQ4uJilys4AJGRkaSllT0lQosWLXj//ff56quv+OSTT7Db7XTv3p3du3cDOLc7m31OmjSJ0NBQ59KwYcOKnlrN07gXBEfD0SOwdaHZ1QBw1yVNCfbzYtO+LL5cs8fsckREpBYx/RbY2erWrRvDhg2jQ4cO9OzZky+++IL69evz1ltvnfM+x40bR2ZmpnNJTTX3cXBTWG1/PxJv8tQYJcIDfbjrkqYAvLRwC8cKzZu0VUREahdTA1C9evWw2Wykp6e7rE9PTycqKuqM9uHt7U3Hjh3Ztm0bgHO7s9mnr68vISEhLotHanf8NtjWBZB7yNxajhvePY7zwvzZl3mM939JMbscERGpJUwNQD4+PnTq1Ink5GTnOrvdTnJyMt26dTujfRQXF7Nhwwaio6MBiI+PJyoqymWfWVlZLFu27Iz36bEiW0N0e7AXOfoCVQN+3jbG9m4OwJs//MWhHPP7J4mISM1n+i2wMWPG8M477/DRRx+xadMm7rjjDnJzcxk5ciQAw4YNY9y4cc72TzzxBAsXLmT79u2sXr2aG2+8kZ07d3LLLbcAjifERo8ezVNPPcXXX3/Nhg0bGDZsGDExMQwcONCMU6xZ2v+f42c1eRoM4Kr259EmJoTs/CJe+36b2eWIiEgt4GV2AYMHD+bAgQOMHz+etLQ0OnTowPz5852dmHft2oXV+ndOO3LkCLfeeitpaWmEh4fTqVMnfv31V1q3bu1s88ADD5Cbm8ttt91GRkYGF154IfPnzy81YKKUoe01sPAR2LsG9m+GiJZmV4TV6hgccei7y/jkt52M6B5HXL1As8sSEZEazGIYJg/9Ww1lZWURGhpKZmamZ/YHmnYD/DkPLvw3JE00uxqnER8sZ/GWA/RPiGbK0PPNLkdERKqZs/n9bfotMKmGSqbGWDcD7NXnyauH+rbEaoE5G/axetcRs8sREZEaTAFISmveB/zCIHsvpPxodjVOLaNCuLZTAwCembMJXbwUEZFzpQAkpXn5OvoCAayrHmMClRhzWQv8vK2s3HmEhX+kn34DERGRMigASdnaH78NtukbyM82t5YTRIX6ccuFjQF4bt5mCovtJlckIiI1kQKQlK1BZ6jTBArzHCGoGvlXz8bUDfRh+8FcPl3hgaN2i4hIhSkASdkslr87Q6+tPmMCAQT7eXNvUjMA/vPdn+TkF5lckYiI1DQKQFK+doMdP3f8BBm7zK3lJEO6NqJxvUAO5hTw1pK/zC5HRERqGAUgKV9YI4i7yPF6/UxzazmJt83KA30cgzS+89N20rOOmVyRiIjUJApAcmolnaHXTYdq9th57zaRdI4N51ihnZcX/ml2OSIiUoMoAMmptb4SvAPg0DbYs8rsalxYLBbG9WsFwGerUtmSVn2eVhMRkepNAUhOzTcYWg1wvK5mnaEBOsWG0y8hCrsBz87bZHY5IiJSQygAyem1v8Hxc+PnUJRvbi1luL93S7ysFn7YcoBftx00uxwREakBFIDk9OJ7QnAMHMuAPxeYXU0p8fUCufGCWACembcJu7169VUSEZHqRwFITs9qg3bXO15Xs6kxStz9j6YE+3qxcU8WX6/ba3Y5IiJSzSkAyZkpuQ22dSHkVr/bTHWDfLm9VxMAXliwhWOF1WcWexERqX4UgOTMRLSC6A5gL3L0BaqGbr4wnuhQP/ZkHOXjpTvMLkdERKoxBSA5cx3+z/GzGj4NBuDnbWPMZc0BeP37bWTkFZhckYiIVFcKQHLm2l4DVi/Ytxb2V89Hzq8+vwEto4LJOlbE699vM7scERGpphSA5MwF1oNmlzter/vU3FrKYbP+PTjix0t3kno4z+SKRESkOlIAkrNTMjXG+hlgr54djXs2r89FzepRUGzn+QVbzC5HRESqIQUgOTvNe4NfGGTvg5QlZldTrof6tsRigW/W7WVdaobZ5YiISDWjACRnx8sXEq51vE5+ArLTza2nHG1iQhnU8TwAnpm7CaOaTeQqIiLmUgCSs5d4O/iGwt418HYv2F29JkktMfbyFvh6WVmWcpjvN+83uxwREalGFIDk7NVrBrd+D/VaQPZe+KAvrPnE7KpKiQnz56YL4wGYNG8zRcV2kysSEZHqQgFIzk29pnDLd9DyCijOh69GwdwHoLjQ7Mpc3NGrCeEB3mzbn8PMlbvNLkdERKoJBSA5d34hcP1/odfDjvfL34KPB0LOAVPLOlGInzf3XNoMgFe++5Pc/CKTKxIRkepAAUgqxmqFXg/CDdPBJxh2/uzoF7R3jdmVOQ1NjCW2bgAHsvN556ftZpcjIiLVgAKQVI6W/Rz9guo2hazd8H6fajNYoo+XlQd6twTg7R+3sz/7mMkViYiI2RSApPLUb+4IQc37QNEx+PJfMP9hKDb/tlO/hCg6NAwjr6CYyd9tNbscERExmQKQVC6/UMftsIsfcLz/bQp8MghyD5lalsVi4ZH+jikyZqxIZdv+bFPrERERc1WLADRlyhTi4uLw8/MjMTGR5cuXl9v2nXfe4aKLLiI8PJzw8HCSkpJKtR8xYgQWi8Vl6dOnT1WfhpSwWuEfjzg6SPsEQcqPjn5B+9abWlaXuDpc3jqSYrvBs/M0RYaIiCczPQDNmDGDMWPGMGHCBFavXk379u3p3bs3+/eXPXDd4sWLGTJkCD/88ANLly6lYcOGXH755ezZs8elXZ8+fdi3b59zmT59ujtOR07U+krHo/J1GkPmLnjvctgwy9SSHuzbEpvVwneb0lm23dyrUiIiYh6LYfIcAYmJiXTp0oXXX38dALvdTsOGDbn77rt56KGHTrt9cXEx4eHhvP766wwbNgxwXAHKyMhg9uzZ51RTVlYWoaGhZGZmEhISck77kBMcPQKf3wrbFjned78bLp0INi9Tynl09gY++W0X7RuE8uWdPbBaLabUISIiletsfn+begWooKCAVatWkZSU5FxntVpJSkpi6dKlZ7SPvLw8CgsLqVOnjsv6xYsXExERQYsWLbjjjjs4dEj/2jeNfzj83wy4cIzj/a+vwf+uhbzDppRz76XNCfSxsW53JnM27DOlBhERMZepAejgwYMUFxcTGRnpsj4yMpK0tLQz2seDDz5ITEyMS4jq06cPH3/8McnJyTz33HMsWbKEvn37UlxcXOY+8vPzycrKclmkklltkDQBrvsQvANg+w/wziWQ/rvbS6kf7Mu/ejYB4PkFm8kvKvu/CxERqb1M7wNUEc8++yyffvopX375JX5+fs71N9xwA1deeSUJCQkMHDiQb7/9lhUrVrB48eIy9zNp0iRCQ0OdS8OGDd10Bh6ozSC4eRGExcKRHfBuEvz+pdvLuOWieCKCfUk9fJQ3fvhLs8WLiHgYUwNQvXr1sNlspKenu6xPT08nKirqlNu++OKLPPvssyxcuJB27dqdsm3jxo2pV68e27ZtK/PzcePGkZmZ6VxSU1PP7kTk7ES1hdsWQ+NLoDAPPhsB300Eu/uuxAT4eDHmsuYA/Cd5KwPf+JXlKebckhMREfczNQD5+PjQqVMnkpOTnevsdjvJycl069at3O2ef/55nnzySebPn0/nzp1Pe5zdu3dz6NAhoqOjy/zc19eXkJAQl0WqWEAdGDoLut/jeP/zKzDtekeHaTe5vnND7u/dggAfG+tSM7j+raXc+vFK/jqQ47YaRETEHKY/BTZjxgyGDx/OW2+9RdeuXZk8eTIzZ85k8+bNREZGMmzYMM477zwmTZoEwHPPPcf48eOZNm0aPXr0cO4nKCiIoKAgcnJyePzxx7nmmmuIiorir7/+4oEHHiA7O5sNGzbg6+t72pr0FJibbZgFX90FRUcdj8zfMA0iWrnt8PuzjzH5u63MWJFKsd3AZrUwpGtDRic1p17Q6f97ERGR6uFsfn+bHoAAXn/9dV544QXS0tLo0KEDr776KomJiQD06tWLuLg4PvzwQwDi4uLYuXNnqX1MmDCBiRMncvToUQYOHMiaNWvIyMggJiaGyy+/nCeffLJUZ+vyKACZYN96+HSoY7wgnyAY+KZjHCE32rY/m2fnbea7TY4xqAJ9bNzeswm3XNQYfx+bW2sREZGzV+MCUHWjAGSS3EPw2XDY8ZPj/cX3Q6+HHSNLu9Fv2w/xzNxNrN+dCUBkiC/3XdaCazo1wKYxg0REqi0FoApSADJRcREsegx+e8PxvnkfuPptxxxjbmS3G3yzfi8vLNjC7iNHAWgZFcxDfVvSs3l9LBYFIRGR6kYBqIIUgKqBdZ/CN/c6ZpWv29QxwWr95m4vI7+omI9/3clr328l65hjVvsLm9ZjXL+WtIlxbygTEZFTUwCqIAWgamLvGvj0RsjaDT7BjitBLfuZUkpGXgGvf7+Nj5fupKDYjsUCgzqex9jLWxAT5m9KTSIi4koBqIIUgKqRnAOOfkE7f3G87zUOLn7A7f2CSqQezuP5BVv4Zt1eAHy9rNx0YTx39GpCiJ+3KTWJiIiDAlAFKQBVM8WFsOBhWP62432L/jBoKviZ992sS83g6bmbnIMn1gn04Z5/NOX/EmPx8arRA6yLiNRYCkAVpABUTa35BL79NxQXQL0WjvGC6jU1rRzDMPhu036enbeJvw7kAhBXN4AH+7SkT9sodZQWEXEzBaAKUgCqxnavghk3QvZe8A2Fa96B5r1NLamo2M6Mlam8smgrB3PyAegUG87D/VrSKbaOqbWJiHgSBaAKUgCq5rLTYeYwSP0NsMA/HoGLxoLJV1xy8ot4+8ftvPPjdo4WOuY169s2igf6tCS+XqCptYmIeAIFoApSAKoBigpg/oOw8n3H+1ZXOkaP9g0yty4gPesYryz6k5krU7Eb4GW1cOMFsdxzaTPqBPqYXZ6ISK2lAFRBCkA1yKoPYc5YsBdC/VYwZJpjPrFqYEtaNs/O28QPWw4AEOzrxR2XNOGmHvH4eWtqDRGRyqYAVEEKQDVM6nKY8U/ISXOMGH3t+9A0yeyqnH7ddpCn527i971ZAESH+nHf5S24uuN5WDW1hohIpVEAqiAFoBooax/M/CfsXgEWK1w6HnqMNr1fUAm73eCrdXt4ccGf7MlwTK3RKjqEh/u15KJm9U2uTkSkdlAAqiAFoBqqKB/mjoXVHzvetxkEV00Bn+rTAflYYTEf/rqDKT9sI/v41Bo9m9dnXL+WtIzSf2siIhWhAFRBCkA1mGE4OkbPewDsRRDZFgZ/AnXiza7MxZHcAl79fiuf/LaTwmIDqwWu7dSAMZe1ICrUz+zyRERqJAWgClIAqgV2LnU8Kp+7H/zD4doPoMklZldVys5DuTw/fwtzNuwDwM/byq0XNeZfPZsQ5OtlcnUiIjWLAlAFKQDVEpl7HIMm7l3t6Bd02RPQ7a5q0y/oRGt2HeGZuZtYseMIAHUDfRid1IwbujbC26apNUREzoQCUAUpANUihcdgzhhY+z/H+4TrYMCr4BNgbl1lMAyDhX+k89y8zWw/6Jhao3H9QB7s05LLW0dqag0RkdNQAKogBaBaxjBg+Tsw/yEwiiEqwTGPWFgjsysrU2GxnU+X72Lyd1s5lFsAQNe4Oozr15KOjcJNrk5EpPpSAKogBaBaasfPjn5BeYcgoC5c9yHEX2x2VeXKPlbIW0u28+7P2zlWaAegf7toHuzdkkZ1q98VLBERsykAVZACUC2WkQozhsK+dWCxQe+nIfH2atkvqMS+zKO8vPBPZq3ejWGAt83CPy+I4+5/NCVcU2uIiDgpAFWQAlAtV3gUvrkX1s9wvI9sCzEdIKqd4/ZYZBvHiNLVzKZ9WUyat5kf/zw+tYafF3dd0pTh3eM0tYaICApAFaYA5AEMA357ExY+6ugXdLLwOEcYcoaithDaoFpcKfpp6wGembuZTfscU2ucF+bP/b1bcGX7GE2tISIeTQGoghSAPEjWXti9EtI2/L1k7S67rV+YayiKSoD6LcDm7daSAYrtBl+u2cNLC7ewL/MYAGEB3sTVDSS+XiBxdQOJqxdw/Gcgof7ur1FExN0UgCpIAcjD5R2G9I2uoejAZsfI0iez+UD9lq6hKKqt226hHSss5r2fU5i6+C+y88uo77g6gT7E1f07EMXVC3S8rxdIiJ/CkYjUDgpAFaQAJKUU5TtC0ImhKG0D5GeV3T4stvTVoiq8hXassJjtB3LZcSiXlIO57DyUy46DeaQcyuVAdv4pt60b6ENcvUBi6wYQfzwgxR9/H6xwJCI1iAJQBSkAyRkxDMjYWToUZaaW3d7lFlpbx+t6LcCrap/kyskvcgaiEwNSysE8DuacOhzVC/Ihrm4gsXUDia8XcPzKkSMkaaoOEaluFIAqSAFIKuRsbqFZvSHipFtokW3BP8wtpWYfK2TnIUcw2nHQEYp2HnJcSTqYU3DKbesF+RJfL+B4OHLtdxSocCQiJlAAqiAFIKl0Z30LrdFJ/YoSILShW59CyzpWyM6DJ4SjQ7mOsHQw1zlCdXnqB/sev53mGpBi6wYoHIlIlVEAqiAFIHELw4CMXWXcQttVdnu/0NKhyA230MqSebTw+JUiRyA6MSAdPk04igj2dfQzqhtIbL2/+x3F1g0gwEfhSETOnQJQBSkAianyDkP67yfdQtt0+ltooQ3AJxB8gsA3+PjPoBPeH//MJwisVTfDfGZeoeOqURn9jo7kFZ5y28gQXyJD/PDzthHgY8Pf+/jic8LP468DfGz4eZe89sLfx4q/t5drW28bPl5Vd64iUr0oAFWQApBUO0X5cGBLGbfQMs9tf96BJ4SjoL+DkUtgKnkfCD7BJ7UP/vsz78AzDlSZeYWkHL+l9vetNcdVpMyjpw5H58rLasHf24afzwmhqoxwFeDjaON/Yvjy8Tr+0zVclYSvkp82DUApUi3UuAA0ZcoUXnjhBdLS0mjfvj2vvfYaXbt2Lbf9Z599xmOPPcaOHTto1qwZzz33HP369XN+bhgGEyZM4J133iEjI4MePXrw5ptv0qxZszOqRwFIaoQTb6Glb4Sc/VCQCwU5kJ99/GfOCT+zwbBXQSGWE648lXHFqcxQVTpwZRb7sCPbwsF8G0eLDI4WFHO0sJijBcXkFRRzrNDxPu+E9Se2KfnsWGExeQVF2N34fzYfL2upcOXrZcXbVrJY8Dr+09tmxctqxcfLgpfVipfNgo/N8bOkvZfV4tzO22Y9aVsL3l5WvK2l9+vS3lry+u/9KqhJbVejAtCMGTMYNmwYU6dOJTExkcmTJ/PZZ5+xZcsWIiIiSrX/9ddfufjii5k0aRJXXHEF06ZN47nnnmP16tW0bdsWgOeee45Jkybx0UcfER8fz2OPPcaGDRv4448/8PPzO21NCkBSKxkGFB37Owy5hKOTXpcEqILcU7evkkAFYAGrl2OUbas3WG3HX3v9vZzivWH1wm71wo6NohMXw0ohNgoNG4WGlULDSr7dRoFhpcCwkl9s5ZhhJb/YwrFiK8fsVo4WWThmt5BX5FiXW2whrxDyiq0UY6XQsFGEF0VYT/hpoxgrdsOKHSt2LMcXq/OngcXRxrn+73YGFsefQSWzWvg7HHkdD2LHQ1SpIHY8nHnZrNgsYLVYsFot2CwWbFYLFgvYjr+3Wi1Yj7+3Hv/cail57bqts23JuuPvS7Zz7LvkOJzwuoxtLRas1hO2tRzf1nrCcU+ox1JyHhbH8wQWiwULf7ezHF9/YluLBSyUbPP3T5d9YSm1XUk7ca8aFYASExPp0qULr7/+OgB2u52GDRty991389BDD5VqP3jwYHJzc/n222+d6y644AI6dOjA1KlTMQyDmJgY7rvvPsaOHQtAZmYmkZGRfPjhh9xwww2nrUkBSOQMGIZjYtmyrjiVdfUpP+c0V6hyANMvSFcbJSHJjhW75e/XrsHJgmFYKT4hXBUZxz8zHJ+XtDWO/yx27rfks+PvjRM/s56w3d+L4z0u64wTQptzMUrqP7lt2evtJ73G2bac9YbVZV+47KO89a6141IDpd67tDHKWOd8f/Kfieu+HJs42lksVozjgYnj4QuL9Xgbq+MhT8vx28nHf1osx9fjTG3Hg5XFGbBO3I/lxHZwQtuSfVqc4e/v9SW1WLDgerwT25W8xvL3e8uJ7ZxhsGQfjvpPPD9HMysWq4VOzWNJOr/F2f7VOKWz+f1t6iMXBQUFrFq1inHjxjnXWa1WkpKSWLp0aZnbLF26lDFjxris6927N7NnzwYgJSWFtLQ0kpKSnJ+HhoaSmJjI0qVLzygAicgZsFjAJ8CxBJW+WnvWDAMK8xz9nYoLHZ2+7YVgL67C90Vn19al/aneFzteG/YTluKzumJmxdHWRvHZ5ULLST+l5jDwqH8DLM0aDue/atrxTQ1ABw8epLi4mMjISJf1kZGRbN68ucxt0tLSymyflpbm/LxkXXltTpafn09+/t8j4mZllTM2i4hUHUtJX6JAsyupWobhCEhlhSPDXsbnJ7W1l7fdyZ+d9Lm92LHvcrc98fOTtsNwfFby0/naXvq1YT+hXclryll/4mv7mR+jzOOdyTGMMn6e8L2U28ag5GaJ4azNfvwj+9+fH1+Ot3S8LmN/zhsvJ64r78/4eDvnfk6s21m/cQavT9H+xFo4fk3Lpe0J7XA9dqm2Jfs4aRuLy34c6xvUM/cOiwbdACZNmsTjjz9udhki4gksFrDpf701jS6sVb6GJh/f1AEy6tWrh81mIz093WV9eno6UVFRZW4TFRV1yvYlP89mn+PGjSMzM9O5pKaWM5eTiIiI1AqmBiAfHx86depEcnKyc53dbic5OZlu3bqVuU23bt1c2gMsWrTI2T4+Pp6oqCiXNllZWSxbtqzcffr6+hISEuKyiIiISO1l+nXYMWPGMHz4cDp37kzXrl2ZPHkyubm5jBw5EoBhw4Zx3nnnMWnSJADuvfdeevbsyUsvvUT//v359NNPWblyJW+//Tbg6JU+evRonnrqKZo1a+Z8DD4mJoaBAweadZoiIiJSjZgegAYPHsyBAwcYP348aWlpdOjQgfnz5zs7Me/atQvrCaPMdu/enWnTpvHoo4/y8MMP06xZM2bPnu0cAwjggQceIDc3l9tuu42MjAwuvPBC5s+ff0ZjAImIiEjtZ/o4QNWRxgESERGpec7m97dmCRQRERGPowAkIiIiHkcBSERERDyOApCIiIh4HAUgERER8TgKQCIiIuJxFIBERETE4ygAiYiIiMdRABIRERGPY/pUGNVRyeDYWVlZJlciIiIiZ6rk9/aZTHKhAFSG7OxsABo2bGhyJSIiInK2srOzCQ0NPWUbzQVWBrvdzt69ewkODsZisVTqvrOysmjYsCGpqamaZ6wa0PdRvej7qF70fVQv+j5OzzAMsrOziYmJcZlIvSy6AlQGq9VKgwYNqvQYISEh+g+4GtH3Ub3o+6he9H1UL/o+Tu10V35KqBO0iIiIeBwFIBEREfE4CkBu5uvry4QJE/D19TW7FEHfR3Wj76N60fdRvej7qFzqBC0iIiIeR1eARERExOMoAImIiIjHUQASERERj6MAJCIiIh5HAciNpkyZQlxcHH5+fiQmJrJ8+XKzS/JIkyZNokuXLgQHBxMREcHAgQPZsmWL2WXJcc8++ywWi4XRo0ebXYpH27NnDzfeeCN169bF39+fhIQEVq5caXZZHqm4uJjHHnuM+Ph4/P39adKkCU8++eQZzXcl5VMAcpMZM2YwZswYJkyYwOrVq2nfvj29e/dm//79ZpfmcZYsWcKoUaP47bffWLRoEYWFhVx++eXk5uaaXZrHW7FiBW+99Rbt2rUzuxSPduTIEXr06IG3tzfz5s3jjz/+4KWXXiI8PNzs0jzSc889x5tvvsnrr7/Opk2beO6553j++ed57bXXzC6tRtNj8G6SmJhIly5deP311wHHfGMNGzbk7rvv5qGHHjK5Os924MABIiIiWLJkCRdffLHZ5XisnJwczj//fN544w2eeuopOnTowOTJk80uyyM99NBD/PLLL/z0009mlyLAFVdcQWRkJO+9955z3TXXXIO/vz+ffPKJiZXVbLoC5AYFBQWsWrWKpKQk5zqr1UpSUhJLly41sTIByMzMBKBOnTomV+LZRo0aRf/+/V3+nog5vv76azp37sx1111HREQEHTt25J133jG7LI/VvXt3kpOT+fPPPwFYt24dP//8M3379jW5sppNk6G6wcGDBykuLiYyMtJlfWRkJJs3bzapKgHHlbjRo0fTo0cP2rZta3Y5HuvTTz9l9erVrFixwuxSBNi+fTtvvvkmY8aM4eGHH2bFihXcc889+Pj4MHz4cLPL8zgPPfQQWVlZtGzZEpvNRnFxMU8//TRDhw41u7QaTQFIPNqoUaPYuHEjP//8s9mleKzU1FTuvfdeFi1ahJ+fn9nlCI5/GHTu3JlnnnkGgI4dO7Jx40amTp2qAGSCmTNn8r///Y9p06bRpk0b1q5dy+jRo4mJidH3UQEKQG5Qr149bDYb6enpLuvT09OJiooyqSq56667+Pbbb/nxxx9p0KCB2eV4rFWrVrF//37OP/9857ri4mJ+/PFHXn/9dfLz87HZbCZW6Hmio6Np3bq1y7pWrVrx+eefm1SRZ7v//vt56KGHuOGGGwBISEhg586dTJo0SQGoAtQHyA18fHzo1KkTycnJznV2u53k5GS6detmYmWeyTAM7rrrLr788ku+//574uPjzS7Jo1166aVs2LCBtWvXOpfOnTszdOhQ1q5dq/Bjgh49epQaGuLPP/8kNjbWpIo8W15eHlar669rm82G3W43qaLaQVeA3GTMmDEMHz6czp0707VrVyZPnkxubi4jR440uzSPM2rUKKZNm8ZXX31FcHAwaWlpAISGhuLv729ydZ4nODi4VP+rwMBA6tatq35ZJvn3v/9N9+7deeaZZ7j++utZvnw5b7/9Nm+//bbZpXmkAQMG8PTTT9OoUSPatGnDmjVrePnll7npppvMLq1G02PwbvT666/zwgsvkJaWRocOHXj11VdJTEw0uyyPY7FYylz/wQcfMGLECPcWI2Xq1auXHoM32bfffsu4cePYunUr8fHxjBkzhltvvdXssjxSdnY2jz32GF9++SX79+8nJiaGIUOGMH78eHx8fMwur8ZSABIRERGPoz5AIiIi4nEUgERERMTjKACJiIiIx1EAEhEREY+jACQiIiIeRwFIREREPI4CkIiIiHgcBSARkTNgsViYPXu22WWISCVRABKRam/EiBFYLJZSS58+fcwuTURqKM0FJiI1Qp8+ffjggw9c1vn6+ppUjYjUdLoCJCI1gq+vL1FRUS5LeHg44Lg99eabb9K3b1/8/f1p3Lgxs2bNctl+w4YN/OMf/8Df35+6dety2223kZOT49Lm/fffp02bNvj6+hIdHc1dd93l8vnBgwcZNGgQAQEBNGvWjK+//rpqT1pEqowCkIjUCo899hjXXHMN69atY+jQodxwww1s2rQJgNzcXHr37k14eDgrVqzgs88+47vvvnMJOG+++SajRo3itttuY8OGDXz99dc0bdrU5RiPP/44119/PevXr6dfv34MHTqUw4cPu/U8RaSSGCIi1dzw4cMNm81mBAYGuixPP/20YRiGARi33367yzaJiYnGHXfcYRiGYbz99ttGeHi4kZOT4/x8zpw5htVqNdLS0gzDMIyYmBjjkUceKbcGwHj00Ued73NycgzAmDdvXqWdp4i4j/oAiUiNcMkll/Dmm2+6rKtTp47zdbdu3Vw+69atG2vXrgVg06ZNtG/fnsDAQOfnPXr0wG63s2XLFiwWC3v37uXSSy89ZQ3t2rVzvg4MDCQkJIT9+/ef6ymJiIkUgESkRggMDCx1S6qy+Pv7n1E7b29vl/cWiwW73V4VJYlIFVMfIBGpFX777bdS71u1agVAq1atWLduHbm5uc7Pf/nlF6xWKy1atCA4OJi4uDiSk5PdWrOImEdXgESkRsjPzyctLc1lnZeXF/Xq1QPgs88+o3Pnzlx44YX873//Y/ny5bz33nsADB06lAkTJjB8+HAmTpzIgQMHuPvuu/nnP/9JZGQkABMnTuT2228nIiKCvn37kp2dzS+//MLdd9/t3hMVEbdQABKRGmH+/PlER0e7rGvRogWbN28GHE9offrpp9x5551ER0czffp0WrduDUBAQAALFizg3nvvpUuXLgQEBHDNNdfw8ssvO/c1fPhwjh07xiuvvMLYsWOpV68e1157rftOUETcymIYhmF2ESIiFWGxWPjyyy8ZOHCg2aWISA2hPkAiIiLicRSARERExOOoD5CI1Hi6ky8iZ0tXgERERMTjKACJiIiIx1EAEhEREY+jACQiIiIeRwFIREREPI4CkIiIiHgcBSARERHxOApAIiIi4nEUgERERMTj/D90LxeooKWvcAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC: you want coffee\n",
            "REF: du will kaffee\n",
            "HYP: du will kaffee\n",
            "----\n",
            "SRC: i like bread\n",
            "REF: ich mag brot\n",
            "HYP: ich mag brot\n",
            "----\n",
            "SRC: they see music\n",
            "REF: sie sehe musik\n",
            "HYP: sie sehe musik\n",
            "----\n",
            "SRC: they know bread\n",
            "REF: sie kenne brot\n",
            "HYP: sie kenne brot\n",
            "----\n",
            "SRC: we want bread\n",
            "REF: wir will brot\n",
            "HYP: wir will brot\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– Academic Analysis of Results\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Training Dynamics\n",
        "\n",
        "### Rapid Convergence\n",
        "- **Training loss:** Decreased from **2.011 â†’ 0.007** across 10 epochs.  \n",
        "- **Validation loss:** Decreased from **1.139 â†’ 0.004** across 10 epochs.  \n",
        "\n",
        "### Stability\n",
        "- Both curves converge smoothly, **without oscillations or divergence**, showing that optimization was well-tuned.  \n",
        "- No evidence of **overfitting** was observed.  \n",
        "\n",
        "### Generalization\n",
        "- Validation loss closely follows training loss, confirming that the model is not merely memorizing but **generalizing effectively** within the synthetic domain.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Translation Quality\n",
        "\n",
        "### Correct Mappings\n",
        "- **SRC:** â€œi like breadâ€ â†’ **REF:** â€œich mag brotâ€ â†’ **HYP:** â€œich mag brotâ€  \n",
        "- **SRC:** â€œwe want breadâ€ â†’ **REF:** â€œwir will brotâ€ â†’ **HYP:** â€œwir will brotâ€  \n",
        "\n",
        "These examples show **near-perfect reproduction** of the reference translations.  \n",
        "\n",
        "### Consistency Across Forms\n",
        "- Sentences such as â€œyou want coffeeâ€ and â€œthey see musicâ€ produce outputs that **align with the references**, demonstrating strong lexical fidelity.  \n",
        "\n",
        "### Grammar Limitations\n",
        "- Example: â€œsie sehe musikâ€ (*HYP*) vs. correct German â€œsie sieht musik.â€  \n",
        "- This reflects the **synthetic grammar bias** â€” the model learned simplified dataset rules rather than authentic linguistic structures.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Theoretical Implications\n",
        "\n",
        "- Even as a **tiny Transformer + Mixture-of-Experts**, the model demonstrates inductive strengths similar to large-scale **NLLB**:  \n",
        "  - Efficient **alignment learning**  \n",
        "  - Robust **convergence**  \n",
        "  - Accurate **token-to-token mappings**  \n",
        "\n",
        "- **Limitation:** Success is bounded by the **synthetic and deterministic dataset**.  \n",
        "- True multilingual systems (e.g., NLLB) must handle **far greater variability, morphology, and syntactic complexity**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Conclusions\n",
        "\n",
        "- The **loss curves** confirm **rapid and stable convergence**.  \n",
        "- The **translation samples** confirm that the system successfully learned bilingual mappings, though constrained by dataset simplicity.  \n",
        "- At this scale, the experiment validates the **architectureâ€™s effectiveness**, while exposing the **gap between toy replication and real-world deployment**.  \n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Final Statement\n",
        "\n",
        "This experiment demonstrates that even in a **miniature synthetic environment**, a **Transformerâ€“Mixture of Experts architecture** can converge quickly and produce accurate translations.  \n",
        "\n",
        "The results highlight both:  \n",
        "- The **promise** of scalable architectures like **NLLB**, and  \n",
        "- The **necessity of rich, natural corpora** for genuine linguistic generalization.  "
      ],
      "metadata": {
        "id": "yMNtS9QNtEse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– Comparison at a Glance\n",
        "\n",
        "| Model family                          | Core idea                                           | Scalability (langs) | Low-resource quality                              | Training cost           | Inference latency      | Typical use                                            |\n",
        "|---------------------------------------|----------------------------------------------------|----------------------|--------------------------------------------------|--------------------------|-------------------------|--------------------------------------------------------|\n",
        "| **This mini NLLB (Transformer + tiny MoE)** | Encoder-decoder Transformer with sparse expert head | High in principle (MoE shards capacity) | Good on synthetic/regular data; depends on mining/backtranslation for real | Low (toy) â†’ High (real) | Moderate (k active experts) | Demos, pedagogy; blueprint for scaled MoE MT |\n",
        "| **RNN Seq2Seq (Sutskeverâ€™14)**        | LSTM encoder/decoder, no attention                  | Poor (long-range issues) | Weak; struggles with morphology                  | Lowâ€“Mid                 | Mid (autoregressive)   | Historical baseline                                   |\n",
        "| **RNN + Attention (Bahdanauâ€™15 / Luongâ€™15)** | Additive/dot attention over encoder states          | Moderate             | Better than plain RNNs; still limited             | Mid                     | Mid                    | Classic academic baselines; low-resource with strong priors |\n",
        "| **ConvS2S (Gehringâ€™17)**              | Fully convolutional sequence model                  | Moderate             | Mixed; better than RNNs, worse than Transformers at scale | Mid                     | Low (parallel conv)    | Latency-sensitive MT when training budget is small   |\n",
        "| **Transformer (Vaswaniâ€™17)**          | Self-attention + FFN; dense layers                  | High (multilingual with shared vocab) | Strong with backtranslation and shared subwords | Midâ€“High                | Mid                    | Default NMT backbone                                  |\n",
        "| **mBART (Liuâ€™20)**                    | Multilingual denoising autoencoder + fine-tune      | High (pretrained)    | Good, especially with fine-tuning per language pair | Mid (fine-tune)         | Mid                    | Few-shot/low-resource adaptation                     |\n",
        "| **M2M-100 (Fan/Nguyenâ€™20)**           | Single model for 100 langs, many-to-many            | High                 | Strong; depends on mined bitext                   | High                    | Midâ€“High               | Fully multilingual deployment (100+ langs)           |\n",
        "| **NLLB (Metaâ€™22)**                    | 200+ languages; massive data mining; MoE            | Very high (200+)     | SOTA for many low-resource; toxicity checks       | Very high               | Midâ€“High (MoE routing) | Web-scale MT, public goods                           |\n",
        "| **LLM prompting (GPT-style)**         | Prompted translation without pair-specific training | Unlimited (implicit) | Variable; strong for high-resource, uneven for low-resource | Very low (no fine-tune) | Midâ€“High (long context) | Quick coverage, zero-shot or inline translation      |\n",
        "\n",
        "---\n",
        "\n",
        "## What changes with the MoE head?\n",
        "\n",
        "**Capacity vs. compute:**\n",
        "- Dense Transformer: capacity grows **linearly** with compute.  \n",
        "- MoE: adds parameters (experts) but routes each token to **k experts** â†’ higher representational power at similar FLOPs.  \n",
        "\n",
        "Your tiny MoE mirrors NLLBâ€™s conditional compute idea; at scale it enables **language-specialized experts** without exploding per-token cost.  \n",
        "\n",
        "---\n",
        "\n",
        "## Strengths & Limitations (your model)\n",
        "\n",
        "### Strengths\n",
        "- Captures alignment quickly; converges smoothly (as curves showed).  \n",
        "- Modular scaling path: you can increase experts/layers or shard experts across devicesâ€”same principle NLLB uses.  \n",
        "- Clean pipeline (local vocab; detachable tokenizer), easy to extend to multilingual.  \n",
        "\n",
        "### Limitations\n",
        "- Current dataset is synthetic & deterministic â†’ results **overestimate real performance**.  \n",
        "- No bitext mining, backtranslation, or toxicity controlsâ€”all critical in production multilingual MT.  \n",
        "- Decoding is greedy; SOTA uses **beam search, length penalty, or Nucleus/contrastive decoding variants**.  \n",
        "\n",
        "---\n",
        "\n",
        "## When each family wins\n",
        "\n",
        "- **Low budget / low latency:** ConvS2S or small Transformers with distillation.  \n",
        "- **Classic academic baselines or small corpora:** RNN+Attention (still instructive).  \n",
        "- **General purpose MT (few languages, strong data):** Dense Transformer (standard NMT).  \n",
        "- **Massively multilingual (50â€“200+):** Transformer + MoE (M2M-100, NLLB); invest in mining, backtranslation, safety.  \n",
        "- **Rapid coverage without training:** LLM prompting; add guardrails for consistency and safety.  \n",
        "\n",
        "---\n",
        "\n",
        "## Practical upgrade path for your replica\n",
        "\n",
        "- **Data realism:** add mined or open parallel corpora + backtranslation from monolingual data.  \n",
        "- **Routing quality:** switch to Top-k balanced load routing (aux loss to avoid expert collapse).  \n",
        "- **Decoding:** implement beam search with length penalty; report BLEU + ChrF + COMET.  \n",
        "- **Safety:** integrate a lightweight toxicity filter (character n-grams or small classifier).  \n",
        "- **Multilinguality:** train with 3â€“5 languages sharing a SentencePiece vocab; add language tags for many-to-many.  \n",
        "\n",
        "---\n",
        "\n",
        "## Bottom line\n",
        "\n",
        "Your model is architecturally aligned with the modern SOTA direction (**Transformer+MoE**) that powers systems like **NLLB**.  \n",
        "\n",
        "Against older competitors (RNN/ConvS2S), it enjoys **better long-range modeling and scaling**.  \n",
        "\n",
        "To match real **NLLB-class systems**, the differentiators are:  \n",
        "- **Data scale/quality**  \n",
        "- **Routing stability**  \n",
        "- **Evaluation rigor**  \n",
        "- **Safety tooling**  \n",
        "\n",
        "â€”not the basic network skeleton."
      ],
      "metadata": {
        "id": "7NU3GHLwtnD7"
      }
    }
  ]
}