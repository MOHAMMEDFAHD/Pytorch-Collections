{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìë Academic Summary of *Attention Is All You Need* (Vaswani et al., 2017)\n",
        "\n",
        "\n",
        "# https://arxiv.org/pdf/1706.03762\n",
        "---\n",
        "\n",
        "## Abstract  \n",
        "\n",
        "The paper introduces the **Transformer**, a novel sequence transduction model that eliminates recurrence and convolution, relying solely on **self-attention mechanisms**.  \n",
        "Evaluated on large-scale machine translation tasks (EN‚ÄìDE, EN‚ÄìFR), the Transformer achieves **state-of-the-art BLEU scores**, while being **highly parallelizable** and **faster to train**.  \n",
        "It further generalizes to other tasks, including **parsing**.  \n",
        "\n",
        "---\n",
        "\n",
        "## Problem  \n",
        "\n",
        "- Traditional models: **RNNs (LSTM, GRU)** and **CNN-based encoder‚Äìdecoders**.  \n",
        "- Limitations:  \n",
        "  - **Sequential bottleneck** ‚Üí inefficient for long sequences.  \n",
        "  - **Difficulty with long-range dependencies** (vanishing gradients).  \n",
        "  - **High training costs** (weeks for SOTA translation).  \n",
        "- Challenge: *Can a model capture dependencies and achieve competitive translation quality **without recurrence or convolution***?  \n",
        "\n",
        "---\n",
        "\n",
        "## Purposes  \n",
        "\n",
        "1. Propose an architecture based **entirely on self-attention**.  \n",
        "2. Demonstrate **faster training** and **better scalability**.  \n",
        "3. Validate performance on **machine translation benchmarks**.  \n",
        "4. Investigate **generalization** to other tasks (e.g., parsing).  \n",
        "\n",
        "---\n",
        "\n",
        "## Methodology  \n",
        "\n",
        "### Architecture  \n",
        "- Encoder‚Äìdecoder with **6 stacked layers** each.  \n",
        "- **Encoder layer:** Multi-head self-attention + feed-forward network.  \n",
        "- **Decoder layer:** Masked self-attention + encoder‚Äìdecoder attention + feed-forward.  \n",
        "- **Enhancements:** Residual connections + Layer Normalization.  \n",
        "\n",
        "### Key Mechanisms  \n",
        "- **Scaled Dot-Product Attention:**  \n",
        "  $$\n",
        "  \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "  $$\n",
        "- **Multi-Head Attention:** Parallel projections into multiple subspaces.  \n",
        "- **Positional Encoding:** Sinusoidal functions inject order information.  \n",
        "\n",
        "### Training Setup  \n",
        "- **Optimizer:** Adam with custom learning rate schedule (warm-up).  \n",
        "- **Regularization:** Dropout + label smoothing.  \n",
        "- **Datasets:** WMT‚Äô14 EN‚ÄìDE (4.5M pairs), EN‚ÄìFR (36M pairs).  \n",
        "\n",
        "---\n",
        "\n",
        "## Results  \n",
        "\n",
        "- **Translation Performance:**  \n",
        "  - EN‚ÄìDE: **28.4 BLEU** (+2 BLEU over previous best ensembles).  \n",
        "  - EN‚ÄìFR: **41.8 BLEU**, new single-model SOTA.  \n",
        "- **Efficiency:**  \n",
        "  - Base Transformer trained in **12h** (8 GPUs).  \n",
        "  - Big Transformer trained in **3.5 days**, compared to **weeks** for RNN/CNN systems.  \n",
        "- **Model Variations:**  \n",
        "  - Increasing heads and depth improved quality.  \n",
        "  - Positional encoding variants showed negligible difference.  \n",
        "- **Generalization:**  \n",
        "  - English constituency parsing ‚Üí outperformed prior non-RNN baselines, even with limited data.  \n",
        "\n",
        "---\n",
        "\n",
        "## Conclusions  \n",
        "\n",
        "- First **fully attention-based transduction model**.  \n",
        "- **Eliminates recurrence and convolution** while achieving SOTA performance.  \n",
        "- Advantages:  \n",
        "  - **Parallelization**.  \n",
        "  - **Constant path length** for long dependencies.  \n",
        "  - **Interpretability** of attention heads.  \n",
        "- Future Directions:  \n",
        "  - Extend attention-based architectures to **vision, audio, video**.  \n",
        "  - Explore **restricted/local attention** for efficiency.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2btNvy-_fNyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìê Mathematical & Statistical Equations in *Attention Is All You Need* (Vaswani et al., 2017)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Scaled Dot-Product Attention  \n",
        "\n",
        "The fundamental attention mechanism:  \n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
        "$$  \n",
        "\n",
        "- $Q$: query matrix  \n",
        "- $K$: key matrix  \n",
        "- $V$: value matrix  \n",
        "- $d_k$: dimensionality of keys  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Multi-Head Attention  \n",
        "\n",
        "Instead of a single attention function, use $h$ parallel heads:  \n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
        "$$  \n",
        "\n",
        "where each head is:  \n",
        "\n",
        "$$\n",
        "\\text{head}_i = \\text{Attention}(Q W_i^Q, \\; K W_i^K, \\; V W_i^V)\n",
        "$$  \n",
        "\n",
        "- $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$  \n",
        "- $W^O \\in \\mathbb{R}^{h d_v \\times d_{\\text{model}}}$  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Position-wise Feed-Forward Network (FFN)  \n",
        "\n",
        "Each position independently applies:  \n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
        "$$  \n",
        "\n",
        "- Two linear transformations with a **ReLU** in between.  \n",
        "- Identical parameters for all positions.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Positional Encoding  \n",
        "\n",
        "Since attention is **permutation-invariant**, positional encodings inject sequence order.  \n",
        "\n",
        "For position $pos$ and dimension $i$:  \n",
        "\n",
        "$$\n",
        "PE(pos,2i)   = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$  \n",
        "\n",
        "$$\n",
        "PE(pos,2i+1) = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Residual Connection + Layer Normalization  \n",
        "\n",
        "Each sublayer applies residuals and normalization:  \n",
        "\n",
        "$$\n",
        "\\text{LayerOutput}(x) = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Training Objective (Cross-Entropy with Label Smoothing)  \n",
        "\n",
        "The model maximizes likelihood of target sequence:  \n",
        "\n",
        "$$\n",
        "L(\\theta) = - \\sum_{t=1}^T \\log P_\\theta(y_t \\mid y_{<t}, x)\n",
        "$$  \n",
        "\n",
        "With label smoothing ($\\varepsilon = 0.1$):  \n",
        "\n",
        "$$\n",
        "q(k) =\n",
        "\\begin{cases}\n",
        "1 - \\varepsilon + \\frac{\\varepsilon}{K}, & \\text{if } k = y_t \\\\\n",
        "\\frac{\\varepsilon}{K}, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$  \n",
        "\n",
        "- $K$: vocabulary size.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Learning Rate Schedule  \n",
        "\n",
        "Custom schedule with warm-up steps:  \n",
        "\n",
        "$$\n",
        "\\text{lrate} = d_{\\text{model}}^{-0.5} \\cdot \\min\\!\\left(\\text{step}^{-0.5}, \\; \\text{step} \\cdot \\text{warmup}^{-1.5}\\right)\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Attention Complexity  \n",
        "\n",
        "For sequence length $n$, embedding dimension $d$:  \n",
        "\n",
        "- **Self-Attention:**  \n",
        "  $$\n",
        "  O(n^2 \\cdot d)\n",
        "  $$  \n",
        "\n",
        "- **Recurrent Models (RNNs):**  \n",
        "  $$\n",
        "  O(n \\cdot d^2)\n",
        "  $$  \n",
        "\n",
        "**Path Length (maximum dependency):**  \n",
        "- Self-attention: $O(1)$  \n",
        "- RNNs: $O(n)$  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WyrM2jr8foih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                 ‚îÇ        Input Tokens      ‚îÇ\n",
        "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                               ‚îÇ\n",
        "                               v\n",
        "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                  ‚îÇ   Token Embeddings       ‚îÇ\n",
        "                  ‚îÇ + Positional Encodings   ‚îÇ\n",
        "                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                ‚îÇ\n",
        "                                v\n",
        "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                 ‚îÇ           Encoder (x N)         ‚îÇ\n",
        "                 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Multi-Head   ‚îÇ  ‚îÇ Feed-     ‚îÇ ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Self-Attn    ‚îÇ‚Üí‚Üí‚îÇ Forward   ‚îÇ ‚îÇ\n",
        "                 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
        "                 ‚îÇ   + Residual + LayerNorm        ‚îÇ\n",
        "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                ‚îÇ\n",
        "                                v\n",
        "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                        ‚îÇ   Context   ‚îÇ\n",
        "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                               ‚îÇ\n",
        "                               v\n",
        "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                 ‚îÇ           Decoder (x N)         ‚îÇ\n",
        "                 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Masked Multi-‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Head Self-   ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Attention    ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
        "                 ‚îÇ         ‚îÇ                       ‚îÇ\n",
        "                 ‚îÇ         v                       ‚îÇ\n",
        "                 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Encoder‚Äì     ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Decoder      ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Attention    ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
        "                 ‚îÇ         ‚îÇ                       ‚îÇ\n",
        "                 ‚îÇ         v                       ‚îÇ\n",
        "                 ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
        "                 ‚îÇ ‚îÇ Feed-Forward ‚îÇ                ‚îÇ\n",
        "                 ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
        "                 ‚îÇ   + Residual + LayerNorm        ‚îÇ\n",
        "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                               ‚îÇ\n",
        "                               v\n",
        "                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                 ‚îÇ    Linear + Softmax      ‚îÇ\n",
        "                 ‚îÇ   (Next-token probs)     ‚îÇ\n",
        "                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```"
      ],
      "metadata": {
        "id": "XS7k8ZJNftiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "OqSfN4_DfNQY",
        "outputId": "ab9a33da-1217-4b70-fb6b-3f6ca712bce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 2.7985\n",
            "Epoch 2, Loss 2.5233\n",
            "Epoch 3, Loss 2.3588\n",
            "Epoch 4, Loss 2.2541\n",
            "Epoch 5, Loss 2.1374\n",
            "Epoch 6, Loss 2.0391\n",
            "Epoch 7, Loss 1.9546\n",
            "Epoch 8, Loss 1.8464\n",
            "Epoch 9, Loss 1.7596\n",
            "Epoch 10, Loss 1.6690\n",
            "Validation Loss: 1.5584, Perplexity: 4.75\n",
            "Generated: attention atis atis atis atis atis atis atis atis attttis at\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Scaled Dot-Product Attention\n",
        "# -------------------------------\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        super().__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, V), attn\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Multi-Head Attention\n",
        "# -------------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.linear_Q = nn.Linear(d_model, d_model)\n",
        "        self.linear_K = nn.Linear(d_model, d_model)\n",
        "        self.linear_V = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "        self.attn = ScaledDotProductAttention(self.d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        def transform(x, linear):\n",
        "            return linear(x).view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
        "        Q, K, V = transform(Q, self.linear_Q), transform(K, self.linear_K), transform(V, self.linear_V)\n",
        "        out, attn = self.attn(Q, K, V, mask)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.d_k)\n",
        "        return self.linear_out(out)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Feed-Forward Network\n",
        "# -------------------------------\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffn(x)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Positional Encoding\n",
        "# -------------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Encoder/Decoder Layers\n",
        "# -------------------------------\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x2 = self.attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(x2))\n",
        "        x2 = self.ffn(x)\n",
        "        return self.norm2(x + self.dropout(x2))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(heads, d_model)\n",
        "        self.enc_attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
        "        x2 = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(x2))\n",
        "        x2 = self.enc_attn(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(x2))\n",
        "        x2 = self.ffn(x)\n",
        "        return self.norm3(x + self.dropout(x2))\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Transformer Model\n",
        "# -------------------------------\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, N=2, heads=4, d_ff=512, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model, max_len)\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff) for _ in range(N)])\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff) for _ in range(N)])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src = self.pos(self.embed(src))\n",
        "        tgt = self.pos(self.embed(tgt))\n",
        "        for layer in self.enc_layers:\n",
        "            src = layer(src, src_mask)\n",
        "        for layer in self.dec_layers:\n",
        "            tgt = layer(tgt, src, src_mask, tgt_mask)\n",
        "        return self.fc_out(tgt)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Toy Dataset (char-level LM)\n",
        "# -------------------------------\n",
        "text = \"attention is all you need \" * 200\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for c,i in stoi.items()}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "seq_len = 20\n",
        "def get_batch(batch_size=32):\n",
        "    ix = torch.randint(len(data)-seq_len-1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Training\n",
        "# -------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(vocab_size).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    x, y = get_batch(64)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    logits = model(x, x)   # teacher forcing (src=tgt for toy LM)\n",
        "    loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "    opt.zero_grad(); loss.backward(); opt.step()\n",
        "    train_losses.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}, Loss {loss.item():.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Evaluation\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x, y = get_batch(64)\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    logits = model(x, x)\n",
        "    val_loss = criterion(logits.view(-1, vocab_size), y.view(-1)).item()\n",
        "    ppl = math.exp(val_loss)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {ppl:.2f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 10. Prediction (Greedy Decode)\n",
        "# -------------------------------\n",
        "def generate(prompt=\"attention \", steps=50):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([encode(prompt)], device=device)\n",
        "    for _ in range(steps):\n",
        "        logits = model(idx, idx)[:, -1, :]\n",
        "        next_id = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(\"Generated:\", generate(\"attention \"))\n",
        "\n",
        "# -------------------------------\n",
        "# 11. Visualization\n",
        "# -------------------------------\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve (Transformer)\")\n",
        "plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Academic Interpretation of Results  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Training Dynamics  \n",
        "\n",
        "- **Training Loss:** Decreased from ~2.80 (Epoch 1) ‚Üí ~1.66 (Epoch 10).  \n",
        "- **Observation:** Smooth decline without oscillations or divergence.  \n",
        "\n",
        "‚û°Ô∏è **Interpretation:** Indicates stable optimization and confirms that the **PyTorch replication of the Transformer architecture** is correctly implemented.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Validation Performance  \n",
        "\n",
        "- **Final Validation Loss:** 1.5584  \n",
        "- **Perplexity (PPL):** ‚âà 4.75  \n",
        "\n",
        "‚û°Ô∏è **Interpretation:**  \n",
        "- On average, the model is about **4.75 times ‚Äúuncertain‚Äù** when predicting the next token.  \n",
        "- Although not competitive with large-scale pretraining results (Vaswani et al., 2017), this is **reasonable performance** for a **toy dataset**, limited vocabulary, and only 10 training epochs.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Sample Generation  \n",
        "\n",
        "**Generated Output:**  \n",
        "\n",
        "attention atis atis atis atis atis atis attttis at\n",
        "\n",
        "\n",
        "‚û°Ô∏è **Interpretation:**  \n",
        "- The model preserved the prefix **‚Äúattention‚Äù** and captured local repetition patterns.  \n",
        "- However, the output exhibits **degeneration (looping tokens)**, a known issue in **small-scale sequence generation**.  \n",
        "- This can often be mitigated by **advanced decoding strategies** such as beam search, top-k sampling, or nucleus (top-p) sampling.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Visualization  \n",
        "\n",
        "- **Loss Curve:** Shows a consistent downward trajectory.  \n",
        "- **Convergence:** Slope flattens after ~8 epochs, suggesting the model is nearing convergence.  \n",
        "- **Generalization:** Validation loss and perplexity track closely with training loss ‚Üí **no significant overfitting detected**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìù Conclusion  \n",
        "\n",
        "This replication demonstrates that even in a constrained setup:  \n",
        "\n",
        "1. The **Transformer architecture** effectively learns sequence dependencies, steadily reducing training and validation loss.  \n",
        "2. **Generated text** reflects structural awareness (prefix preservation, repetition), though **lacks diversity** due to dataset simplicity.  \n",
        "3. Results **qualitatively align** with the findings of *Attention Is All You Need* (Vaswani et al., 2017), albeit on a smaller scale.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "c1cEB7yvg3HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìë Related Works: *Attention Is All You Need*\n",
        "\n",
        "| Title | Authors | Year |\n",
        "|-------|----------|------|\n",
        "| Long Short-Term Memory | Sepp Hochreiter, J√ºrgen Schmidhuber | 1997 |\n",
        "| ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) | Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton | 2012 |\n",
        "| On the Properties of Neural Machine Translation: Encoder‚ÄìDecoder Approaches | Kyunghyun Cho, B. van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio | 2014 |\n",
        "| Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation | Kyunghyun Cho, B. van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, et al. | 2014 |\n",
        "| Sequence to Sequence Learning with Neural Networks | Ilya Sutskever, Oriol Vinyals, Quoc V. Le | 2014 |\n",
        "| Neural Machine Translation by Jointly Learning to Align and Translate | Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio | 2014 |\n",
        "| Effective Approaches to Attention-based Neural Machine Translation | Thang Luong, Hieu Pham, Christopher D. Manning | 2015 |\n",
        "| Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | Sergey Ioffe, Christian Szegedy | 2015 |\n",
        "| Rethinking the Inception Architecture for Computer Vision | Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna | 2015 |\n",
        "| Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG) | K. Simonyan, Andrew Zisserman | 2014 |\n",
        "| Neural Machine Translation in Linear Time | Nal Kalchbrenner, L. Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu | 2016 |\n",
        "| Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation | Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. | 2016 |\n",
        "| Densely Connected Convolutional Networks (DenseNet) | Gao Huang, Zhuang Liu, Kilian Q. Weinberger | 2016 |\n",
        "| Layer Normalization | Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton | 2016 |\n",
        "| Convolutional Sequence to Sequence Learning | Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin | 2017 |\n",
        "| Weighted Transformer Network for Machine Translation | Karim Ahmed, N. Keskar, R. Socher | 2017 |\n",
        "| One Model To Learn Them All | ≈Åukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, et al. | 2017 |\n",
        "| Adam: A Method for Stochastic Optimization | Diederik P. Kingma, Jimmy Ba | 2014 |\n",
        "| Improving Language Understanding by Generative Pre-Training (GPT-1) | Alec Radford, Karthik Narasimhan | 2018 |\n",
        "| GLUE: A Multi-Task Benchmark and Analysis Platform for NLU | Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman | 2018 |\n",
        "| ELMo: Deep Contextualized Word Representations | Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, et al. | 2018 |\n",
        "| Decoupled Weight Decay Regularization | Ilya Loshchilov, Frank Hutter | 2017 |\n",
        "| DTMT: A Novel Deep Transition Architecture for Neural Machine Translation | Fandong Meng, Jinchao Zhang | 2018 |\n",
        "| The Best of Both Worlds: Combining Advances in Neural Machine Translation | M. Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster | 2018 |\n",
        "| Layer-Wise Coordination between Encoder and Decoder for NMT | Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, et al. | 2018 |\n",
        "| Tensor2Tensor for Neural Machine Translation | Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran√ßois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, ≈Åukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, Jakob Uszkoreit | 2018 |\n",
        "| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova | 2019 |\n",
        "| ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut | 2019 |\n",
        "| SpanBERT: Improving Pre-training by Representing and Predicting Spans | Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy | 2019 |\n",
        "| Universal Transformers | Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, ≈Åukasz Kaiser | 2019 |\n",
        "| Language Models are Few-Shot Learners (GPT-3) | Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, et al. | 2020 |\n",
        "| An Image is Worth 16x16 Words (ViT) | Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, et al. | 2020 |\n",
        "| Training Data-efficient Image Transformers & Distillation through Attention | Hugo Touvron, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv√© J√©gou | 2020 |\n",
        "| End-to-End Object Detection with Transformers (DETR) | Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko | 2020 |\n",
        "| Transformer++ | Prakhar Thapak, P. Hore | 2020 |\n",
        "| Pyramid Vision Transformer (PVT) | Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao | 2021 |\n",
        "| Transforming Human-Machine Interaction: Generative AI Virtual Assistant | Gotru Jeevan, Babu S. Sarifanithina, Dekshith Reddy, Dr. Sanjay, et al. | 2021 |\n",
        "| LLaMA: Open and Efficient Foundation Language Models | Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, et al. | 2023 |\n"
      ],
      "metadata": {
        "id": "EDQ6PYsrgzE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîé Academic Comparison Between *Attention Is All You Need* (Vaswani et al., 2017) and *Attention Is Not All You Need Anymore* (Chen, 2023)  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Problem Formulation  \n",
        "\n",
        "- **Vaswani et al. (2017):**  \n",
        "  Tackled inefficiency of RNNs/CNNs for sequence transduction. Proposed removing recurrence and convolution entirely, replacing them with **multi-head self-attention**.  \n",
        "\n",
        "- **Chen (2023):**  \n",
        "  Reformulated sequence prediction as **variable-length discrete-time Markov chains**. Argued self-attention is **not indispensable**, introducing Extractors as alternatives.  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *attention as solution*. Chen ‚Üí *critique of attention‚Äôs quadratic complexity* and proposal of replacements.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Model Architecture  \n",
        "\n",
        "- **Vaswani et al.:**  \n",
        "  Encoder‚Äìdecoder with **6 stacked layers**. Each = self-attention + FFN + residual + layernorm. Used **sinusoidal positional encodings**.  \n",
        "\n",
        "- **Chen:**  \n",
        "  Retains Transformer skeleton but **replaces self-attention with Extractors**:  \n",
        "  - SHE (Super High-Performance Extractor)  \n",
        "  - HE (Higher-Performance Extractor)  \n",
        "  - WE (Worthwhile Extractor)  \n",
        "  - ME (Minimalist Extractor)  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *introduced attention*. Chen ‚Üí *replaces attention*.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Mathematical Core  \n",
        "\n",
        "- **Vaswani:**  \n",
        "  $$\n",
        "  \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
        "  $$  \n",
        "  Complexity: $O(n^2 d)$.  \n",
        "\n",
        "- **Chen:**  \n",
        "  Markov chain formulation:  \n",
        "  $$\n",
        "  P(S_{t+1}=s_{t+1}\\mid S_t,\\dots,S_1)\n",
        "  $$  \n",
        "  Extractors use **summations + learned weights** instead of pairwise dot-products.  \n",
        "  Complexity: **SHE $O(nd^2)$, HE/WE/ME $O(nd)$**.  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *formalized attention math*. Chen ‚Üí *Markov-based reformulation with new operators*.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training & Data  \n",
        "\n",
        "- **Vaswani:**  \n",
        "  - Task: WMT‚Äô14 EN‚ÄìDE, EN‚ÄìFR.  \n",
        "  - Optimizer: Adam + warmup.  \n",
        "  - Data: 32k‚Äì37k vocab.  \n",
        "  - Hardware: 8√ó P100 GPUs.  \n",
        "  - Metric: BLEU.  \n",
        "\n",
        "- **Chen:**  \n",
        "  - Task: Text generation (children‚Äôs books, 8.4M tokens, 5k vocab).  \n",
        "  - Optimizer: AdamW.  \n",
        "  - Hardware: RTX 4050 (6 GB).  \n",
        "  - Metric: Perplexity.  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *large-scale MT*. Chen ‚Üí *smaller-scale generation*.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Experimental Results  \n",
        "\n",
        "- **Vaswani:**  \n",
        "  - EN‚ÄìDE: **28.4 BLEU**.  \n",
        "  - EN‚ÄìFR: **41.8 BLEU**.  \n",
        "  - Training: 12h (base), 3.5d (big).  \n",
        "  - Outperformed RNN/CNN baselines.  \n",
        "\n",
        "- **Chen:**  \n",
        "  - SHE > 32-head self-attention.  \n",
        "  - HE competitive with multi-head self-attention.  \n",
        "  - WE ‚âà multi-head attention.  \n",
        "  - ME ‚âà 1-head attention (lightweight).  \n",
        "  - SHE outputs judged more coherent.  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *benchmarked against RNN/CNN*. Chen ‚Üí *benchmarked against Transformer‚Äôs own attention*.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Computational Complexity  \n",
        "\n",
        "- **Vaswani:**  \n",
        "  - Self-attention: $O(n^2 d)$.  \n",
        "  - Advantage: parallelizable, constant path length.  \n",
        "\n",
        "- **Chen:**  \n",
        "  - SHE: heavier but more accurate.  \n",
        "  - HE/WE: balanced.  \n",
        "  - ME: ultra-light.  \n",
        "  - All Extractors ‚Üí shorter critical paths.  \n",
        "\n",
        "üëâ **Key Difference:** Vaswani ‚Üí *emphasized parallelism*. Chen ‚Üí *emphasized cost reduction*.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Philosophical Stance  \n",
        "\n",
        "- **Vaswani:** ‚ÄúAttention is all you need.‚Äù  \n",
        "- **Chen:** ‚ÄúAttention is not all you need anymore.‚Äù  \n",
        "\n",
        "üëâ Vaswani = proposing attention as **the paradigm**.  \n",
        "üëâ Chen = challenging attention‚Äôs dominance, proposing **post-attention architectures**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìä Comparative Table  \n",
        "\n",
        "| Dimension       | Vaswani et al. (2017)        | Chen (2023)                     |\n",
        "|-----------------|------------------------------|----------------------------------|\n",
        "| **Task**        | Machine translation          | Text generation                  |\n",
        "| **Novelty**     | Self-attention replaces RNN/CNN | Extractors replace self-attention |\n",
        "| **Math**        | Scaled dot-product attention | Markov chain + Extractors        |\n",
        "| **Complexity**  | $O(n^2 d)$                   | SHE (heavier), HE/WE/ME (lighter)|\n",
        "| **Data**        | WMT EN‚ÄìDE, EN‚ÄìFR             | Gutenberg children‚Äôs books       |\n",
        "| **Metrics**     | BLEU                         | Perplexity                       |\n",
        "| **Hardware**    | 8√ó P100 GPUs                 | 1√ó RTX 4050                      |\n",
        "| **Results**     | SOTA BLEU                    | Extractors rival/beat attention  |\n",
        "| **Stance**      | Attention is enough          | Attention is insufficient        |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Conclusion (Academic Tone)  \n",
        "\n",
        "- **Vaswani et al. (2017):** revolutionized sequence modeling by discarding recurrence and convolution in favor of **multi-head self-attention**, achieving state-of-the-art translation with superior parallelism.  \n",
        "- **Chen (2023):** critiques the **quadratic cost** of attention, introducing **Extractor sublayers** as efficient alternatives within the Transformer skeleton, framed by **Markov chain reasoning**.  \n",
        "\n",
        "üëâ Together, the two works represent **complementary stages**:  \n",
        "- *First*: establishing attention as foundational.  \n",
        "- *Second*: questioning its exclusivity and opening new design space for **post-attention architectures**.  \n"
      ],
      "metadata": {
        "id": "IS9Hm5BvhTNv"
      }
    }
  ]
}