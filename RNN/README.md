# ğŸ”„ Recurrent Neural Networks (RNN) â€” Replication Labs

Welcome to the **Recurrent Neural Networks (RNN) Collection** by **Programming Ocean Academy** ğŸŒŠ  
A curated repository of **hands-on reproductions** of classic and modern RNN architectures â€” from the vanilla RNNs to attention-powered neural translation models.

---

## ğŸ§  Whatâ€™s Inside?

Each notebook in this collection is crafted to:

- Deliver **pedagogical clarity**: *math â†’ code â†’ visualization*
- Benchmark **Vanilla RNN**, **LSTM**, **GRU**, and attention-based models
- Trace the **evolution** of recurrent models in NLP and beyond
- Serve as a **didactic resource** for students, researchers, and engineers alike

---

## ğŸ“š The RNN Landscape

Recurrent Neural Networks (RNNs) have historically been the backbone of sequence modeling, enabling progress in:

- Natural Language Processing  
- Time Series Prediction  
- Speech Recognition  
- Machine Translation  

However, challenges like **vanishing gradients** sparked innovations:

- ğŸ§¬ **LSTM** â€” Hochreiter & Schmidhuber (1997)  
- ğŸ”„ **GRU** â€” Cho et al. (2014)  
- ğŸ¯ **Attention & Seq2Seq** â€” Bahdanau et al., Sutskever et al. (2014)  
- ğŸ§­ **Neural Turing Machines**, **Pointer Networks**, and **ConvS2S**

This collection rebuilds these breakthroughs step-by-step with a strong link to the **original papers**.

---

## ğŸ“‚ Repository Structure

### ğŸ”¬ Replication Labs â€” Notebook Summaries

- `Empirical_Replication_of_Gated_Recurrent_Neural_Networks_Comparative_Evaluation.ipynb`  
  ğŸ“Š Replicates Chung et al. (2014) â€” RNN vs LSTM vs GRU on sequential tasks  

- `LSTM-SeqRepNet_A_Didactic_Reproduction_of_Hochreiter_&_Schmidhuber's_LSTM.ipynb`  
  ğŸ§  Reproduces the foundational LSTM model using PyTorch  

- `RNN-TanhEvalNet_A_Reimplementation_of_Vanilla_Recurrent_Networks_for_Empirical_Baselines.ipynb`  
  ğŸ”§ Minimal RNN baseline for controlled experiments  

- `Modeling_Long_Term_Dependencies_with_LSTM_A_PyTorch_Replication.ipynb`  
  ğŸ” Explores LSTM's capacity for long-term memory  

- `Neural_Horizons_A_PyTorch_Replication_of_Recurrent_Continuous_Translation.ipynb`  
  ğŸŒ Pre-attention encoderâ€“decoder translation models  

- `Neural_Phrase_to_Phrase_Translation_A_PyTorch_Replication_of_RNN_Encoder-Decoder.ipynb`  
  âœ¨ Phrase-level encoderâ€“decoder sequence modeling  

- `ReproLuongNMT_A_PyTorch_Replication_of_Attention-Based_NMT.ipynb`  
  ğŸ¯ Luong et al. (2015) â€” Local vs Global Attention  

- `ReproSeq2SeqAttn_A_PyTorch_Replication_of_Bahdanau_Attention.ipynb`  
  ğŸ¯ Additive attention (Bahdanau et al.) â€” align and translate  

- `Convolutional_Sequence_to_Sequence_Translation_A_PyTorch_Replication_of_Fairseq_ConvS2S.ipynb`  
  ğŸ§± ConvS2S â€” sequence modeling without recurrence  

- `Coverage_Augmented_Neural_Machine_Translation_A_PyTorch_Replication.ipynb`  
  ğŸ›¡ï¸ Coverage attention for reducing over-translation (Tu et al.)  

- `Hands_On_Replication_of_Neural_Turing_Machines_Copy_Task_in_PyTorch.ipynb`  
  ğŸ§  Differentiable memory with Neural Turing Machines  

- `Hands_On_Replication_of_Pointer_Networks_Learning_to_Point_with_Attention.ipynb`  
  ğŸ“ Pointer Networks â€” solving combinatorial problems via RNNs  

---

## ğŸ”— Key Papers Reproduced

- Hochreiter & Schmidhuber (1997) â€” *Long Short-Term Memory*  
- Pascanu et al. (2012) â€” *On the Difficulty of Training RNNs*  
- Sutskever et al. (2014) â€” *Seq2Seq Learning with Neural Networks*  
- Bahdanau et al. (2014) â€” *Attention-Based NMT*  
- Xu et al. (2015) â€” *Show, Attend and Tell*  
- Chung et al. (2014â€“2015) â€” *GRU and Gated Feedback RNNs*  
- Gehring et al. (2017) â€” *ConvS2S*  
- Tu et al. (2016) â€” *Coverage Attention in NMT*  
- Graves et al. (2014) â€” *Neural Turing Machines*  
- Vinyals et al. (2015) â€” *Pointer Networks*

---

## ğŸ¯ Goals of This Collection

- ğŸ§­ Build a **living roadmap** of RNN research via PyTorch implementations  
- ğŸ§  Bridge **theory â†” equations â†” code â†” results**  
- ğŸ“ Provide **teaching labs** for instructors and self-learners  
- ğŸ”¬ Enable **reproducible benchmarks** for research

---

## ğŸ§¾ References

- Chung, J. et al. (2014). *Empirical Evaluation of Gated RNNs on Sequence Modeling*. arXiv:1412.3555  
- Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation  
- Bahdanau, D. et al. (2014). *Neural Machine Translation by Jointly Learning to Align and Translate*  
- Sutskever, I. et al. (2014). *Sequence to Sequence Learning with Neural Networks*  
- Xu, K. et al. (2015). *Show, Attend and Tell*  

---

âœ¨ **From early RNNs to attention-powered translation, this lab brings the full arc of sequential modeling to your fingertips.**  
Letâ€™s replicate, learn, and innovate together! ğŸ”¨ğŸ¤–ğŸ”§

