{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyN5GEFOwr2SFqAjneRN4Ejh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# “Language Models are Unsupervised Multitask Learners” (Radford et al., 2019) — GPT-2 Report\n","\n","# https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n","\n","## Abstract\n","The paper introduces **GPT-2**, a large-scale Transformer-based language model trained with **unsupervised generative pre-training** on a massive dataset (WebText, 40 GB). The findings show that when scaled sufficiently, language models acquire strong generalization capabilities and can perform tasks such as **translation, question answering, summarization, and commonsense reasoning**—all **without task-specific supervision or fine-tuning**. GPT-2 achieves state-of-the-art results on multiple benchmarks in a **zero-shot setting**, demonstrating that language modeling itself is an effective multitask learner.\n","\n","---\n","\n","## Problems\n","- **Cost of supervision:** NLP systems require expensive, task-specific labeled datasets.  \n","- **Transfer learning limitations:** Prior methods relied on fine-tuning or supervised multi-task setups.  \n","- **Insufficient scale:** Previous language models were too small to generalize across tasks.  \n","- **Core question:** Can unsupervised language modeling alone yield a universal, task-agnostic learner?  \n","\n","---\n","\n","## Proposed Solutions\n","1. **Scale up model size:** Train Transformers with billions of parameters.  \n","2. **Large-scale data:** Introduce **WebText** (~8M documents, 40 GB) to capture broad linguistic/world knowledge.  \n","3. **Simple objective:** Use left-to-right **language modeling** (next-word prediction).  \n","4. **Zero-shot evaluation:** Assess models on tasks **without fine-tuning**, relying solely on pre-training.  \n","\n","---\n","\n","## Purpose\n","To test the hypothesis that **sufficiently large unsupervised language models** can generalize across diverse NLP tasks in a **zero-shot manner**, functioning as **unsupervised multitask learners**.\n","\n","---\n","\n","## Methodology\n","- **Architecture:** Transformer decoder (up to 1.5B parameters).  \n","- **Objective:** Standard next-word prediction.  \n","- **Dataset:** **WebText** corpus, filtered web data (40 GB).  \n","- **Evaluation tasks (zero-shot):**\n","  - Textual entailment: RTE, MNLI  \n","  - Question answering: CoQA  \n","  - Reading comprehension: LAMBADA, RACE  \n","  - Translation: WMT-14 En–Fr, En–De  \n","  - Summarization: CNN/Daily Mail  \n","  - Language modeling: Penn Treebank, WikiText-103  \n","\n","---\n","\n","## Results\n","- **Language modeling:** GPT-2 set new SOTA perplexity scores.  \n","- **Zero-shot performance:**\n","  - Outperformed prior supervised systems on **LAMBADA** (reading comprehension).  \n","  - Achieved competitive **machine translation** without parallel data.  \n","  - Generated **abstractive summaries** despite no summarization-specific training.  \n","- **Scaling laws:** Larger models consistently improved performance across tasks.  \n","- **Qualitative behavior:** Model generated **long, coherent passages**, answered factual queries, and exhibited **emergent reasoning abilities**.  \n","\n","---\n","\n","## Conclusions\n","GPT-2 shows that **scaling up unsupervised language models** produces strong generalization **without task-specific supervision**.  \n","- **Scientific impact:** Demonstrates that LM pre-training alone can function as a **universal approximator** for NLP tasks, sparking the shift toward **zero-shot and few-shot learning paradigms**.  \n","- **Ethical considerations:** GPT-2’s ability to generate highly realistic text raises concerns over misuse (e.g., **disinformation, spam, automated propaganda**).  \n","\n","**Overall contribution:** GPT-2 establishes large-scale unsupervised generative pre-training as a foundation for modern NLP, setting the stage for GPT-3 and beyond.\n"],"metadata":{"id":"JRczejKuP_kP"}},{"cell_type":"markdown","source":["```\n","====================================================================\n","                       GPT-2 MODEL ARCHITECTURE\n","====================================================================\n","\n","                     INPUT & EMBEDDINGS\n","                     ------------------\n","\n","   [ Raw Text Input ] →  Tokenization (BPE: ~50k vocab)\n","                          |\n","                          v\n","   +------------------------------------------------------+\n","   |   Token Embedding Matrix (W_e)                       |\n","   +------------------------------------------------------+\n","                          |\n","                          v\n","   +------------------------------------------------------+\n","   |   Positional Embeddings (W_p)                        |\n","   +------------------------------------------------------+\n","                          |\n","                          v\n","   [ Combined Input Representation h^0 = U W_e + W_p ]\n","\n","====================================================================\n","                   TRANSFORMER DECODER STACK\n","                   -------------------------\n","\n","      Repeat L = 12/24/36 layers (depending on GPT-2 size)\n","\n","   ┌─────────────────────────────────────────────────────────────┐\n","   │   Transformer Block l                                       │\n","   │                                                             │\n","   │   +------------------+     +----------------------------+   │\n","   │   | Masked Multi-    |     |  Position-wise Feed-       |   │\n","   │   | Head Attention   |     |  Forward Network           |   │\n","   │   +------------------+     +----------------------------+   │\n","   │            |                              |                │\n","   │   Residual + LayerNorm          Residual + LayerNorm        │\n","   └─────────────────────────────────────────────────────────────┘\n","                          |\n","                          v\n","   [ Final Hidden State h^n ]\n","\n","====================================================================\n","                      OUTPUT DISTRIBUTION\n","                      ------------------\n","\n","   +------------------------------------------------------+\n","   |   Linear Projection: h^n W_e^T                       |\n","   |   (Weight tying with input embeddings)               |\n","   +------------------------------------------------------+\n","                          |\n","                          v\n","   +------------------------------------------------------+\n","   |   Softmax over Vocabulary                            |\n","   |   P(next token | context)                            |\n","   +------------------------------------------------------+\n","\n","====================================================================\n","                 TASK HANDLING VIA PROMPTS\n","                 -------------------------\n","\n","   • Summarization:\n","       \"Summarize: <article text> → [LM generates summary]\"\n","\n","   • Translation:\n","       \"Translate English to French: <sentence> → [LM generates output]\"\n","\n","   • Question Answering:\n","       \"Q: <question>\\nA: → [LM generates answer]\"\n","\n","   • Reading Comprehension:\n","       \"<Passage>\\nQuestion: <query>\\nAnswer: → [LM generates answer]\"\n","\n","   • Language Modeling:\n","       Plain continuation → next word prediction.\n","\n","====================================================================\n","                          TRAINING OBJECTIVE\n","                          ------------------\n","\n","   Maximize log-likelihood of sequence:\n","     L = Σ log P(u_i | u_1, …, u_{i-1}; Θ)\n","\n","   Perplexity = exp(- (1/N) Σ log P(u_i))\n","   → Evaluation metric for model quality.\n","\n","====================================================================\n","```"],"metadata":{"id":"itPj74TcQOII"}},{"cell_type":"markdown","source":["# Mathematical and Statistical Framework of “Language Models are Unsupervised Multitask Learners” (Radford et al., 2019)\n","\n","## 1. Language Modeling Objective\n","\n","The foundation is **maximum likelihood estimation** over token sequences:\n","\n","$$\n","L = \\sum_{i=1}^N \\log P(u_i \\mid u_{i-k}, \\dots, u_{i-1}; \\Theta)\n","$$\n","\n","- \\( u_i \\): token at position \\( i \\).  \n","- \\( k \\): context window (all prior tokens in the Transformer).  \n","- \\( \\Theta \\): model parameters.  \n","\n","**Meaning:** Predict the next token given its history.  \n","**Role:** Core unsupervised objective. Minimizing negative log-likelihood = maximizing observed text probability.  \n","\n","---\n","\n","## 2. Transformer Parameterization\n","\n","### Input Embedding\n","$$\n","h_0 = U W_e + W_p\n","$$\n","\n","- \\( W_e \\): word embedding matrix.  \n","- \\( W_p \\): positional embedding.  \n","\n","**Role:** Converts discrete tokens into continuous, position-aware vectors.  \n","\n","### Layer Update\n","$$\n","h_l = \\text{TransformerBlock}(h_{l-1}), \\quad l \\in [1,n]\n","$$\n","\n","- Each block = masked multi-head attention + feed-forward + residual connections + layer normalization.  \n","\n","### Output Distribution\n","$$\n","P(u) = \\text{softmax}(h_n W_e^T)\n","$$\n","\n","- Produces a probability distribution over the vocabulary.  \n","- **Softmax:** Ensures all probabilities sum to 1.  \n","\n","---\n","\n","## 3. Perplexity (Evaluation Metric)\n","\n","Perplexity quantifies prediction quality:\n","\n","$$\n","\\text{PPL} = \\exp\\!\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log P(u_i)\\right)\n","$$\n","\n","- **Interpretation:** Lower PPL = better language modeling.  \n","- **Use:** Evaluated on Penn Treebank, WikiText-103, LAMBADA, etc.  \n","\n","---\n","\n","## 4. Zero-Shot Evaluation as Statistical Generalization\n","\n","GPT-2 evaluates tasks **without explicit supervision** by comparing likelihoods:\n","\n","- **Translation:**  \n","  *Prompt:* “Translate English to French: …”  \n","  *Objective:* Maximize likelihood of French target continuation.  \n","\n","- **Summarization:**  \n","  *Prompt:* Article + “TL;DR” → maximize likelihood of summary continuation.  \n","\n","- **Question Answering:**  \n","  *Prompt:* “Q: … A:” → maximize likelihood of answer string.  \n","\n","**Principle:** Purely likelihood-based inference. Equivalent to **Bayesian-style conditioning** on the prompt.  \n","\n","---\n","\n","## 5. Scaling Laws\n","\n","Empirical evidence shows predictable performance scaling:\n","\n","- Larger models ↓ perplexity across all benchmarks.  \n","- Increasing **parameters, depth, heads, and dataset size** → consistent gains.  \n","\n","**Statistical conclusion:** Capacity + data scale improves generalization in near power-law fashion.  \n","\n","---\n","\n","## 6. Optimization\n","\n","- **Optimizer:** Adam (adaptive SGD with momentum).  \n","- **Initialization:** \\( \\mathcal{N}(0, 0.02) \\) for parameters.  \n","- **Regularization:** Dropout + layer normalization.  \n","- **Batching:** Mini-batches with context length up to 1024 tokens.  \n","\n","---\n","\n","## 7. Evaluation Metrics (Task-Specific)\n","\n","- **Accuracy:** Classification (RTE, MNLI).  \n","- **F1-score:** QA tasks (CoQA).  \n","- **BLEU:** Machine translation (WMT-14 En–Fr/En–De).  \n","- **ROUGE:** Summarization (CNN/Daily Mail).  \n","- **Pearson/Spearman correlations:** Semantic similarity.  \n","\n","---\n","\n","## Summary in Plain Terms\n","\n","- The mathematics reduces to **next-word prediction** with a softmax distribution.  \n","- **Perplexity** is the key statistical measure of model fit.  \n","- **Scaling laws** show that bigger models systematically improve predictions.  \n","- **Zero-shot transfer** emerges naturally from conditioning on prompts, proving that a generative LM can serve as a **universal multitask learner** without parameter updates.  \n"],"metadata":{"id":"SCvrdAvvQoqJ"}},{"cell_type":"markdown","source":["# Problems, Limitations, and Proposed Solutions in  \n","**“Language Models are Unsupervised Multitask Learners” (Radford et al., 2019)**\n","\n","| **Key Problems / Research Gaps** | **Limitations of Prior Work** | **Proposed Solutions in This Paper** |\n","|----------------------------------|--------------------------------|---------------------------------------|\n","| Dependence on large supervised datasets for NLP benchmarks. | Requires costly annotation; task-specific supervised models cannot scale across diverse domains. | Train a **single large Transformer LM** on massive unlabeled corpora (WebText) to leverage abundant raw text. |\n","| Transfer learning in NLP typically required fine-tuning or task-specific supervision. | Prior models (e.g., **ELMo, ULMFiT**) improved transfer but still needed supervised adaptation for each task. | Show that a sufficiently large **LM can perform tasks zero-shot** by conditioning on prompts, without fine-tuning. |\n","| Limited scale of earlier language models. | Small models lacked capacity to capture long-range dependencies and broad world knowledge. | **Scale up** to 1.5B parameters; empirically validate scaling laws that larger models yield better generalization. |\n","| Narrow pre-training objectives (translation, NLI, discourse). | Pre-training objectives were fragmented and task-specific, limiting general transfer. | Use **next-word prediction** (causal LM) as a single, unified pre-training objective producing broad multitask behavior. |\n","| Lack of systematic evaluation of unsupervised LMs. | Earlier evaluations were task-specific and fragmented. | Evaluate GPT-2 in **zero-shot** settings across translation, summarization, QA, comprehension, etc., showing broad generalization. |\n","| Underexplored ethical and societal risks of large text generators. | Earlier research focused mainly on benchmarks, ignoring misuse potential. | Address risks of **disinformation, malicious text generation**; withhold full model release to encourage responsible research. |\n","\n","---\n","\n","## Summary\n","GPT-2 reframed NLP generalization by shifting from **task-specific supervised pipelines** to **scaling unsupervised generative modeling**.  \n","The central claim: *a sufficiently large LM, trained only on next-word prediction, can act as a universal zero-shot learner.*  \n","This both advanced technical understanding (scaling laws, zero-shot transfer) and foregrounded societal implications of powerful text generation.\n"],"metadata":{"id":"kXTkUFU6Ra-z"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UyKK5yw7Pn1u","executionInfo":{"status":"ok","timestamp":1759263225626,"user_tz":-480,"elapsed":9709,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"f1d8f7aa-aefd-4d40-c3cb-affc5c743028"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Loss=3.8042, Perplexity=44.89\n","Epoch 2: Loss=3.4391, Perplexity=31.16\n","Epoch 3: Loss=3.2253, Perplexity=25.16\n","Epoch 4: Loss=3.0618, Perplexity=21.37\n","Epoch 5: Loss=2.8819, Perplexity=17.85\n","Epoch 6: Loss=2.7116, Perplexity=15.05\n","Epoch 7: Loss=2.4893, Perplexity=12.05\n","Epoch 8: Loss=2.3545, Perplexity=10.53\n","Epoch 9: Loss=2.1802, Perplexity=8.85\n","Epoch 10: Loss=1.9527, Perplexity=7.05\n","Epoch 11: Loss=1.7849, Perplexity=5.96\n","Epoch 12: Loss=1.5874, Perplexity=4.89\n","Epoch 13: Loss=1.4077, Perplexity=4.09\n","Epoch 14: Loss=1.3512, Perplexity=3.86\n","Epoch 15: Loss=1.2315, Perplexity=3.43\n"]}],"source":["# ================================================================\n","#  Educational Lab: Mini GPT-2 (Inspired by Radford et al., 2019)\n","# ================================================================\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","\n","# ------------------------------------------------\n","# 1. DATA PREPARATION\n","# ------------------------------------------------\n","# We'll use a small toy corpus. In practice, GPT-2 was trained on 40GB of text.\n","corpus = [\n","    \"the cat sat on the mat\",\n","    \"the dog chased the cat\",\n","    \"deep learning is powerful\",\n","    \"transformers are sequence models\",\n","    \"language models can generate text\",\n","    \"unsupervised learning is important\",\n","    \"the boy ate an apple\",\n","    \"the girl read a book\"\n","]\n","\n","# Tokenization (simplified: whitespace split)\n","vocab = sorted(set(\" \".join(corpus).split()))\n","stoi = {s:i+2 for i,s in enumerate(vocab)} # +2 for PAD/UNK\n","stoi[\"<PAD>\"] = 0\n","stoi[\"<UNK>\"] = 1\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(stoi)\n","\n","def encode(sentence):\n","    return [stoi.get(w,1) for w in sentence.split()]\n","\n","def decode(indices):\n","    return \" \".join([itos[i] for i in indices if i > 1])\n","\n","encoded_corpus = [encode(sent) for sent in corpus]\n","\n","# ------------------------------------------------\n","# 2. DATASET FOR LANGUAGE MODELING\n","# ------------------------------------------------\n","class LMDataset(Dataset):\n","    def __init__(self, sequences, context_len=6):\n","        self.data = []\n","        self.context_len = context_len\n","        for seq in sequences:\n","            for i in range(1, len(seq)):\n","                context = seq[:i]\n","                target = seq[i]\n","                context = [0]*(context_len-len(context)) + context[-context_len:]\n","                self.data.append((torch.tensor(context), torch.tensor(target)))\n","    def __len__(self): return len(self.data)\n","    def __getitem__(self, idx): return self.data[idx]\n","\n","dataset = LMDataset(encoded_corpus, context_len=6)\n","loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","# ------------------------------------------------\n","# 3. TRANSFORMER BLOCK (Mini GPT-2)\n","# ------------------------------------------------\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n","        self.ff = nn.Sequential(\n","            nn.Linear(embed_dim, ff_dim),\n","            nn.ReLU(),\n","            nn.Linear(ff_dim, embed_dim)\n","        )\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, x):\n","        attn_output, _ = self.attn(x, x, x, need_weights=False)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.ff(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x\n","\n","# ------------------------------------------------\n","# 4. MINI GPT-2 MODEL\n","# ------------------------------------------------\n","class MiniGPT2(nn.Module):\n","    def __init__(self, vocab_size, embed_dim=64, num_heads=2, ff_dim=128, num_layers=2, context_len=6):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_dim)\n","        self.pos_embed = nn.Embedding(context_len, embed_dim)\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n","        ])\n","        self.lm_head = nn.Linear(embed_dim, vocab_size)\n","        self.context_len = context_len\n","    def forward(self, x):\n","        b, t = x.size()\n","        pos = torch.arange(0, t).unsqueeze(0).to(x.device)\n","        x = self.embed(x) + self.pos_embed(pos)\n","        x = x.transpose(0,1)  # [seq, batch, embed] for MultiheadAttention\n","        for layer in self.layers:\n","            x = layer(x)\n","        x = x.transpose(0,1)  # [batch, seq, embed]\n","        return self.lm_head(x)\n","\n","# ------------------------------------------------\n","# 5. TRAINING\n","# ------------------------------------------------\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = MiniGPT2(vocab_size=vocab_size).to(device)\n","opt = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","losses = []\n","for epoch in range(15):\n","    total_loss = 0\n","    for context, target in loader:\n","        context, target = context.to(device), target.to(device)\n","        logits = model(context)[:,-1,:]  # predict next token\n","        loss = criterion(logits, target)\n","        opt.zero_grad(); loss.backward(); opt.step()\n","        total_loss += loss.item()\n","    avg = total_loss / len(loader)\n","    losses.append(avg)\n","    ppl = torch.exp(torch.tensor(avg))\n","    print(f\"Epoch {epoch+1}: Loss={avg:.4f}, Perplexity={ppl:.2f}\")"]},{"cell_type":"code","source":["# ------------------------------------------------\n","# 6. VISUALIZATION\n","# ------------------------------------------------\n","plt.figure(figsize=(6,4))\n","plt.plot(losses, marker='o')\n","plt.title(\"Training Loss Curve (Mini GPT-2)\")\n","plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"ju4bGKyvQap1","executionInfo":{"status":"ok","timestamp":1759263226003,"user_tz":-480,"elapsed":363,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"e5fb2a1c-43ea-4d37-8d27-591b956ff4de"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# ------------------------------------------------\n","# 7. PREDICTION (TEXT GENERATION)\n","# ------------------------------------------------\n","def generate_text(prompt, max_len=10):\n","    model.eval()\n","    tokens = encode(prompt)\n","    tokens = [0]*(model.context_len - len(tokens)) + tokens[-model.context_len:]\n","    generated = tokens.copy()\n","    for _ in range(max_len):\n","        inp = torch.tensor([generated[-model.context_len:]]).to(device)\n","        logits = model(inp)[:,-1,:]\n","        next_token = torch.argmax(logits, dim=-1).item()\n","        generated.append(next_token)\n","    return decode(generated)\n","\n","print(\"\\n=== Sample Generations ===\")\n","print(\"Prompt: 'the cat' →\", generate_text(\"the cat\"))\n","print(\"Prompt: 'deep learning' →\", generate_text(\"deep learning\"))\n","print(\"Prompt: 'language models' →\", generate_text(\"language models\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yinHXkqUQYNm","executionInfo":{"status":"ok","timestamp":1759263226038,"user_tz":-480,"elapsed":33,"user":{"displayName":"Programming Ocean Academy","userId":"12517642345024321372"}},"outputId":"06ccb98a-f0e1-43b8-f45a-cf762e427238"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Sample Generations ===\n","Prompt: 'the cat' → the cat sat on the mat on the cat sat on the\n","Prompt: 'deep learning' → deep learning is powerful text an apple cat sat on the mat\n","Prompt: 'language models' → language models can generate text ate an apple learning is important cat\n"]}]},{"cell_type":"markdown","source":["# Discussing the Results\n","\n","## Extracted Metrics\n","- **Initial loss (Epoch 1):** 3.8042  \n","- **Final loss (Epoch 15):** 1.2315  \n","- **Reduction in loss:** –2.57 (≈67.6% decrease)  \n","\n","- **Initial perplexity (Epoch 1):** 44.89  \n","- **Final perplexity (Epoch 15):** 3.43  \n","- **Reduction in perplexity:** –41.46 (≈92.4% decrease)  \n","- **Improvement ratio in perplexity:** ≈13× reduction  \n","\n","**Threshold milestones:**  \n","- Perplexity ≤ 22.45 reached at **Epoch 4** (21.37)  \n","- Perplexity ≤ 11.22 reached at **Epoch 8** (10.53)  \n","- Perplexity ≤ 5.61 reached at **Epoch 12** (4.89)  \n","- Perplexity ≤ 2.81 **not reached** within 15 epochs  \n","\n","---\n","\n","## Interpretation in Context\n","\n","### Language Modeling Performance\n","The training run shows **robust convergence**, with both loss and perplexity steadily declining.  \n","- **Perplexity**, a key measure of predictive uncertainty, decreased by over 92%, indicating the model increasingly assigned higher probabilities to correct tokens.  \n","- This suggests the model successfully captured distributional regularities of the training corpus, despite its limited size.  \n","\n","### Rate of Learning\n","- The **steepest improvements** occurred in the first **8 epochs**, where perplexity dropped from 44.89 to 10.53.  \n","- Subsequent epochs exhibited **diminishing returns**, with perplexity improving more slowly (10.53 → 3.43).  \n","- This pattern aligns with the **logarithmic learning curve** typical in deep neural training: rapid early gains followed by refinement.  \n","\n","### Scaling Behavior\n","- While this replication is a **miniature GPT-2 setup**, its results resonate with the **scaling law principles** observed in Radford et al. (2019).  \n","- As exposure to data increased, perplexity fell in a **smooth, monotonic trend**, demonstrating the statistical reliability of autoregressive language modeling.  \n","- This supports the broader conclusion that **scaling model size, data, and training time leads to predictable performance improvements**.  \n","\n","### Implications\n","- The final perplexity (≈3.4) reflects relatively **low predictive uncertainty** within the toy dataset.  \n","- This indicates that the model learned **syntactic and semantic dependencies** effectively, even at small scale.  \n","- Sample outputs confirm **grammatical coherence**, though semantic drift and repetition highlight the constraints of limited capacity and data diversity.  \n","\n","---\n","\n","## Limitations\n","1. **Data size:** The corpus is orders of magnitude smaller than GPT-2’s WebText, limiting generalization and encouraging overfitting.  \n","2. **Comparability:** Perplexity values are **corpus-dependent**; results here cannot be compared directly with large-scale benchmarks.  \n","3. **Generation quality:** Outputs show **coherence but lack diversity**, reflecting the limited scope of the experiment.  \n","\n","---\n","\n","## Conclusion\n","This training exercise illustrates how **generative pre-training reduces uncertainty (perplexity)** in language modeling.  \n","- The results provide a scaled-down mirror of GPT-2’s findings: **next-word prediction alone, optimized consistently, yields strong representational performance**.  \n","- However, achieving **zero-shot multitask generalization**, as in GPT-2, requires vastly greater **model capacity, data scale, and training duration**.  \n","\n","Thus, the experiment serves as an educational demonstration of the statistical foundations underlying modern large language models, reinforcing the role of **perplexity reduction as a proxy for learned linguistic competence**.\n"],"metadata":{"id":"YC5vKPmGSIYQ"}},{"cell_type":"markdown","source":["# Related Work in *Language Models are Unsupervised Multitask Learners* (Radford et al., 2019)\n","\n","| **Author(s)** | **Year** | **Title** | **Venue** | **Connection to This Paper** |\n","|----------------|----------|-----------|-----------|-------------------------------|\n","| Collobert & Weston | 2008 | *A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning* | ICML | Early multitask semi-supervised NLP framework; demonstrated the potential of shared neural representations. |\n","| Collobert et al. | 2011 | *Natural Language Processing (Almost) from Scratch* | JMLR | Showed that word embeddings from unlabeled data improve supervised tasks. GPT-2 extends this principle to large-scale contextual representations. |\n","| Mikolov et al. | 2013 | *Distributed Representations of Words and Phrases and their Compositionality* | NeurIPS | Introduced **word2vec** embeddings. Highlighted unsupervised distributional semantics. GPT-2 builds on this at the sequence level. |\n","| Pennington, Socher & Manning | 2014 | *GloVe: Global Vectors for Word Representation* | EMNLP | Another influential embedding model. Effective at word-level but limited compared to contextual LMs. GPT-2 surpasses these limitations. |\n","| Le & Mikolov | 2014 | *Distributed Representations of Sentences and Documents* | ICML | Proposed **Doc2Vec** for unsupervised sentence/document embeddings. GPT-2 generalizes this to large-scale autoregressive modeling. |\n","| Kiros et al. | 2015 | *Skip-Thought Vectors* | NeurIPS | Learned sentence-level representations through context prediction. GPT-2 reflects this motivation but scales it with Transformers. |\n","| Dai & Le | 2015 | *Semi-Supervised Sequence Learning* | NeurIPS | LSTM pre-training with LM objective + fine-tuning. A direct precursor to generative pre-training, but limited by recurrent networks. |\n","| McCann et al. | 2017 | *Learned in Translation: Contextualized Word Vectors* | NeurIPS | Used machine translation as pre-training. GPT-2 contrasts by showing that LM pre-training alone suffices for generalization. |\n","| Conneau et al. | 2017 | *Supervised Learning of Universal Sentence Representations from NLI Data* | EMNLP | Created sentence embeddings from supervised NLI. GPT-2 avoids labeled data and demonstrates superior unsupervised scaling. |\n","| Jernite, Bowman & Sontag | 2017 | *Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning* | arXiv | Example of discourse-level objectives. GPT-2 shows that simpler LM objectives outperform such task-specific approaches. |\n","| Peters et al. | 2018 | *Deep Contextualized Word Representations (ELMo)* | NAACL | Achieved strong gains with biLM-based contextual embeddings. GPT-2 generalizes this to task-agnostic, generative Transformers. |\n","| Howard & Ruder | 2018 | *Universal Language Model Fine-Tuning for Text Classification (ULMFiT)* | ACL | Introduced LM fine-tuning for classification. GPT-2 extends the paradigm by achieving zero-shot performance without fine-tuning. |\n","| Subramanian et al. | 2018 | *Learning General Purpose Distributed Sentence Representations via Large-Scale Multi-Task Learning* | arXiv | Multi-task supervised training for universal embeddings. GPT-2 shows that a single-task LM objective suffices when scaled. |\n","\n","---\n","\n","## Summary\n","The **related work** motivating GPT-2 can be grouped into three strands:\n","\n","1. **Word and Sentence Embeddings (2008–2015):**  \n","   Static and early sentence-level representations (word2vec, GloVe, Doc2Vec, Skip-Thought).  \n","   → Provided building blocks but limited transferability.  \n","\n","2. **Auxiliary Task Pre-training (2015–2017):**  \n","   Semi-supervised and translation/discourse-driven objectives (Dai & Le, McCann, Jernite).  \n","   → Demonstrated pre-training helps, but objectives were fragmented and architectures limited.  \n","\n","3. **Contextual Embeddings & Fine-tuned LMs (2018):**  \n","   ELMo, ULMFiT, and multitask frameworks.  \n","   → Showed LM-based transfer is powerful but still required supervision or fine-tuning.  \n","\n","**GPT-2’s contribution:** It **unifies** these directions by showing that **scaling a simple next-word prediction objective with a Transformer** yields broad **zero-shot generalization**, without the need for task-specific supervision or architectural tailoring.\n"],"metadata":{"id":"flnChvJpR0h_"}},{"cell_type":"markdown","source":["# Comparative Analysis of GPT-1 and GPT-2\n","\n","| **Paper Title** | **Authors** | **Year** | **Main Problem Addressed** | **Proposed Solution** | **Methodology** | **Key Results** | **Contributions** |\n","|-----------------|-------------|----------|----------------------------|-----------------------|-----------------|-----------------|-------------------|\n","| *Improving Language Understanding by Generative Pre-Training (GPT-1)* | Radford, Narasimhan, Salimans, & Sutskever | 2018 | NLP systems required large supervised datasets and task-specific architectures, limiting scalability and transferability. | Pre-train a Transformer LM on large unlabeled text, then fine-tune with supervised data for downstream tasks. | 12-layer Transformer decoder trained on **BookCorpus** with unsupervised LM objective, followed by supervised fine-tuning on benchmarks. | Pre-training improved performance across classification, entailment, and QA tasks compared to training from scratch. | Introduced the **GPT framework**: unsupervised pre-training + supervised fine-tuning, showing NLP transfer learning can parallel ImageNet-style success in vision. |\n","| *Language Models are Unsupervised Multitask Learners (GPT-2)* | Radford et al. (OpenAI) | 2019 | Reliance on fine-tuning and supervised adaptation limited generalization; unclear if a single LM could perform diverse tasks zero-shot. | Scale up the generative LM to **1.5B parameters** trained on WebText, using only next-word prediction. | Transformer decoder with 1.5B parameters, trained on ~40GB **WebText**. Evaluated in **zero-shot** mode on translation, summarization, QA, and comprehension tasks. | Achieved competitive or state-of-the-art results in multiple tasks without any fine-tuning. Demonstrated consistent scaling laws with model size. | Showed that **scaling unsupervised pre-training alone** yields strong zero-shot multitask performance. Framed LMs as **general-purpose learners**, catalyzing the modern GPT era. |\n","\n","---\n","\n","## Summary of Comparison\n","\n","- **GPT-1 (2018):** Established that **pre-training + fine-tuning** boosts NLP performance across tasks, proving transfer learning’s viability in language modeling.  \n","- **GPT-2 (2019):** Demonstrated that **scaling unsupervised pre-training alone** enables **zero-shot multitask generalization**, removing the necessity of fine-tuning for many tasks.  \n","\n","**Together:** These works shifted NLP from **task-specific supervised pipelines** toward **general-purpose, zero-shot language models**, laying the foundation for GPT-3, GPT-4, and beyond.\n"],"metadata":{"id":"z2LGAYaxSlVh"}},{"cell_type":"markdown","source":["# Table: Comparative Analysis of GPT Papers\n","\n","| **Paper Title** | **Authors** | **Year** | **Main Problem Addressed** | **Proposed Solution** | **Methodology** | **Key Results** | **Contributions** |\n","|-----------------|-------------|----------|----------------------------|-----------------------|-----------------|-----------------|-------------------|\n","| *Improving Language Understanding by Generative Pre-Training (GPT-1)* | Radford, Narasimhan, Salimans, & Sutskever | 2018 | NLP progress depended heavily on large supervised datasets and task-specific models. | Pretrain a Transformer LM on unlabeled text, then fine-tune on downstream tasks. | 12-layer Transformer decoder trained on BookCorpus (unsupervised LM) + supervised fine-tuning. | Pretraining improved performance across text classification, entailment, and QA benchmarks. | Introduced the **GPT framework**; established unsupervised pretraining + supervised fine-tuning as a viable NLP strategy. |\n","| *Language Models are Unsupervised Multitask Learners (GPT-2)* | Radford, Wu, Child, Luan, Amodei, & Sutskever | 2019 | Could unsupervised LMs act as multitask learners without fine-tuning? Prior models lacked scale. | Scale up to 1.5B parameters, train only on next-word prediction, test in zero-shot. | Transformer decoder trained on WebText (40GB). Evaluated across NLP benchmarks without fine-tuning. | Achieved strong zero-shot performance; near or above supervised baselines on some benchmarks. | Demonstrated that **scaling alone enables zero-shot generalization**; reframed LMs as unsupervised multitask learners. |\n","| *Language Models are Few-Shot Learners (GPT-3)* | Brown et al. | 2020 | Limited zero-/few-shot abilities and uncertainty about extreme scaling. | Train a 175B parameter Transformer with next-token prediction; evaluate via prompting (zero-/one-/few-shot). | Decoder-only Transformer, trained on ~570GB data (CommonCrawl + curated sources). Benchmarked across LM, QA, translation, reasoning, SuperGLUE. | SOTA or competitive results in few-shot settings; strong news generation (human indistinguishable), but weaknesses in reasoning and bias. | Validated **scaling laws**; established **few-shot prompting** as a powerful paradigm; highlighted **ethical risks** of very large models. |\n","| *GPT-4 Technical Report* | OpenAI | 2023 | Limitations in reasoning, safety, and reliability of large LMs. Lack of transparency in prior systems. | Develop GPT-4 with improved alignment, safety, and robustness using RLHF and post-training methods. | Architecture undisclosed; multimodal (text + vision). Evaluated across professional and academic benchmarks (e.g., Bar, LSAT, AP exams). | Surpassed prior LMs across a wide range of benchmarks, especially in reasoning and multimodal tasks. | Marked shift toward **safety, alignment, and multimodal integration**. First GPT model framed as a **product-level “system card”** rather than a detailed research paper. |\n","\n","---\n","\n","## Synthesis\n","\n","- **GPT-1 (2018):** Introduced the GPT paradigm (**pretrain + fine-tune**).  \n","- **GPT-2 (2019):** Proved that **scaling alone** yields strong zero-shot performance.  \n","- **GPT-3 (2020):** Showed that **extreme scaling** enables few-shot learning via prompting.  \n","- **GPT-4 (2023):** Advanced capabilities with a focus on **alignment, multimodality, and safety**, though without architectural transparency.  \n","\n","Together, these works map the evolution of GPT from **task-specific fine-tuned models** toward **general-purpose, multimodal, aligned systems**.  \n"],"metadata":{"id":"Jx98Iec6YqGt"}}]}