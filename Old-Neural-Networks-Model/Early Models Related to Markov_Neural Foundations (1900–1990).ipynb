{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxth+oos4C9qq73NF843x8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Early Models Related to Markov/Neural Foundations (1900–1990)\n","\n","## 1. Ising Model (1925)\n","- **Domain**: Statistical physics  \n","- **Description**: Models binary spins (+1 / –1) with local interactions.  \n","- **Relevance**: Inspired later probabilistic models in AI (Boltzmann Machines, Hopfield Nets).  \n","- **Connection to Markov**: Markov Random Fields can be seen as generalizations of the Ising model.  \n","\n","---\n","\n","## 2. McCulloch–Pitts Neuron (1943)\n","- **Domain**: Neuroscience-inspired computing  \n","- **Description**: Simplified model of a biological neuron, using binary threshold logic.  \n","- **Relevance**: First formal model of neural networks.  \n","- **Connection**: Like Markov chains, it formalized computation with discrete states.  \n","\n","---\n","\n","## 3. Hebbian Learning Rule (1949)\n","- **Concept**: “Cells that fire together, wire together.”  \n","- **Description**: Strengthens connections between co-activated neurons.  \n","- **Relevance**: Early rule for updating weights in networks.  \n","- **Connection**: Provided a probabilistic, local learning mechanism (like stochastic updates in Markov processes).  \n","\n","---\n","\n","## 4. Perceptron (Rosenblatt, 1958)\n","- **Description**: Linear classifier using weighted sums of inputs + threshold.  \n","- **Relevance**: First trainable machine-learning model inspired by biology.  \n","- **Limitations**: Could not solve XOR (proved by Minsky & Papert, 1969).  \n","- **Connection**: Simple state-to-output mapping, like transitions in a Markov model.  \n","\n","---\n","\n","## 5. Adaline & Delta Rule (Widrow & Hoff, 1960)\n","- **Description**: Linear unit trained with gradient descent (LMS rule).  \n","- **Relevance**: Introduced optimization for weight updates.  \n","- **Connection**: Early continuous-valued extension of discrete Markov-like state models.  \n","\n","---\n","\n","## 6. Hopfield Network (1982)\n","- **Description**: Recurrent neural network with symmetric weights, converging to stable attractor states (energy minimization).  \n","- **Relevance**: Linked neural computation to statistical physics.  \n","- **Connection**: Strongly connected to Ising models and Markov Random Fields.  \n","\n","---\n","\n","## 7. Boltzmann Machine (Hinton & Sejnowski, 1985)\n","- **Description**: Stochastic recurrent network; hidden and visible nodes with probabilistic activations.  \n","- **Relevance**: One of the first deep generative models.  \n","- **Connection**: Uses Markov Chain Monte Carlo (MCMC) sampling → explicit link between Markov processes and neural networks.  \n","\n","---\n","\n","## 8. Backpropagation (Rumelhart, Hinton, Williams, 1986)\n","- **Description**: Algorithm for training multi-layer networks via gradient descent and chain rule.  \n","- **Relevance**: Opened the path for deep learning.  \n","- **Connection**: Generalized learning beyond local updates like Hebbian rules — but still probabilistic at its core.  \n","\n","---\n","\n","## 9. Time-Dependent Models (1986–1989)\n","- **Jordan Networks (1986)** and **Elman Networks (1989)**  \n","- **Description**: Introduced recurrent feedback to model sequences.  \n","- **Relevance**: Early inspiration for later RNNs and sequence models (GRUs, LSTMs, Transformers).  \n","- **Connection**: Like Hidden Markov Models, these modeled sequential/temporal dependencies.  \n","\n","---\n","\n","##  Summary\n","Between 1900 and 1990, the “Markov-like” predecessors in neural and deep learning were:\n","\n","- **Ising Model (1925)** → Probabilistic physics model, inspired energy-based networks.  \n","- **McCulloch–Pitts Neuron (1943)** → First abstract neuron.  \n","- **Hebbian Learning (1949)** → First biologically motivated weight update.  \n","- **Perceptron (1958)** → First practical neural network.  \n","- **Adaline (1960)** → Gradient-based training.  \n","- **Hopfield Networks (1982)** → Energy minimization, attractor dynamics.  \n","- **Boltzmann Machines (1985)** → Stochastic generative model with MCMC.  \n","- **Backpropagation (1986)** → Training multi-layer nets.  \n","- **Elman/Jordan Networks (1986–1989)** → Recurrent sequence modeling.  \n"],"metadata":{"id":"qI2D9l6WQM6U"}}]}