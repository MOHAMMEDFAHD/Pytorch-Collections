{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![citations.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "OAdCojVHyA3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings: A Survey — Structured Summary  \n",
        "*(Based on Felipe Almeida & Geraldo Xexéo)*\n",
        "\n",
        "# https://arxiv.org/abs/1901.09069\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This paper surveys the main approaches for building dense, low-dimensional, distributed word representations known as **word embeddings**. The authors integrate the perspectives of **prediction-based neural models** and **count-based matrix-factorization models**, explaining their mathematical foundations, design choices, and empirical behaviors. The survey emphasizes how embeddings encode syntactic and semantic relations and how these representations significantly improve natural-language processing tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Problems\n",
        "\n",
        "1. Traditional sparse, high-dimensional representations (e.g., one-hot vectors, classical Vector Space Models) fail to capture semantic similarity.  \n",
        "2. Statistical n-gram language models suffer from data sparsity, poor generalization, and dimensional explosion.  \n",
        "3. Early neural language models were slow to train due to expensive softmax computations:  \n",
        "   $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{|V|} e^{z_j}}$$  \n",
        "4. Count-based representations depend heavily on normalization, weighting, and dimensionality reduction choices.  \n",
        "5. Existing literature lacked a unified survey connecting classical techniques, neural models, and modern theoretical results that show equivalence between model families.\n",
        "\n",
        "---\n",
        "\n",
        "## Proposed Solutions\n",
        "\n",
        "### Prediction-based Models  \n",
        "- Learn vector representations jointly with a neural language model.  \n",
        "- Key improvements include hierarchical softmax, negative sampling, subword modeling, and efficient log-linear formulations.  \n",
        "- Examples: NNLM, CBOW, Skip-gram, FastText.\n",
        "\n",
        "### Count-based Models  \n",
        "- Use global co-occurrence statistics from large corpora.  \n",
        "- Employ matrix factorization techniques such as SVD, Hellinger PCA, and CCA.  \n",
        "- GloVe formalizes word-vector learning using co-occurrence ratios:  \n",
        "  $$w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j = \\log X_{ij}$$  \n",
        "  where \\(X_{ij}\\) is the co-occurrence count.\n",
        "\n",
        "### Unified Perspective  \n",
        "- Shows how prediction methods implicitly factorize variants of the **Pointwise Mutual Information (PMI)** matrix:  \n",
        "  $$\\text{PMI}(w,c) = \\log \\frac{p(w,c)}{p(w)p(c)}$$  \n",
        "- Demonstrates that Skip-gram with Negative Sampling (SGNS) ≈ Factorizing a **shifted PMI** matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "To present a clear, structured, and comprehensive review of both **neural** and **statistical** embedding methods, highlight their historical evolution, compare architectures, and offer theoretical insight demonstrating how previously distinct families of models relate mathematically. The goal is to guide researchers toward a unified understanding and identify open directions for future work.\n",
        "\n",
        "---\n",
        "\n",
        "## Methodology\n",
        "\n",
        "- Systematic literature review covering:\n",
        "  - Vector Space Models  \n",
        "  - Statistical and neural language models  \n",
        "  - Prediction-based embeddings (NNLM, CBOW, Skip-gram, FastText)  \n",
        "  - Count-based embeddings (LSA, HAL, COALS, LR-MVL, GloVe)  \n",
        "- Categorization by:\n",
        "  - **Prediction-based**: local context windows, neural network training, sampling strategies.  \n",
        "  - **Count-based**: global matrices, normalization, factorization.  \n",
        "- Comparative tables show differences in architecture, context representation, complexity, and empirical results.  \n",
        "- Mathematical synthesis explaining equivalences (e.g., SGNS and PMI factorization).\n",
        "\n",
        "---\n",
        "\n",
        "## Results\n",
        "\n",
        "### Prediction-Based Models\n",
        "- CBOW and Skip-gram (especially SGNS) produce high-quality embeddings with strong semantic regularities.  \n",
        "- Hierarchical softmax and negative sampling significantly reduce training cost from \\(O(|V|)\\) to near-constant time.  \n",
        "- Subword embedding approaches (FastText) yield better performance for morphologically complex languages.\n",
        "\n",
        "### Count-Based Models\n",
        "- Early methods like LSA and HAL perform moderately well but require improved normalization.  \n",
        "- COALS, CCA, and Hellinger PCA enhance robustness and interpretability.  \n",
        "- GloVe achieves near state-of-the-art performance using a principled log-bilinear model based on co-occurrence ratios.\n",
        "\n",
        "### Cross-Model Insights\n",
        "- Empirical convergence of semantic structures across both families.  \n",
        "- SGNS mathematically interpretable as factorizing a shifted PMI matrix:  \n",
        "  $$\\text{SGNS} \\approx \\text{SVD}(\\text{PMI}(w,c) - \\log k)$$  \n",
        "  where \\(k\\) is the number of negative samples.\n",
        "\n",
        "### Practical Impact\n",
        "- Word embeddings improve a wide range of NLP tasks: POS tagging, chunking, NER, parsing, sentiment analysis, analogy tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "Word embeddings form a foundational component of modern NLP. Their development followed two complementary lines:\n",
        "\n",
        "1. **Prediction-based neural embeddings** that learn representations from local context and optimize predictive objectives.  \n",
        "2. **Count-based matrix embeddings** that extract global corpus statistics and apply factorization.\n",
        "\n",
        "These approaches ultimately converge mathematically and empirically, leading to widely adopted tools such as **Word2Vec**, **GloVe**, and **FastText**. Research continues in areas such as fine-tuning embeddings for downstream tasks, theoretical unification of embedding paradigms, and the construction of higher-level sentence and document representations.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TF_Zvzv8uh7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Summary of Core Concepts from *Word Embeddings: A Survey*  \n",
        "*(Formatted in full Markdown with LaTeX using $$ ... $$ as required)*\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Vector Space Model (VSM)\n",
        "\n",
        "### Mathematical Idea\n",
        "Represent each document or word as a vector in a high-dimensional space.  \n",
        "Each dimension corresponds to a term. Term weights commonly use **TF-IDF**:\n",
        "\n",
        "- Term Frequency:  \n",
        "  $$\\text{TF}(t,d) = \\text{count of term } t \\text{ in document } d$$\n",
        "\n",
        "- Inverse Document Frequency:  \n",
        "  $$\\text{IDF}(t) = \\log\\frac{N}{n_t}$$  \n",
        "  where \\(N\\) = number of documents and \\(n_t\\) = number of documents containing \\(t\\).\n",
        "\n",
        "Similarity measures:\n",
        "$$\\text{cosine}(x,y) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}$$\n",
        "\n",
        "### Role in the Paper  \n",
        "Provides the classical mathematical foundation for representing words as vectors.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Statistical Language Models\n",
        "\n",
        "### 2.1 n-gram Probability Model\n",
        "Probability of a word sequence:\n",
        "$$P(w_1^T) = \\prod_{t=1}^{T} P(w_t \\mid w_1^{t-1})$$\n",
        "\n",
        "Using the Markov **n-gram** assumption:\n",
        "$$P(w_t \\mid w_1^{t-1}) \\approx P(w_t \\mid w_{t-n+1}^{t-1})$$\n",
        "\n",
        "### Role  \n",
        "Demonstrates sparseness and motivates continuous embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "Conditional probability estimate:\n",
        "$$P(w_t \\mid \\text{context}) = \\frac{\\text{count(context + } w_t)}{\\text{count(context)}}$$\n",
        "\n",
        "### Role  \n",
        "Basic estimation method for n-gram models.\n",
        "\n",
        "---\n",
        "\n",
        "### 2.3 Smoothing Methods\n",
        "\n",
        "- **Laplace smoothing**:  \n",
        "  $$P(w_t \\mid c) = \\frac{\\text{count}(c,w_t)+1}{\\text{count}(c)+|V|}$$\n",
        "\n",
        "- **Backoff/Interpolation**:  \n",
        "  Combine probabilities from higher- and lower-order n-grams.\n",
        "\n",
        "### Role  \n",
        "Early solutions for sparsity and data scarcity.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Neural Language Models (NNLMs)\n",
        "\n",
        "### 3.1 Embedding as Projection\n",
        "\n",
        "For a one-hot vector \\(x\\) and weight matrix \\(W\\):\n",
        "$$e = Wx$$  \n",
        "This extracts the embedding for the active index.\n",
        "\n",
        "### Role  \n",
        "Defines the core operation transforming discrete words into continuous vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Softmax Function\n",
        "\n",
        "Prediction distribution:\n",
        "$$P(w \\mid \\text{context}) = \\frac{\\exp(z_w)}{\\sum_{w'} \\exp(z_{w'})}$$\n",
        "\n",
        "### Role  \n",
        "Creates a normalized probability; expensive for large vocabularies.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Partition Function\n",
        "\n",
        "Denominator of softmax:\n",
        "$$Z = \\sum_{w' \\in V} \\exp(z_{w'})$$\n",
        "\n",
        "### Role  \n",
        "Major computational bottleneck → motivates approximations.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Approximation and Acceleration Methods\n",
        "\n",
        "### 4.1 Importance Sampling\n",
        "Approximate gradients by sampling from an auxiliary distribution.\n",
        "\n",
        "### Role  \n",
        "Reduces softmax cost.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.2 Hierarchical Softmax\n",
        "\n",
        "Binary tree over vocabulary.  \n",
        "Word probability = product of binary decisions:  \n",
        "$$P(w) = \\prod_{j=1}^{\\text{path}(w)} P(b_j \\mid \\text{context})$$\n",
        "\n",
        "Complexity improves to:\n",
        "$$O(\\log|V|)$$\n",
        "\n",
        "### Role  \n",
        "Efficient structured softmax.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.3 Noise-Contrastive Estimation (NCE)\n",
        "\n",
        "Transform density estimation into binary classification between true and noise samples.\n",
        "\n",
        "### Role  \n",
        "Avoids computing the partition function.\n",
        "\n",
        "---\n",
        "\n",
        "### 4.4 Negative Sampling (Word2Vec)\n",
        "\n",
        "Sample \\(k\\) negative words for each positive pair.  \n",
        "Train logistic regression to distinguish real vs. noise:\n",
        "\n",
        "$$\\log \\sigma(v_w^\\top v_c) + \\sum_{i=1}^{k} \\log \\sigma(-v_{n_i}^\\top v_c)$$\n",
        "\n",
        "### Role  \n",
        "Highly efficient for skip-gram embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Log-Linear Models\n",
        "\n",
        "General form:\n",
        "$$\n",
        "P(y \\mid x; v) =\n",
        "\\frac{\\exp\\big(v \\cdot f(x,y)\\big)}\n",
        "{\\sum_{y'} \\exp\\big(v \\cdot f(x,y')\\big)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(f(x,y)\\): feature function  \n",
        "- \\(v\\): parameter vector\n",
        "\n",
        "### Role  \n",
        "Mathematical framework encompassing CBOW, skip-gram, and GloVe.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Matrix Factorization and Count-Based Models\n",
        "\n",
        "### 6.1 Word-Context Matrix\n",
        "Entries:\n",
        "$$M_{ij} = \\text{co-occurrence count of word } i \\text{ with context } j$$\n",
        "\n",
        "### Role  \n",
        "Basis of classical count-based embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.2 Singular Value Decomposition (SVD)\n",
        "\n",
        "Factorization:\n",
        "$$M = U \\Sigma V^\\top$$\n",
        "\n",
        "Truncate to rank \\(k\\) to obtain embeddings.\n",
        "\n",
        "### Role  \n",
        "Used in Latent Semantic Analysis (LSA).\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3 Weighted Co-occurrence Methods (HAL, COALS)\n",
        "Weight counts by:\n",
        "- distance  \n",
        "- normalization  \n",
        "- conditional probability forms  \n",
        "\n",
        "### Role  \n",
        "Improved statistical quality and robustness.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4 Canonical Correlation Analysis (CCA)\n",
        "\n",
        "Finds projections \\(a^\\top X, b^\\top Y\\) maximizing their correlation.\n",
        "\n",
        "### Role  \n",
        "Used in LR-MVL to combine left and right context views.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.5 Hellinger Distance & Hellinger PCA\n",
        "\n",
        "Hellinger distance between distributions \\(p\\) and \\(q\\):\n",
        "$$H(p,q) = \\frac{1}{\\sqrt{2}} \\| \\sqrt{p} - \\sqrt{q} \\|_2$$\n",
        "\n",
        "Embeddings obtained by:\n",
        "1. Normalizing co-occurrence distributions  \n",
        "2. Taking square roots  \n",
        "3. Applying PCA\n",
        "\n",
        "### Role  \n",
        "Captures geometry of probability distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Pointwise Mutual Information (PMI)\n",
        "\n",
        "Definition:\n",
        "$$\n",
        "\\text{PMI}(w,c) = \\log \\frac{P(w,c)}{P(w)P(c)}\n",
        "$$\n",
        "\n",
        "Shifted PMI:\n",
        "$$\\text{SPPMI}(w,c) = \\max\\big(\\text{PMI}(w,c) - \\log k, 0\\big)$$\n",
        "\n",
        "### Role  \n",
        "Reveals theoretical connection:  \n",
        "**SGNS ≈ Factorizing a shifted PMI matrix**.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. GloVe Mathematical Model\n",
        "\n",
        "Objective:\n",
        "$$\n",
        "J=\\sum_{i,j}\n",
        "f(X_{ij})\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(X_{ij}\\): co-occurrence  \n",
        "- \\(w_i,\\tilde{w}_j\\): word/context embeddings  \n",
        "- \\(f\\): weighting function  \n",
        "\n",
        "### Insight  \n",
        "Enforces vector differences to reflect **log co-occurrence ratios**, capturing meaning.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Distributed Representations and Arithmetic\n",
        "\n",
        "Empirical linearity:\n",
        "$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n",
        "\n",
        "### Role  \n",
        "Shows embeddings encode semantic relationships as vector offsets.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Dimensionality Reduction\n",
        "\n",
        "All embedding models reduce dimensionality:\n",
        "\n",
        "- Neural: embeddings = low-rank parameters of projection matrix.  \n",
        "- Count-based: SVD/PCA on sparse matrices.  \n",
        "- PMI: derive dense low-dimensional factors.\n",
        "\n",
        "### Role  \n",
        "Transforms high-dimensional sparse signals into compact continuous spaces.\n",
        "\n",
        "---\n",
        "\n",
        "# Summary of the Mathematical Core\n",
        "\n",
        "Word embeddings arise from solving statistical estimation problems using **two complementary mathematical families**:\n",
        "\n",
        "### 1. **Probability Modeling**\n",
        "- n-gram estimators  \n",
        "- MLE and smoothing  \n",
        "- Neural log-linear models  \n",
        "- Softmax and its approximations (IS, NCE, Negative Sampling, hierarchical softmax)\n",
        "\n",
        "### 2. **Matrix Factorization**\n",
        "- Co-occurrence matrices  \n",
        "- SVD, PCA, CCA, Hellinger geometry  \n",
        "- PMI and shifted PMI formulations\n",
        "\n",
        "### 3. **Optimization of Objective Functions**\n",
        "- Least-squares regression in GloVe  \n",
        "- Logistic objectives in SGNS  \n",
        "- Dimensionality reduction to dense spaces\n",
        "\n",
        "### Core Insight  \n",
        "**Neural prediction-based models and count-based matrix factorization methods estimate the same underlying statistical structure of word–context relationships.**  \n",
        "What differs is the algorithmic path taken—but mathematically they converge to equivalent embeddings.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "v6SoxhcfwZGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Table: Problems, Limitations of Prior Work, and Proposed Solutions  \n",
        "*(Formatted exactly in Markdown with a clean research-paper style table)*\n",
        "\n",
        "| **Research Problem / Gap** | **How It Limits Prior Work** | **How the Paper Addresses / Resolves It** |\n",
        "|----------------------------|-------------------------------|-------------------------------------------|\n",
        "| **Lack of a comprehensive, up-to-date survey on word embeddings** | Prior work scattered across NNLMs, IR, distributional semantics, statistical models, and matrix factorization; no unified source consolidating all approaches. | Provides a systematic, chronological survey covering prediction-based (NNLM, CBOW, Skip-gram, FastText) and count-based (LSA, HAL, COALS, GloVe) models in one framework. |\n",
        "| **Fragmentation between prediction-based and count-based research communities** | Models seen as unrelated; conceptual gaps led to redundant experimentation and inconsistent terminology. | Shows theoretical equivalences (e.g., SGNS ≈ shifted PMI), bridging neural and count-based paradigms under a unified statistical interpretation. |\n",
        "| **Historical reliance on sparse Vector Space Models (VSMs)** | Sparse TF-IDF vectors fail to encode semantic similarity, require huge dimensional spaces, and cannot support vector arithmetic relations. | Surveys dense distributed representations that capture semantics in low-dimensional continuous vector spaces. |\n",
        "| **Limitations of n-gram statistical language models** | High dimensionality, poor generalization to unseen sequences, memory explosion, and inability to model long-range dependencies. | Reviews neural models as a remedy: embeddings compress context and improve generalization while reducing sparsity. |\n",
        "| **Training inefficiency in early NNLMs** | Full softmax over large vocabularies made training extremely slow (days to weeks); limited scalability. | Highlights key innovations: hierarchical softmax, importance sampling, negative sampling, log-linear architectures, and NCE. |\n",
        "| **Absence of structured comparison across embedding models** | Researchers lacked a consistent reference to compare architectures, objectives, computational costs, and empirical behavior. | The survey provides structured comparison tables for prediction-based and count-based models, summarizing differences and trade-offs. |\n",
        "| **Underexplored role of global co-occurrence statistics** | Early count-based models (LSA, HAL) suffered from raw, unnormalized counts, noisy matrices, and lack of weighting principles. | Reviews modern count-based methods (COALS, Hellinger PCA, CCA, GloVe) improving normalization, factorization, and statistical grounding. |\n",
        "| **Insufficient attention to task-specific adaptation of embeddings** | Generic embeddings often underperform in specialized NLP tasks; little guidance in prior literature. | Identifies empirical evidence supporting task-specific fine-tuning and presents it as an active research direction. |\n",
        "| **Lack of exploration of composing word embeddings into higher-level units** | Word-level vectors alone cannot capture sentence-, paragraph-, or document-level meaning. | Highlights emerging compositional models: Paragraph2Vec, Skip-Thought, FastText sentence/document representations. |\n",
        "| **Need for deeper research on interpretability and unified embedding frameworks** | Prior literature rarely addressed theoretical unification or interpretability for evolving NLP systems. | Outlines future directions: enhanced unification between model classes, improved co-occurrence modeling, interpretability, and compositional embeddings. |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2icVYgyjwuZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related Work Extracted from the Reference Section  \n",
        "*(Structured Table with Columns: Author(s), Year, Title, Venue, Connection to This Paper)*\n",
        "\n",
        "| **Author(s)** | **Year** | **Title** | **Venue** | **Connection to This Paper** |\n",
        "|---------------|----------|-----------|-----------|-------------------------------|\n",
        "| Bengio, Ducharme, Vincent & Janvin | 2003 | *A Neural Probabilistic Language Model* | Journal of Machine Learning Research | Foundational neural language model; introduces distributed representations underlying prediction-based embeddings. |\n",
        "| Bengio & Senécal | 2003 | *Quick Training of Probabilistic Neural Nets by Importance Sampling* | Technical Report | Proposes importance sampling to reduce softmax cost; influences training efficiency of embedding models. |\n",
        "| Morin & Bengio | 2005 | *Hierarchical Probabilistic Neural Network Language Model* | AISTATS | Introduces hierarchical softmax; enables scalable prediction-based embedding training. |\n",
        "| Mnih & Hinton | 2007 | *Three New Graphical Models for Statistical Language Modelling* | ICML | Introduces log-bilinear models; precursors to modern neural embedding architectures. |\n",
        "| Mnih & Hinton | 2008 | *A Scalable Hierarchical Distributed Language Model* | NIPS | Demonstrates fast hierarchical softmax trees; key efficiency component in embedding learning. |\n",
        "| Mikolov, Kopecký, Burget, Glembek & Černocký | 2009 | *Neural Network Based Language Models for Highly Inflective Languages* | ICASSP | Early NNLM work using multi-stage training; influenced Word2Vec training design. |\n",
        "| Mikolov et al. | 2010 | *Recurrent Neural Network Based Language Model* | INTERSPEECH | Introduces RNN-based LMs capable of modeling long context; supports understanding of embedding regularities. |\n",
        "| Mikolov, Yih & Zweig | 2013a | *Linguistic Regularities in Continuous Space Word Representations* | NAACL-HLT | Demonstrates vector arithmetic properties (semantic/syntactic regularities); establishes embeddings as independent research direction. |\n",
        "| Mikolov et al. | 2013b | *Efficient Estimation of Word Representations in Vector Space* | arXiv | Introduces CBOW and Skip-gram; foundational prediction-based embedding models. |\n",
        "| Mikolov et al. | 2013c | *Distributed Representations of Words and Phrases and Their Compositionality* | NIPS | Introduces negative sampling and subsampling; accelerates and strengthens Word2Vec models. |\n",
        "| Bojanowski, Grave, Joulin & Mikolov | 2016 | *Enriching Word Vectors with Subword Information* | arXiv | FastText; extends embeddings with character n-grams, critical for morphologically rich languages. |\n",
        "| Joulin, Grave, Bojanowski & Mikolov | 2016 | *Bag of Tricks for Efficient Text Classification* | arXiv | Expands FastText; shows practical advantages of subword-based embeddings. |\n",
        "| Collobert & Weston | 2008 | *A Unified Architecture for NLP: Deep Neural Networks with Multitask Learning* | ICML | Uses embeddings as core features; early negative sampling idea; major milestone in neural NLP. |\n",
        "| Deerwester et al. | 1990 | *Indexing by Latent Semantic Analysis* | JASIST | LSA introduces SVD-based count embeddings; earliest classical vector-space embedding method. |\n",
        "| Lund & Burgess | 1996 | *Producing High-Dimensional Semantic Spaces from Lexical Co-occurrence* | Behavior Research Methods | HAL; early co-occurrence-based embedding approach. |\n",
        "| Rohde, Gonnerman & Plaut | 2006 | *An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence* | Cognitive Science | COALS; normalized count-based embeddings improving earlier models. |\n",
        "| Dhillon, Foster & Ungar | 2011 | *Multi-View Learning of Word Embeddings via CCA* | NIPS | LR-MVL; applies CCA to left and right contexts; major count-based technique. |\n",
        "| Lebret & Collobert | 2013 | *Word Embeddings through Hellinger PCA* | arXiv | Uses Hellinger PCA on normalized co-occurrence matrices; improves distributional geometry. |\n",
        "| Pennington, Socher & Manning | 2014 | *GloVe: Global Vectors for Word Representation* | EMNLP | Defines GloVe; major count-based model using co-occurrence ratios. |\n",
        "| Bullinaria & Levy | 2007; 2012 | *Extracting Semantic Representations from Word Co-Occurrence Statistics* | Behavior Research Methods | Systematic study of weighting, normalization, and dimensionality; important groundwork for count-based embeddings. |\n",
        "| Levy & Goldberg | 2014 | *Neural Word Embedding as Implicit Matrix Factorization* | NIPS | Shows SGNS ≈ factorizing shifted PMI; crucial theoretical unification of prediction- and count-based models. |\n",
        "| Turney & Pantel | 2010 | *From Frequency to Meaning: Vector Space Models of Semantics* | JAIR | Comprehensive overview of vector-space approaches; foundational survey for count-based embeddings. |\n",
        "| Harris | 1954 | *Distributional Structure* | Word | Origin of distributional hypothesis; conceptual basis for all embedding methods. |\n",
        "| Goodman | 2001 | *Classes for Fast Maximum Entropy Training* | arXiv | Improves maximum entropy training; influences statistical language modeling underpinning embeddings. |\n",
        "| Katz | 1987 | *Estimation of Probabilities from Sparse Data* | IEEE Trans. ASSP | Classic smoothing; highlights sparsity issues motivating embeddings. |\n",
        "| Brown et al. | 1992 | *Class-Based N-gram Models of Natural Language* | Computational Linguistics | Class-based distributions; early structured alternative to high-dimensional n-grams. |\n",
        "| Mnih & Teh | 2012 | *A Fast and Simple Algorithm for Training Neural Probabilistic Language Models* | ICML | Introduces NCE; major softmax approximation used in embedding training. |\n",
        "| Maas et al. | 2011 | *Learning Word Vectors for Sentiment Analysis* | ACL | Demonstrates task-specific embedding tuning; inspires research on specialized embeddings. |\n",
        "| Kiros et al. | 2015 | *Skip-Thought Vectors* | arXiv | Extends distributional learning to sentence-level embedding models. |\n",
        "| Le & Mikolov | 2014 | *Distributed Representations of Sentences and Documents* | ICML | Paragraph2Vec; foundational model for compositional (document-level) embeddings. |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "O4BaAPrqxOgQ"
      }
    }
  ]
}