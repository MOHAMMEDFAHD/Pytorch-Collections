{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  1️⃣ Overview\n",
        "\n",
        "| **Aspect** | **One-Hot Encoding** | **Modern Word Embeddings** |\n",
        "|-------------|----------------------|-----------------------------|\n",
        "| **Type of Representation** | Discrete, symbolic | Continuous, distributed |\n",
        "| **Vector Values** | Binary (0s and 1s) | Real-valued (floats) |\n",
        "| **Dimensionality** | Equal to vocabulary size (e.g., 50,000+) | Typically 50–1024 dimensions |\n",
        "| **Storage & Efficiency** | Extremely sparse → inefficient | Dense & compact → efficient |\n",
        "| **Meaning Representation** | No semantics (just identity) | Captures semantic and syntactic relationships |\n",
        "| **Similarity Measure** | Orthogonal (all vectors equally distant) | Semantic proximity via cosine similarity |\n",
        "| **Trainability** | Fixed, manually defined | Learned automatically from data |\n",
        "| **Context Sensitivity** | None | Context-aware (esp. in models like BERT) |\n",
        "\n",
        "---\n",
        "\n",
        "#  2️⃣ One-Hot Encoding (The Classical Baseline)\n",
        "\n",
        "## Definition\n",
        "\n",
        "Each word is represented as a binary vector of size equal to the vocabulary:\n",
        "\n",
        "$$\n",
        "\\text{word} = [0, 0, 1, 0, 0, \\dots]\n",
        "$$\n",
        "\n",
        "Only one position is 1 (the index of the word), and the rest are 0s.\n",
        "\n",
        "---\n",
        "\n",
        "## Properties\n",
        "\n",
        "- **High-dimensional** (e.g., 50k-dim vector for 50k vocabulary).  \n",
        "- **Sparse** — most entries are zeros.  \n",
        "- **No semantics** — similar meanings have zero similarity:  \n",
        "  - “cat” = [0, 1, 0, 0]  \n",
        "  - “dog” = [0, 0, 1, 0]  \n",
        "  ⇒ Cosine similarity = 0  \n",
        "\n",
        "- **Cannot generalize** — each word is treated independently.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Ignores **context** and **meaning**.  \n",
        "- Inefficient for large vocabularies.  \n",
        "- Cannot express **similarity, analogies, or polysemy**.\n",
        "\n",
        " **Summary:** One-hot encoding = unique identity, no meaning.\n",
        "\n",
        "---\n",
        "\n",
        "#  3️⃣ Modern Word Embeddings\n",
        "\n",
        "Modern embeddings solve one-hot limitations by **learning distributed representations** — dense vectors capturing meaning from context.\n",
        "\n",
        "---\n",
        "\n",
        "##  A. Static Distributed Embeddings (2013–2015)\n",
        "\n",
        "### **Word2Vec (Mikolov et al., 2013)**\n",
        "\n",
        "- Neural network models: **CBOW** & **Skip-gram**.  \n",
        "- Learn to predict context → meaning encoded geometrically.  \n",
        "- Famous relation:  \n",
        "\n",
        "  $$\n",
        "  \\text{King} - \\text{Man} + \\text{Woman} \\approx \\text{Queen}\n",
        "  $$\n",
        "\n",
        "- Compact (e.g., 300D) yet semantically rich.\n",
        "\n",
        "### **GloVe (Pennington et al., 2014)**\n",
        "\n",
        "- Combines **global co-occurrence** statistics with local context.  \n",
        "- Matrix factorization approach capturing both frequency and semantics.\n",
        "\n",
        "**Advantage:** Similarity is measurable geometrically.  \n",
        "Example:\n",
        "\n",
        "$$\n",
        "\\text{cosine}(\"car\",\"automobile\") \\approx 0.9, \\quad\n",
        "\\text{cosine}(\"car\",\"banana\") \\approx 0.0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##  B. Contextual Word Embeddings (2018–Present)\n",
        "\n",
        "### **ELMo (Peters et al., 2018)**\n",
        "\n",
        "- Bi-directional **LSTMs** create *context-dependent* vectors.  \n",
        "  > “bank” (river) ≠ “bank” (finance)  \n",
        "\n",
        "### **BERT (Devlin et al., 2019)**\n",
        "\n",
        "- Transformer-based masked-language model.  \n",
        "- Each token’s embedding depends on full sentence context.  \n",
        "- Handles **polysemy** and **syntax** dynamically.\n",
        "\n",
        "**Key Edge:** Contextual disambiguation — meaning changes with context.\n",
        "\n",
        "---\n",
        "\n",
        "##  C. Domain-Specific & Multimodal Extensions\n",
        "\n",
        "- **BioVec / ProtVec** (Asgari & Mofrad 2015): biological sequences.  \n",
        "- **Sentence-BERT** (Reimers & Gurevych 2019): sentence similarity.  \n",
        "- **CLIP / ALIGN:** cross-modal (vision-language) embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "#  4️⃣ Example: Visualizing the Difference\n",
        "\n",
        "### One-Hot Representation\n",
        "```python\n",
        "vocab = [\"cat\", \"dog\", \"apple\"]\n",
        "cat   = [1, 0, 0]\n",
        "dog   = [0, 1, 0]\n",
        "apple = [0, 0, 1]\n",
        "```\n",
        "\n",
        "#  Example: Visualizing Semantic Difference\n",
        "\n",
        "### One-Hot Representation\n",
        "All orthogonal → cosine similarity = 0.\n",
        "\n",
        "---\n",
        "\n",
        "### Word2Vec / GloVe (Simplified)\n",
        "```\n",
        "cat   = [0.9, 0.7, 0.2]\n",
        "dog   = [0.88, 0.65, 0.25]\n",
        "apple = [-0.1, 0.2, 0.9]\n",
        "\n",
        "cosine(cat, dog) ≈ 0.98  \n",
        "cosine(cat, apple) ≈ 0.2  \n",
        "\n",
        "Geometry now encodes meaning.\n",
        "```\n",
        "---\n",
        "\n",
        "# Comparative Summary\n",
        "\n",
        "| Feature | One-Hot Encoding | Word2Vec / GloVe | ELMo / BERT (Contextual) |\n",
        "|----------|------------------|------------------|---------------------------|\n",
        "| Type | Symbolic | Static semantic | Contextual semantic |\n",
        "| Dimensionality | = Vocab size | 100–300 | 512–1024+ |\n",
        "| Learning | Manual | Neural / Matrix Factorization | Transformer / Contextual |\n",
        "| Context Awareness | None | Limited | Deep |\n",
        "| Polysemy Handling | No | No | Yes |\n",
        "| Similarity Capture | No | Yes | Yes |\n",
        "| Efficiency | Sparse and Large | Compact | Computationally Heavy |\n",
        "| Use Case | Simple categorical NLP | Classical semantic tasks | Modern deep NLP / LLMs |\n",
        "\n",
        "---\n",
        "\n",
        "# In Summary\n",
        "\n",
        "| Perspective | Takeaway |\n",
        "|--------------|-----------|\n",
        "| Conceptual Shift | Discrete symbols → Continuous meaning vectors |\n",
        "| Core Idea | Words in similar contexts share similar vectors |\n",
        "| Mathematical Transition | Orthogonal bases → Low-rank semantic manifolds |\n",
        "| Impact | Enabled deep semantic understanding and foundation models |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1O8dsVW6q5ae"
      }
    }
  ]
}